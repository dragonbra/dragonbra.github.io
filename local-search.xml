<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Analyzing Multi-Head Self-Attention 论文笔记</title>
    <link href="/2022-04-12-AnalyzingMH-Notes/"/>
    <url>/2022-04-12-AnalyzingMH-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="Analyzing-Multi-Head-Self-Attention-论文笔记"><a href="#Analyzing-Multi-Head-Self-Attention-论文笔记" class="headerlink" title="Analyzing Multi-Head Self-Attention 论文笔记"></a>Analyzing Multi-Head Self-Attention 论文笔记</h1><h2 id="论文主要信息"><a href="#论文主要信息" class="headerlink" title="论文主要信息"></a>论文主要信息</h2><ul><li>标题：Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</li><li>作者：<a href="https://arxiv.org/search/cs?searchtype=author&query=Voita,+E">Elena Voita</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Talbot,+D">David Talbot</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Moiseev,+F">Fedor Moiseev</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Sennrich,+R">Rico Sennrich</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Titov,+I">Ivan Titov</a></li><li>机构：Yandex, Russia; University of Amsterdam, Netherlands</li><li>来源：ACL 2019</li><li>代码：<a href="https://github.com/lena-voita/the-story-of-heads">https://github.com/lena-voita/the-story-of-heads</a></li></ul><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>这篇工作对Transformer中的多头注意力机制进行研究。通过剪枝等实验分析评估了单个注意头对模型整体性能的贡献。实验发现最重要、最有用的头通常都扮演着语言上可解释的角色。</p><p>当使用基于随机门和$L_0$的可微分松弛的方法进行剪枝的时候，观察到这种特殊的头通常都是最后被剪枝掉的。作者提出的新剪枝的方法可以去除多头注意力机制中绝大部分的头，而不严重影响性能。例如，在英语-俄语的WMT数据集上，编码器中48个头在剪去38个头之后，性能仅下降了0.15的BLEU。</p><h2 id="8-结论-Conclusion"><a href="#8-结论-Conclusion" class="headerlink" title="8 结论 Conclusion"></a>8 结论 Conclusion</h2><p>作者评估了每个单独的注意头对Transformer模型的翻译任务性能的贡献。作者使用层与层之间关联传播来表明了不同的头的相对贡献是不同的，只有一小部分的头看起来是对翻译任务重要的。在该模型中，重要的头具有一个或多个可解释的功能，包括注意相邻词和跟踪特定的句法关系。</p><p>为了确定剩余的难以解释的头是否对模型的性能至关重要，我们引入了一种新的方法来剪枝注意头。我们观察到，重要的注意是最后被剪掉的，这直接证实了他们的重要性。此外，绝大多数的头，特别是编码器的自注意头，可以在不严重影响性能的情况下被移除。</p><h2 id="1-导言-Introduction"><a href="#1-导言-Introduction" class="headerlink" title="1 导言 Introduction"></a>1 导言 Introduction</h2><p>最近Transformer很火，对其使用的多头注意力机制对翻译任务的重要性的探究是一项具有挑战性的任务。作者对此尝试去探究以下问题：</p><ol><li>翻译质量在多大程度上依赖于单个编码器的头？</li><li>单个编码器头是否扮演一致和可解释的角色？如果是的话，哪些是对翻译质量最重要的？</li><li>哪种类型的模型注意（编码器自我注意、解码器自我注意或解码器-编码器注意）对注意头的数量最敏感，和在哪些层上最敏感？</li><li>我们能否在保持翻译质量的同时显著减少注意头的数量？</li></ol><p>我们首先使用分层关联传播来确定每个编码器层中最重要的头，对于被认为很重要的头，我们试图描述它们所扮演的角色。我们观察到以下几种角色：位置(头指向相邻的标记)、句法(头指向特定句法依赖关系中的标记)和罕见词(头指向句子中出现频率最低的标记)。</p><p>虽然我们不能轻易地将活动头的数量作为惩罚项纳入我们的学习目标(即L0正则化器)，但我们可以使用可微分的松弛。我们从融合的完整模型开始，在一个持续学习的场景中修剪注意力头，并确定那些留在模型中的角色。</p><p>这些实验证实了分层关联传播的研究结果；特别地，具有明确可识别的位置和句法功能的头被最后修剪，因此被证明它在翻译任务中是最重要的。</p><h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><p>作者列举的几点key findings：</p><ol><li>只有一小部分头对翻译任务很重要</li><li>重要的头在模型中有一个或多个专门的和可解释的功能</li><li>这些功能对应的是对相邻单词和特定句法依赖关系中的标记的注意。</li></ol><h2 id="2-Transformer的架构-Transformer-Architecture"><a href="#2-Transformer的架构-Transformer-Architecture" class="headerlink" title="2 Transformer的架构 Transformer Architecture"></a>2 Transformer的架构 Transformer Architecture</h2><p>Transformer是一个编码器-解码器模型，编码器和解码器都使用堆叠的自注意层和全连接层。该编码器由$N$层组成，每层包含两个子层：(a)多头自注意机制，(b)前馈网络。多头注意机制依赖于规模的点积注意，它对一个查询$Q$、一个键$K$和一个值$V$进行操作：</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn1.png"></p><p>其中$d_k$是Key的维度。在自我注意力中，查询、键和值来自前一层的输出。</p><p>多头注意机制获得h个(即每个头一个)不同的$(Q, K, V)$表示，为每个表示计算缩放的点积注意，将结果串接，并通过前馈层投影串接。这可以用与式(1)相同的符号表示：</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn2.png"></p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn3.png"></p><p>其中$W_i$和$W^O$是参数矩阵。</p><p>Transformer网络每一层的第二个组成部分是前馈网络。作者建议使用一个激活ReLU的两层网络。</p><p>类似地，解码器的每一层包含上述两个子层以及一个附加的多头注意子层。这个额外的子层接收编码器的输出作为它的键和值。</p><p>Transformer以三种不同的方式使用多头注意:编码器自注意、解码器自注意和解码器-编码器注意。在这项工作中，我们主要集中在编码器的自我注意。</p><h2 id="4-识别重要的头-Identifying-Important-Heads"><a href="#4-识别重要的头-Identifying-Important-Heads" class="headerlink" title="4 识别重要的头 Identifying Important Heads"></a>4 识别重要的头 Identifying Important Heads</h2><p>我们将头部的“信心”定义为其最大注意力权重的平均值(不包括句子末尾符号)，其中平均值是用于评价的一组句子(开发集)中的符号。一个自信的头通常会将其高度的注意力集中在一个标记上。直觉上，我们可能会认为自信的头对于翻译任务很重要。 </p><p>分层关联传播(LRP) (Ding et al.， 2017)是一种计算网络中某一点神经元对另一点神经元的相对贡献的方法在。这里，我们提出使用LRP来评估每个层的不同头对模型预测的最高logit的贡献程度。结果具有较高相关性值的头可能被认为对模型的预测更重要。</p><p>LRP的结果如图1a、2a、2c所示。在每一层中，LRP将少量的头像排列得比其他所有头像都重要。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig1.png"></p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig2.png"></p><p>每个头的置信度如图1b所示。我们可以看到，由LRP计算的头像的相关性与其置信度在一个合理的程度上是一致的。这个模式唯一明显的例外是LRP判断为第一层中最重要的头。它是第一层中最相关的头部，但其平均最大关注权重较低。</p><h2 id="5-头扮演的是什么角色-Characterizing-heads"><a href="#5-头扮演的是什么角色-Characterizing-heads" class="headerlink" title="5 头扮演的是什么角色 Characterizing heads"></a>5 头扮演的是什么角色 Characterizing heads</h2><p>我们研究了一些特别关注被LRP排名高的头像的注意矩阵，并确定了头像可能发挥的三种功能：</p><ol><li>位置的作用：头部指向相邻的token</li><li>句法上的作用:头指向特定句法关系中的符号</li><li>生僻字：头首指的是句子中出现频率最低的符号</li></ol><h3 id="5-1-Positional-heads"><a href="#5-1-Positional-heads" class="headerlink" title="5.1 Positional heads"></a>5.1 Positional heads</h3><p>如果一个头至少90%的时间被分配到一个特定的相对位置(在实验中是-1或+1，即对相邻标记的关注)，我们就称其为“位置型”。图1c中英-俄、英-德、英-法的头部分别用紫色表示，并标出相对位置。</p><p>可以看出，位置头像在很大程度上对应LRP排名中最自信的头像和最重要的头像。事实上，对于这里考虑的所有语言对，每个位置头的平均最大注意力权重超过0.8。</p><h3 id="5-2-Syntactic-heads"><a href="#5-2-Syntactic-heads" class="headerlink" title="5.2 Syntactic heads"></a>5.2 Syntactic heads</h3><p>我们假设，当用于执行翻译时，Transformer的编码器可能负责消除源句子的句法结构的歧义。因此，我们希望知道一个头是否会处理句子中与任何主要句法关系相对应的符号。在我们的分析中，我们观察了以下依赖关系:名义主语(nsubj)、直接宾语(dobj)、形容词修饰语(mod)和副词修饰语(advmod)。这包括一个句子的主要言语论点和一些其他的共同关系。</p><p>许多依赖关系经常观察到特定的相对位置(例如,他们常常举行相邻标记,如图3所示)。我们说一头是“语法”如果其准确性是至少高出10%基线看起来最常见的相对位置的依赖关系</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig3.png"></p><p>表1显示了英语-俄语在两个领域中所考虑的依赖关系中最精确的标题的准确性。图4比较了在WMT上训练的模型在不同目标语言下的得分。几个头似乎负责相同的依赖关系。这些头在图1c、2b、2d中用绿色表示。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Tab1.png"></p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig4.png"></p><p>（这个图里展示的是句法头相关的实验内容，但是我不太理解表里的具体含义）</p><h3 id="5-3-Rare-words"><a href="#5-3-Rare-words" class="headerlink" title="5.3 Rare words"></a>5.3 Rare words</h3><p>在所有模型(中，我们发现第一层中的一个头被认为比这一层中的任何其他头对模型的预测更重要。</p><p>我们发现这个头指向句子中出现频率最低的符号。对于在open字幕上训练的模型，在句子中最不频繁的标记不在最频繁的500个标记中，这个头指向66%的情况下最罕见的标记，以及83%的情况下最不频繁的两个标记之一。对于在WMT上训练的模型，这个头指向了在超过50%的这种情况下最不频繁的两个标记之一。这个头部在图1中以橙色表示。</p><h2 id="6-剪枝注意力头-Pruning-Attention-Heads"><a href="#6-剪枝注意力头-Pruning-Attention-Heads" class="headerlink" title="6 剪枝注意力头 Pruning Attention Heads"></a>6 剪枝注意力头 Pruning Attention Heads</h2><p>我们已经确定了每一层最相关的头的某些功能，并表明它们在很大程度上是可解释的。剩下的头呢?它们对翻译质量来说是多余的，还是扮演着同样重要但不那么容易界定的角色?我们介绍了一种修剪注意力头的方法，试图回答这些问题。我们的方法基于Louizos等人(2018)。当他们修剪个体神经网络权值时，我们修剪整个模型组件(即头部)。我们首先描述我们的方法，然后检查当我们删除头像时，性能如何变化，识别在稀疏模型中保留的头像的功能。</p><h3 id="6-1-Method"><a href="#6-1-Method" class="headerlink" title="6.1 Method"></a>6.1 Method</h3><p>在每个头前面加入了一个$g_i$，式子3就变成了：</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn4.png"></p><p>与通常的门不同，gi是特定于头部的参数，独立于输入(即句子)。由于我们希望完全禁用不太重要的头，而不是简单地降低它们的权重，所以理想情况下，我们将对标量gi应用L0正则化。L0范数等于非零组件的数量，会促使模型关闭不太重要的头：</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn5.png"></p><p>式中，$h$为头数，$[[]]$为指示函数。 </p><p>不幸的是，L0范数是不可微的，因此不能直接作为正则项纳入目标函数。相反，我们使用一个随机松弛:每个门的gi现在是一个随机变量独立于特定头像的分布我们使用Hard Concrete分布(Louizos et al.， 2018)，这是闭区间上混合离散-连续分布的参数化族[0,1]，见图6a。分布在0、1、P (gi = 0|φi)、P (gi = 1|φi)处有非零概率质量，其中φi为分布参数。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig6.png"></p><p>修改后的优化目标函数为：</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Eqn6.png"></p><h3 id="6-2-Pruning-encoder-heads"><a href="#6-2-Pruning-encoder-heads" class="headerlink" title="6.2 Pruning encoder heads"></a>6.2 Pruning encoder heads</h3><p>为了确定哪个头函数在编码器中是最重要的，以及该模型需要多少个头，我们进行了一系列的实验，用门只应用于编码器的自我注意。在这里，我们通过对正则化目标的训练模型进行微调来修剪模型在剪枝过程中，译码器的参数是固定的，只对编码器参数和门限进行微调。通过不对解码器进行微调，我们确保经过修剪的编码器头的功能不会迁移到解码器。</p><h4 id="6-2-1-BLEU-量化分数结果"><a href="#6-2-1-BLEU-量化分数结果" class="headerlink" title="6.2.1 BLEU 量化分数结果"></a>6.2.1 BLEU 量化分数结果</h4><p>图7提供了BLEU分数。对于更复杂的WMT任务，编码器中的10个头足以保持在全模式的0.15 BLEU范围内。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig7.png"></p><h4 id="6-2-2-剩余的头的作用"><a href="#6-2-2-剩余的头的作用" class="headerlink" title="6.2.2 剩余的头的作用"></a>6.2.2 剩余的头的作用</h4><p>图7的结果表明，即使只有几个头，编码器仍然有效。在本节中，我们研究在剪枝过程中留在编码器中的那些头的功能。图8显示了在修剪模型中，所有头部的功能都用颜色编码。每一列对应一个模型，在修剪后保留特定数量的头。各层的头部按功能排序。有些头可以执行几种功能(例如，s→v和v→o);在本例中显示了函数的数量。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig8.png"></p><h3 id="6-3-剪枝编码器以外的head-Pruning-all-types-of-attention-heads"><a href="#6-3-剪枝编码器以外的head-Pruning-all-types-of-attention-heads" class="headerlink" title="6.3 剪枝编码器以外的head Pruning all types of attention heads"></a>6.3 剪枝编码器以外的head Pruning all types of attention heads</h3><p>我们发现我们的剪枝技术可以有效地减少编码器中的头的数量，而不会对翻译质量造成重大的下降。现在我们研究修剪模型中所有类型的注意头的效果(不仅仅是在编码器中)。这使我们能够评估不同类型的注意在翻译任务模型中的重要性。在这些实验中，我们对Transformer中的所有多头注意头都添加了门，即编码器和解码器的自注意以及解码器到编码器的注意。</p><h4 id="6-3-1-结果"><a href="#6-3-1-结果" class="headerlink" title="6.3.1 结果"></a>6.3.1 结果</h4><p>各注意层的剪枝实验结果如表2所示。对京东商城模型训练数据,我们能够修剪几乎3⁄4编码器正面和超过1⁄3的正面在解码器self-attention decoder-encoder注意没有任何明显的翻译质量损失(稀疏,行1)。我们也可以删除模型中超过半数的头和损失不超过0.25BLEU。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Tab2.png"></p><h4 id="6-3-2-Heads-Importance"><a href="#6-3-2-Heads-Importance" class="headerlink" title="6.3.2 Heads Importance"></a>6.3.2 Heads Importance</h4><p>图9显示了在不同修剪速率下，每种注意类型的保留头数。我们可以看到，该模型更倾向于先修剪编码器的自注意头，而解码器-编码器的注意头对于这两个数据集似乎是最重要的。显然，如果没有解码器的注意，翻译就不可能发生。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig9.png"></p><p>图10显示了对于不同稀疏率的模型，解码器中不同层的主动自注意头和解码器-编码器注意头的数量(为了减少噪声，我们绘制了相邻层中剩余的对注意头的和)。</p><p><img src="/img/2022-04-12-AnalyzingMH-Notes/Fig10.png"></p><h2 id="简短总结-你看完这篇论文的总结"><a href="#简短总结-你看完这篇论文的总结" class="headerlink" title="简短总结 # 你看完这篇论文的总结"></a>简短总结 # 你看完这篇论文的总结</h2><p>这篇文章主要讲了在Transformer的编码器中对自注意头的重要性探索，通过剪枝掉整个头（使用gate让某个头完全不起作用）来比较实验结果。对每个头的重要性进行了评估，并且探究了这些头在翻译任务中为什么重要，他们可能在其中扮演了什么样的角色，在此基础上，作者提出了三种可能很重要的头的类型：</p><ol><li>注意相邻词的位置的头</li><li>注意语法、句法关系的头</li><li>注意生僻字符的头</li></ol><p>并且在这个基础上，作者对Transformer的编码器进行了剪枝（删掉头）实验，得到的结果是最终剩下的头普遍也是之前较为重要的头，并且在性能损失较小的情况下通过对重要的头的保留，能够将原来48个头剪去38个。</p><p>后来也将这项剪枝头的实验推广到编码器以外的层（解码器等），也进行了实验结果的讨论和比较。</p><h2 id="创新点-这篇论文自己写的贡献"><a href="#创新点-这篇论文自己写的贡献" class="headerlink" title="创新点 # 这篇论文自己写的贡献"></a>创新点 # 这篇论文自己写的贡献</h2><p><a href="https://c9hu5keisa.feishu.cn/docs/doccn5sJJQmFbf9N9OOvFweobLd#DQHmDk">1 引言-贡献</a></p><h2 id="优劣-论文自己写的和你认为的优劣势（相比其他方法）"><a href="#优劣-论文自己写的和你认为的优劣势（相比其他方法）" class="headerlink" title="优劣 # 论文自己写的和你认为的优劣势（相比其他方法）"></a>优劣 # 论文自己写的和你认为的优劣势（相比其他方法）</h2><ol><li>感觉提出每个注意力头可能扮演的角色这一点很新颖。感觉作者用了一个合适的故事来讲他探索自注意力机制的作用。但是代价可能就是这样的工作只足够他在机器翻译这一个任务上进行探究，结果其实不一定有普遍性。</li><li>作者提出的方法其实是基于前人的一个公式改进，使得L0范数可以微分，他只是在这个基础上直接进行了实验，在剪枝方法上显得比较粗暴，也没有什么新颖。总体上感觉是一篇实验结果和讨论为主的文章。</li></ol><h2 id="流程与公式-尽量以图为主，附带必要说明"><a href="#流程与公式-尽量以图为主，附带必要说明" class="headerlink" title="流程与公式 # 尽量以图为主，附带必要说明"></a>流程与公式 # 尽量以图为主，附带必要说明</h2><p><a href="https://c9hu5keisa.feishu.cn/docs/doccn5sJJQmFbf9N9OOvFweobLd#16zmrK">6 剪枝</a></p><h2 id="主要实验-重点是比较的表格"><a href="#主要实验-重点是比较的表格" class="headerlink" title="主要实验 # 重点是比较的表格"></a>主要实验 # 重点是比较的表格</h2><p><a href="https://c9hu5keisa.feishu.cn/docs/doccn5sJJQmFbf9N9OOvFweobLd#lwLMOG">6.2 6.3 实验结果</a></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理 NLP</tag>
      
      <tag>Transformer</tag>
      
      <tag>论文笔记</tag>
      
      <tag>剪枝 Pruning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pruning 泛读论文笔记</title>
    <link href="/2022-04-03-Pruning-Notes/"/>
    <url>/2022-04-03-Pruning-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="Pruning-泛读论文笔记"><a href="#Pruning-泛读论文笔记" class="headerlink" title="Pruning 泛读论文笔记"></a>Pruning 泛读论文笔记</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><table><thead><tr><th>序号</th><th>Tree</th><th>Type</th><th>Short</th><th>发表日期</th><th>会议期刊</th><th>论文名</th><th>代码链接</th><th>备注</th></tr></thead><tbody><tr><td>1</td><td>Pruning</td><td>NLP压缩</td><td></td><td>2019.10.14</td><td><a href="https://arxiv.org/abs/1910.06360">Arxiv</a></td><td>Structured  Pruning of a BERT-based Question Answering Model</td><td></td><td></td></tr><tr><td>2</td><td>Pruning</td><td>NLP压缩</td><td>RPP</td><td>2019.9.27</td><td><a href="https://arxiv.org/abs/1909.12486">Arxiv</a></td><td>Reweighted  Proximal Pruning for Large-Scale Language Representation</td><td></td><td>稀疏</td></tr><tr><td>3</td><td>Pruning</td><td>NLP压缩</td><td>Mask-BERT</td><td>2020.10.11</td><td>EMNLP 2020</td><td>Masking  as an Efficient Alternative to Finetuning for Pretrained Language Models</td><td><a href="https://github.com/ptlmasking/maskbert">CODE_LINK</a></td><td></td></tr><tr><td>4</td><td>Pruning</td><td>模型分析</td><td>Analysis</td><td>2020.2.19</td><td><a href="https://arxiv.org/abs/2002.08307">ACL 2020</a></td><td>Compressing  BERT: Studying the Effects of Weight Pruning on Transfer Learning</td><td><a href="https://github.com/mitchellgordon95/bert-prune">      CODE_LINK</a></td><td>Workshop</td></tr><tr><td>5</td><td>Pruning</td><td>模型分析</td><td></td><td>2020.5.1</td><td><a href="https://arxiv.org/abs/2005.00561">EMNLP 2020</a></td><td>When  BERT Plays the Lottery, All Tickets Are Winning</td><td></td><td></td></tr><tr><td>6</td><td>Pruning</td><td>NLP压缩</td><td>MvP</td><td>2020.5.15</td><td>NIPS 2020</td><td>Movement  Pruning: Adaptive Sparsity by Fine-Tuning</td><td><a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/movement-pruning">CODE_LINK</a></td><td></td></tr><tr><td>7</td><td>Pruning</td><td>NLP压缩</td><td>IMP</td><td>2020.7.23</td><td>NIPS 2020</td><td>The  Lottery Ticket Hypothesis for Pre-trained BERT Networks</td><td></td><td></td></tr><tr><td>8</td><td>Pruning</td><td>NLP压缩</td><td></td><td>2021.12.10</td><td><a href="https://arxiv.org/abs/2112.05705">NIPS 2021</a></td><td>Pruning  Pretrained Encoders with a Multitask Objective</td><td></td><td></td></tr><tr><td>9</td><td>Pruning</td><td></td><td>互信息，压缩</td><td>2021.8.28</td><td><a href="https://arxiv.org/abs/2108.12594">EMNLP 2021</a></td><td>Layer-wise  Model Pruning based on Mutual Information</td><td></td><td>使用互信息，选择最小的特征维度，然后基于此进行权重裁剪</td></tr><tr><td>10</td><td>Pruning</td><td>NLP压缩</td><td>BlockPruning</td><td></td><td><a href="https://arxiv.org/abs/2109.04838">EMNLP 2021</a></td><td>Block  Pruning For Faster Transformers</td><td><a href="https://github.com/huggingface/nn_pruning">CODE_LINK</a></td><td></td></tr></tbody></table><h2 id="1-Structured-Pruning-of-a-BERT-based-Question-Answering-Model"><a href="#1-Structured-Pruning-of-a-BERT-based-Question-Answering-Model" class="headerlink" title="1 - Structured Pruning of a BERT-based Question Answering Model"></a>1 - Structured Pruning of a BERT-based Question Answering Model</h2><h3 id="论文内容"><a href="#论文内容" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>这篇文章研究了在基于BERT和RoBERTa的QA系统里，通过对底层Transformer进行参数结构化剪枝（structured pruning）。发现了一种成本较低的对特定任务进行结构化剪枝和知识蒸馏的混合方法，并且也不需要预训练蒸馏，就能在速度和精度均有的情况下有很好的性能表现。从SQuAD 2.0和Natural Questions的完整模型上开始，引入了能单独消除Transformer中特定部分的门（gates）。</p><p>具体研究了：(1)通过结构化剪枝减少每一层Transformer里的参数。(2)在基于BERT和基于RoBERTa的模型上的适用性。(3)对SQuAD 2.0和Natural Questions的适用性。(4)将结构化剪枝的方法和蒸馏的方法结合起来。</p><p>得到的结果是在Natural Questions上提高了一倍的速度，只损失了不到0.5的F1分数。</p><h4 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h4><p>作者研究了基于Transformer的各类MRC模型的剪枝方法，发现注意力头的层(attention head layers)和前向传播层(feed forward layers)能在很小的精度损失下被很好地剪枝。</p><p>发现相比”Gain”方法，”L_0正则化”的剪枝方法对Transformer的这两个部分更有用。</p><p>不需要重新预训练，只需要使用特定任务的训练数据就能剪枝，这个花费和微调差不多，比预训练的花费小很多。</p><h4 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h4><p>提到了NLP的QA任务中，一个新的领域，机器阅读理解（Machine Reading Comprehension, MRC），它的目标是阅读和理解给定的文本，然后在此基础上回答问题。MRC很难加速，但是蒸馏在MRC上的应用效果不太好，表1中比较了两种著名的BERT压缩方法，Distil-bert 和 TinyBert的F1分数。</p><p><img src="/img/2022-04-03-Pruning-Notes/1-1.png"></p><p>本文的主要贡献是：</p><ol><li> 将结构化剪枝技术应用于前馈层的隐藏维度，而不仅仅是注意头</li><li> 蒸馏和剪枝的结合</li><li> 以最小的准确性损失和相当大的速度显著剪枝MRC系统，不需要重新预训练</li></ol><h4 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h4><p>主要关注MRC的蒸馏、剪枝模型-Distil-bert, TinyBERT, MobileBERT。</p><p>对BERT的剪枝忽略了MRC任务。</p><p>另一些方法忽略了在较低层中文本和查询之间的注意力机制。</p><h4 id="3-Pruning-transformers"><a href="#3-Pruning-transformers" class="headerlink" title="3 Pruning transformers"></a>3 Pruning transformers</h4><h5 id="3-1-Gate-placement"><a href="#3-1-Gate-placement" class="headerlink" title="3.1 Gate placement"></a>3.1 Gate placement</h5><p>作者的剪枝方法就是在Transformer中插入额外的可训练的参数，遮罩(masks)，每个mask的取值$\gamma _i \in (0, 1)$，决定是否使对应的切片生效。插入了两种类型的mask。</p><ol><li>在每个自注意层里放置一个mask，决定每个注意力头是否起作用。</li><li>在每个前向传播层里放置一个mask，决定ReLU/GeLU的结果是否起作用。</li></ol><h5 id="3-2-Determining-Gate-Values"><a href="#3-2-Determining-Gate-Values" class="headerlink" title="3.2 Determining Gate Values"></a>3.2 Determining Gate Values</h5><p>作者研究了四种确定mask gate的值的方法：</p><ol><li>随机：每个$\gamma _i$的取值都从参数为$\alpha$的伯努利分布取出来，$\alpha$是手动调节的，控制稀疏性。</li><li>Gain方法：通过每个$\gamma _i$作为一个连续的参数并计算均值来估计每个$\gamma _i$对整个训练集的似然$L$的影响。</li></ol><p><img src="/img/2022-04-03-Pruning-Notes/1-2.png"></p><p>在每次传递的时候对$g_i$设置阈值，确定保留哪些Transformer。</p><ol><li>L0正则化：$\gamma _i$采样自：</li></ol><p><img src="/img/2022-04-03-Pruning-Notes/1-3.png"></p><p>hard-concrete distribution $hc(α_i)$ (Maddi-son et al., 2017) ，$\alpha _i$是通过优化目标函数$L$来训练的：</p><p><img src="/img/2022-04-03-Pruning-Notes/1-4.png"></p><p>预训练门参数的成本和微调差不多。</p><h5 id="3-3-Structured-Pruning"><a href="#3-3-Structured-Pruning" class="headerlink" title="3.3 Structured Pruning"></a>3.3 Structured Pruning</h5><p>$\gamma _i$通过上述的方法之一确定后，对模型进行剪枝。去掉$\gamma _i ^{attn} = 0$的注意头，和前向传播层中$\gamma _i ^{ff} = 0$的切片。</p><h5 id="3-4-Extended-training"><a href="#3-4-Extended-training" class="headerlink" title="3.4 Extended training"></a>3.4 Extended training</h5><p><img src="/img/2022-04-03-Pruning-Notes/1-5.png"></p><h4 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h4><p>在SQuAD 2.0和NQ数据集上评估了作者提出的方法。</p><p> SQuAD 2.0是一个维基百科文章问题的数据集，是由人类注释者在查看这些维基百科文章时提出的。 NQ是一个谷歌搜索查询的数据集，这些查询的答案来自人类注释者提供的维基百科页面。</p><p>实验想研究几个问题：</p><ol><li>BERT-base的剪枝中学到的技术（超参数等）在BERT-large上适用吗？</li><li>这些技术在不同数据集上是通用的吗？</li><li>合并蒸馏目标是否能改进作者模型的能力？</li></ol><h5 id="4-2-SQuAD-2-0"><a href="#4-2-SQuAD-2-0" class="headerlink" title="4.2 SQuAD 2.0"></a>4.2 SQuAD 2.0</h5><p>作者在自己训练的BERT-qa模型上用不同的方法得到mask的取值，并比较他们的性能：</p><p><img src="/img/2022-04-03-Pruning-Notes/1-6.png"></p><p> “random”的表现突然衰减。“gain”更好。”L0正则化”的效果是最好的，在48%的剪枝率下只减少了5 F1-points。</p><p> 在图2中，我们显示了剪枝后剩余的注意力头和前馈激活的百分比。我们看到中间层保留得更多，而接近嵌入和接近答案的层被修剪得更多：</p><p><img src="/img/2022-04-03-Pruning-Notes/1-7.png"></p><p>表2中的参数在上面给出。</p><p><img src="/img/2022-04-03-Pruning-Notes/1-8.png"></p><h5 id="4-3-Natural-Questions"><a href="#4-3-Natural-Questions" class="headerlink" title="4.3 Natural Questions"></a>4.3 Natural Questions</h5><p>研究三个问题：</p><ol><li>为SQuAD 2.0任务开发的剪枝技术是否适用于NQ任务:4.3.1</li><li>BERT的剪枝是否也适用于RoBERTa？4.3.2</li><li>能否结合蒸馏和剪枝，得到更小、更快的模型？4.3.3</li></ol><p>A1：继续使用上述在SQuAD 2.0上剪枝后的BERT-large模型，并使用相同的模型继续在NQ数据集上继续训练。这个模型记为$retrain(NQ)$。表3表示尽管没得到最好的结果，但也很优秀了。可以认为在剪枝任务里去掉的BERT冗余的参数不是针对特定任务的，而是具有鲁棒性的。</p><p><img src="/img/2022-04-03-Pruning-Notes/1-9.png"></p><p>A2：RoBERTa在各种任务的精度上得到了比BERT更好的结果。RoBERTa和BERT有相同的拓扑结构，只是在标记、训练的过程中略有不同，所以对BERT的剪枝方法对RoBERTa也应该起作用。对RoBERTa-large NQ模型进行剪枝，用了相同的方法，使用L0正则化决定masks/gates的取值，表4展示了结果，表明RoBERTa也可以用这样的技术进行成功的剪枝：</p><p><img src="/img/2022-04-03-Pruning-Notes/1-10.png"></p><p>A3：  将精馏和剪枝相结合的最简单方法是，在模型被剪枝后，将继续训练(retrain(nq))替换为具有蒸馏目标的继续训练(蒸馏(nq))。这里，未修剪的模型充当教师，修剪过的模型充当学生。在表5中，我们显示了仅在继续训练阶段使用蒸馏的结果。</p><p><img src="/img/2022-04-03-Pruning-Notes/1-11.png"></p><p>图3里的base点是RoBERTa-base（表5的e）位于剪枝+蒸馏的左上方，表明剪枝+蒸馏的过程并没有完全实现充分潜力。</p><p><img src="/img/2022-04-03-Pruning-Notes/1-12.png"></p><h3 id="简短总结-你看完这篇论文的总结"><a href="#简短总结-你看完这篇论文的总结" class="headerlink" title="简短总结 # 你看完这篇论文的总结"></a>简短总结 # 你看完这篇论文的总结</h3><p>本文尝试了在Transformer的前向传播层和注意力头层这两个位置上的结构化剪枝策略。</p><p>剪枝的主要方法是：给attn层和ff层都引入一个参数mask，$\gamma _i$，经过L0正则化后去掉该参数为0的对应的层。比较了’随机’、‘gain’、‘L0正则化’后实验结果表明L0正则化得到mask参数的效果最好。</p><p>在后续进行了从SQuAD 2.0到NQ数据集的迁移，和BERT到RoBERTa模型的迁移，探究这种剪枝策略是否有普适性。后续进行了蒸馏和剪枝的结合实验，探索效果如何。</p><p>文章的具体做的事情感觉可以用实验章节提出要探究的三个问题来表示：<a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#iEKFun">Pruning 泛读论文笔记</a> </p><p>得到的结果是在Natural Questions上提高了一倍的速度，只损失了不到0.5的F1分数。</p><h3 id="创新点-这篇论文自己写的贡献"><a href="#创新点-这篇论文自己写的贡献" class="headerlink" title="创新点 # 这篇论文自己写的贡献"></a>创新点 # 这篇论文自己写的贡献</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#LakNRs">1 引言-主要贡献</a></p><h3 id="优劣-论文自己写的和你认为的优劣势（相比其他方法）"><a href="#优劣-论文自己写的和你认为的优劣势（相比其他方法）" class="headerlink" title="优劣 # 论文自己写的和你认为的优劣势（相比其他方法）"></a>优劣 # 论文自己写的和你认为的优劣势（相比其他方法）</h3><ol><li>本文做的实验都是基于机器阅读理解（Machine Reading Comprehension, MRC）这一个任务（感觉不是很全面？），和Distil-BERT和TinyBERT进行了比较。</li><li>本文做了从SQuAD到NQ数据集上的迁移，以探索这样的剪枝策略是否具有普适性。具体的做法是在SQuAD剪枝后的模型（提供mask的参数）直接继续进行NQ的学习和剪枝。觉得想法是合理的，不知道做法是否合理。</li><li>感觉4.3.3的蒸馏+剪枝结合的实验结果图表示，这样的操作至少在MRC任务上是不如RoBERTa-base的，作者说这表明这种方法没有完全实现充分潜力？这是说它这么做不太好吗？因为感觉没提到这样做的好处是什么（推理速度？继续压缩？）这合适吗？</li></ol><h3 id="流程与公式-尽量以图为主，附带必要说明"><a href="#流程与公式-尽量以图为主，附带必要说明" class="headerlink" title="流程与公式 # 尽量以图为主，附带必要说明"></a>流程与公式 # 尽量以图为主，附带必要说明</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#xv4Ahz">3 Pruning Transformers</a></p><h3 id="主要实验-重点是比较的表格"><a href="#主要实验-重点是比较的表格" class="headerlink" title="主要实验 # 重点是比较的表格"></a>主要实验 # 重点是比较的表格</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#iEKFun">4 Experiments</a></p><h2 id="2-Reweighted-Proximal-Pruning-for-Large-Scale-Language-Representation"><a href="#2-Reweighted-Proximal-Pruning-for-Large-Scale-Language-Representation" class="headerlink" title="2 - Reweighted Proximal Pruning for Large-Scale Language Representation"></a>2 - Reweighted Proximal Pruning for Large-Scale Language Representation</h2><h3 id="论文内容-1"><a href="#论文内容-1" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h4><p>这篇文章提出了一种新的剪枝方法Reweighted  Proximal Pruning  (RPP)，该剪枝方法是专为大规模语言表示模型设计的。</p><p>在SQuAD和GLUE数据集上的实验表明，高剪枝率下的BERT仍然在预训练任务和下游微调任务中有着很好的准确率。</p><p>RPP提供了一个新的视角来帮助分析大规模语言表示可能学到的东西。此外，RPP使得在一系列不同的设备（手机、边缘设备）上部署大型的语言表示模型(如BERT)成为可能。</p><h4 id="5-结论-1"><a href="#5-结论-1" class="headerlink" title="5 结论"></a>5 结论</h4><p>本文提出了一种RPP剪枝算法，该算法在大型预训练语言表示模型BERT上获得了第一个有效的权值剪枝结果。</p><p>RPP在不影响训练前和微调任务性能的情况下，获得了59.3%的权重稀疏。</p><p>作者重点研究了预训练DNN模型的剪枝率与下游多任务迁移学习目标的性能之间的关系。我们发现，除了SQuAD之外，许多下游任务允许至少80%的修剪率，而任务SQuAD下的修剪率为59.3%。我们提出的RPP方法为分析大型语言表示模型提供了一个新的视角。</p><h4 id="1-引言-1"><a href="#1-引言-1" class="headerlink" title="1 引言"></a>1 引言</h4><p>大的预训练语言表示模型效果很好，但是太大了，很难部署到计算受限的设备中。本文主要探索两个问题：</p><p>Q1： 是否有可能通过权重剪枝来压缩大规模的语言表示，如BERT？</p><p>Q2： 权重修剪、预训练模型如何影响下游多任务迁移学习目标的表现？</p><p>之前的方法有非结构剪枝（non-structured weight pruning）、结构化（structured）剪枝、过滤（filter）剪枝和通道（channel）剪枝。和CNN类型的模型剪枝不同的是，BERT这样的模型不仅要考虑预训练模型的剪枝效果，还要关注下游微调任务的迁移学习能力。</p><p>本文研究了一些不规则的权重剪枝在BERT上的表现，如迭代剪枝（iterative pruning）和一次剪枝（one-shot pruning）。但是发现这些方法无法在收敛的同时保持精度不显著下降。作者认为这种剪枝方法的失败是因为基于l1正则化和l2正则化学习到的稀疏模式不正确。</p><p>作者在这项工作中提出了Reweighted Proximal Pruning（RPP），由两部分组成：Reweighted l1最小化和近端算子（proximal operator）。重加权l1最小化是比l1正则化更好的稀疏性生成方法。通过近端算子可以使求解稀疏模式（sparsity pattern）和求解训练梯度的过程解耦。</p><p>这样第一次得到了在BERT上有效的权值剪枝方法。实验结果表明，在包括SQuAD和GLUE在内的各种下游任务中，都保持了较高的精度。</p><h5 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h5><p>作者总结的贡献有以下几点：</p><ol><li>开发了一种剪枝算法RPP，在大型预训练模型BERT上获得了第一个有效的权值剪枝结果。在不影响预训练和下游任务的情况下，获得了59.3%的权重稀疏。</li><li>重点研究了预训练DNN模型在下游迁移任务下的性能区别。发现除了SQuAD之外，许多下游任务允许至少80%的剪枝率，SQuAD只有59.3%。</li><li>观察到随着预训练模型的剪枝率增加，下游任务的性能下降。在不同的任务中下降的范围不同，但是比起基于迭代剪枝的方法，RPP方法都能达到高的剪枝率。</li><li>和图像分类任务重的剪枝不同，RPP有助于发现BERT中结构化稀疏模式。此外还研究了网络剪枝对BERT中嵌入语言表示的影响。</li></ol><h4 id="2-相关工作-1"><a href="#2-相关工作-1" class="headerlink" title="2 相关工作"></a>2 相关工作</h4><p>介绍了BERT和对多头注意力机制中的一些头进行mask的工作，和作者的工作正交，可以结合起来进一步压缩/加速。</p><p>介绍了Reweighted l1和近端算法（proximal algorithm），本文应该是第一次把重加权l1最小化应用到网络压缩，特别是BERT剪枝的工作。</p><h4 id="3-RPP-during-预训练-for-大体量语言模型"><a href="#3-RPP-during-预训练-for-大体量语言模型" class="headerlink" title="3 RPP during 预训练 for 大体量语言模型"></a>3 RPP during 预训练 for 大体量语言模型</h4><p>$f_i$表示下游任务中$T(i)\sim p(T)$的损失函数，$p(T)$表示任务的分布。设$w$表示预训练模型的参数，$z_i$表示第$i$个任务指定模型的参数。每个下游任务有单独的微调模型，从预训练模型开始微调，可表示为：</p><p><img src="/img/2022-04-03-Pruning-Notes/2-1.png"></p><h5 id="3-1-迁移学习里的剪枝公式化表示"><a href="#3-1-迁移学习里的剪枝公式化表示" class="headerlink" title="3.1 迁移学习里的剪枝公式化表示"></a>3.1 迁移学习里的剪枝公式化表示</h5><p>首先考虑常规的权值剪枝公式，在训练前的权值剪枝问题：</p><p><img src="/img/2022-04-03-Pruning-Notes/2-2.png"></p><p>$f_0$表示剪枝的损失函数，$p \in (0, 1)$表示选用哪种正则化，$\gamma $表示正则项</p><p><img src="/img/2022-04-03-Pruning-Notes/2-3.png"></p><p>$S_{\hat{w}}$是对公式2求解出来的稀疏模式。</p><p>对每个子任务$i$，允许一个额外的微调步骤去训练权值，从$\hat{w}$开始，遵循确定的稀疏模式$S_{\hat{w}}$，即修正后的公式1：</p><p><img src="/img/2022-04-03-Pruning-Notes/2-4.png"></p><p>目标是在预训练模型中找出一个稀疏模型，权值集合为$\hat{w}$，在每个子任务上的表现和原来的预训练模型一样好。共享同一个稀疏性$S_{\hat{w}}$。</p><h5 id="3-2-RPP"><a href="#3-2-RPP" class="headerlink" title="3.2 RPP"></a>3.2 RPP</h5><p>两部分：1. 重加权l1最小化，更好的求解稀疏模式的方法。2. 近似算子，把计算梯度和求稀疏模式分开，不需要每次都在整个sparsity-penalized loss上求解梯度。</p><h6 id="3-2-1-重加权l1最小化"><a href="#3-2-1-重加权l1最小化" class="headerlink" title="3.2.1 重加权l1最小化"></a>3.2.1 重加权l1最小化</h6><p><img src="/img/2022-04-03-Pruning-Notes/2-5.png"></p><p>$\alpha _i$表示平衡惩罚的因子。如果$T=1$就退化成了l1稀疏训练。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-6.png"></p><p>相比直接l1正则化，这里的公式5会被多次用于求解，迭代去计算更好的$w$和$\alpha$取值，“Reweighted”。</p><h6 id="3-2-2-近端算子"><a href="#3-2-2-近端算子" class="headerlink" title="3.2.2 近端算子"></a>3.2.2 近端算子</h6><p> 近端算法显示，在一组广泛的非凸优化问题上是非常有效的(与原始解相比)。此外，我们提出的重加权’ 1最小化(5)通过近端算子具有解析解。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-7.png"></p><p>带近端算子的AdamW优化器算法见附录C的算法3。</p><p>为什么选择AdamW而不是Adam？传统的权值衰减在Adam中本质上是无效的，并且对基于梯度的更新有负面影响，这也是自适应梯度算法难以在NLU应用的超深度DNN训练的原因。AdamW是基于Adam通过将权值衰减正则化和基于梯度的更新解耦得到的，避免了过拟合，广泛应用于大型预训练语言模型。RPP的设计思想也受到这个的启发。</p><h4 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h4><p>作者做的不包括RPP部分的工作baseline：NIP（New Iterative Pruning）能够剪枝BERT，在附录A中。用作对比。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-8.png"></p><p>RPP算法的具体流程，附录B。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-9.png"></p><p>比较了NIP和RPP两种baseline下的MLM和NSP任务（预训练）的准确性和剪枝率。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-10.png"></p><p>在BERT-large上比较了NIP和RPP两个baseline在不同迁移任务上的表现：</p><p><img src="/img/2022-04-03-Pruning-Notes/2-11.png"></p><h5 id="4-3-对BERT的注意力模式可视化"><a href="#4-3-对BERT的注意力模式可视化" class="headerlink" title="4.3 对BERT的注意力模式可视化"></a>4.3 对BERT的注意力模式可视化</h5><p>剪枝后的BERT-base模型中取样6个矩阵，进行比较：</p><p><img src="/img/2022-04-03-Pruning-Notes/2-12.png"></p><p>Structured Pattern：作者表示剪枝后的Transformer产生了有趣的群结构（group-wise structures）。和图像分类器上的不规则剪枝不同，这体现了语言模型剪枝的特殊性。作者认为重加权l1方法对找到这些细粒度的稀疏模式很重要。</p><p>Semantic interpretation（语义解释）：剪枝后的Transformer学到了什么，查询矩阵Q对每个序列内部的注意信息进行建模，Key矩阵K主要对上下文的信息进行建模。</p><h5 id="4-4-t-SNE可视化"><a href="#4-4-t-SNE可视化" class="headerlink" title="4.4 t-SNE可视化"></a>4.4 t-SNE可视化</h5><p>使用t-SNE可视化比较了剪枝前后的BERT对语言表示的区别。 使用具体的单词”intelligent”为例，RPP的应用保留了与原始BERT相似的大部分语言表示信息。</p><p><img src="/img/2022-04-03-Pruning-Notes/2-13.png"></p><h3 id="简短总结"><a href="#简短总结" class="headerlink" title="简短总结"></a>简短总结</h3><p>这是首篇对BERT进行了有效的权值修建的剪枝方法，RPP。</p><p>RPP由两个重要部分组成：</p><ol><li>重加权l1最小化，替换了l0、l1正则化，使得在BERT这样的大模型上能进行有效地剪枝（精度不会显著下降），是求解稀疏性更好的办法。</li><li>近端算子。通过近端算子可以使求解稀疏模式（sparsity pattern）和求解训练梯度的过程解耦。</li></ol><p>作者比较了RPP和没使用RPP的工作baseline，NIP。得出RPP在相同剪枝率下在预训练和各种下游任务上都有更好的表现，表示这是一个更有效的剪枝方法。</p><p>并且通过剪枝后的矩阵取样和t-SNE可视化分析了BERT这种语言表示模型可能学到的东西。</p><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#FagkUf">1 引言 - 贡献</a></p><h3 id="优劣"><a href="#优劣" class="headerlink" title="优劣"></a>优劣</h3><ol><li>是第一个能有效剪枝BERT这样的预训练大语言表征模型的方法。</li><li>在得到剪枝结果后，对BERT这样的语言模型可能学到的东西给出了一定的讨论。</li></ol><h3 id="流程与公式"><a href="#流程与公式" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#qxoT6A">3 RPP具体内容</a></p><h3 id="主要实验"><a href="#主要实验" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#uFq2bv"> 4 主要实验</a></p><h2 id="3-Masking-as-an-Efficient-Alternative-to-Finetuning-for-Pretrained-Language-Models"><a href="#3-Masking-as-an-Efficient-Alternative-to-Finetuning-for-Pretrained-Language-Models" class="headerlink" title="3 - Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"></a>3 - Masking as an Efficient Alternative to Finetuning for Pretrained Language Models</h2><h3 id="论文内容-2"><a href="#论文内容-2" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h4><p>我们提出了一种利用预训练语言模型的有效方法，通过为预训练的模型权重学习可选的二进制掩码（selective binary masks），而不是通过微调权重修改它们。</p><p>在11个不同的NLP任务上对掩蔽BERT、RoBERTa和DistilBERT进行了评估，结果表明，掩蔽方案可以获得与微调相当的性能，然而在多个任务需要推断时，内存占用要小得多。</p><p>内在评估（Intrinsic evaluations）表明，二进制掩码语言模型计算的语言表示模型编码包含了解决下游任务所必需的信息。</p><p>分析loss的情况，发现通过掩蔽和微调的模型都能达到几乎一样的精度。这证实了掩蔽也可以作为替代微调的一种有效方案。</p><h4 id="7-结论"><a href="#7-结论" class="headerlink" title="7 结论"></a>7 结论</h4><p>作者提出掩蔽masking这样的方法去替代微调去做下游任务。在BERT/RoBERTa/DistilBERT这样的预训练模型上都可用。每个任务不会修改参数，而是只训练一些二进制掩码来选择关键参数进行保留。大量实验表明在NLP各种任务上掩蔽和微调的性能相当。但是在需要解决多个任务的时候，掩蔽可以在不改变预训练参数的情况下提高内存效率。</p><p>代码在<a href="https://github.com/ptlmasking/maskbert">https://github.com/ptlmasking/maskbert</a></p><h4 id="1-引言-2"><a href="#1-引言-2" class="headerlink" title="1 引言"></a>1 引言</h4><p>微调简单，性能好，但是需要调优的参数太多，比如BERT-large中就有3.4亿个参数。这是这些模型广泛部署的主要障碍。微调的模型会占用更大内存，需要解决多个任务的时候，需要保存几个大体积的微调模型进行推断。</p><p>最近的工作指出了固定模型中搜索神经结构的潜力，代替微调调优的步骤。受这些启发，提出了掩蔽，在预训练上的模型进行训练，选择对下游任务重要的权值，丢弃不相关的权值。</p><p>取得了相当的性能。在参数上更高效，只需要保存一组1位的二进制掩码，而不需要保存所有的32位浮点参数，这样能在边缘设备上解决多个任务。</p><h5 id="贡献-1"><a href="#贡献-1" class="headerlink" title="贡献"></a>贡献</h5><ol><li>引入了掩蔽这种利用预训练语言模型的新方案，可以替代微调。性能相当。</li><li>对掩蔽进行了实证分析，揭示了在11个不同NLP任务中取得良好表现的关键因素。</li><li>研究了loss的情况，揭示了为什么掩蔽具有和微调相当的性能的潜在原因。</li></ol><h4 id="3-方法"><a href="#3-方法" class="headerlink" title="3 方法"></a>3 方法</h4><h5 id="3-1-Transformer和微调的背景介绍"><a href="#3-1-Transformer和微调的背景介绍" class="headerlink" title="3.1 Transformer和微调的背景介绍"></a>3.1 Transformer和微调的背景介绍</h5><p><img src="/img/2022-04-03-Pruning-Notes/3-1.png"></p><p>输入的句子是$X\in R^{N\times d}$，$N$是最大的句子长度，$d$是隐含维的大小，$W_K, W_Q, W_V$用来计算X的转换。$X$的自注意力通过以下计算：</p><p><img src="/img/2022-04-03-Pruning-Notes/3-2.png"></p><p>当对下游任务进行微调的时候，将随机初始化一个线性分类器层$W_T$进行投影。放在预先训练的线性层$W_P$(pooler)上更新所有参数，最小化cross-entropy。</p><h5 id="3-2-Learning-the-mask"><a href="#3-2-Learning-the-mask" class="headerlink" title="3.2 Learning the mask"></a>3.2 Learning the mask</h5><p>将每个线性层$W^l \in \lbrace W^l_K, W^l_Q, W^l_V, W^l_{AO}, W^l_O, W^l_I \rbrace$都连接到一个实数矩阵$M^l$，该矩阵从均匀分布中初始化，和每个线性层有同样的大小，然后通过一个基于元素的阈值（二值化器）为$W^l$获得一个二进制掩码$M^l_{bin}$。</p><p><img src="/img/2022-04-03-Pruning-Notes/3-3.png"></p><p>每次前向训练的时候，二值掩码通过Hadamard积（两个相同的矩阵对应位置相乘）求得：</p><p><img src="/img/2022-04-03-Pruning-Notes/3-4.png"></p><p>在后向训练中，不能直接通过loss更新二值化，使用噪声估计量来更新$M^l$</p><p><img src="/img/2022-04-03-Pruning-Notes/3-5.png"></p><p>通过公式1得到一个屏蔽线性层$\hat{W^l}$，再随机初始化一个相关的$M^l$作为一个额外的线性层，放在预训练模型上面。在训练过程中通过公式2更新每个$M^l$。</p><p>在训练结束后把每个$M^L_{bin}$保存下来以供以后推理，与微调后的参数相比，仅需要3%的内存。嵌入层也不必被隐藏， 这会进一步减少内存消耗。</p><p>作者主要提到了$M^l_{bin}$初始化的稀疏性和选择哪些层去mask会造成影响，在第5节中展开讲。</p><h4 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h4><h5 id="5-1-初始稀疏度的影响"><a href="#5-1-初始稀疏度的影响" class="headerlink" title="5.1 初始稀疏度的影响"></a>5.1 初始稀疏度的影响</h5><p>在四个任务上分别使用（1%,3%,5%,10%,…95%）初始稀疏度进行实验。别的超参数相同。每个实验都用不同的随机种子重复4次。</p><p><img src="/img/2022-04-03-Pruning-Notes/3-6.png"></p><p>结论：1. 在初始稀疏度较大，去除了大部分预训练参数的情况下会使性能较差，是因为预训练的知识都被丢掉了。2. 逐步降低稀疏度，通常在3%-10%之间会产生合理的结果。在更大的数据集上更不敏感一些。3. 选择几乎所有的参数，会影响性能。初始化保留过多参数会阻碍优化。</p><h5 id="5-2-每层做了什么-Layer-wise-behaviors"><a href="#5-2-每层做了什么-Layer-wise-behaviors" class="headerlink" title="5.2 每层做了什么 Layer-wise behaviors"></a>5.2 每层做了什么 Layer-wise behaviors</h5><p>神经网络层里，句法信息在底层表示得更好，语义信息在高层表示得更好。因此简单屏蔽所有的Transformer blocks可能是不理想的。</p><p>研究了mask应用于不同层的任务性能。分别探究了从下往上和从上往下的。</p><p><img src="/img/2022-04-03-Pruning-Notes/3-7.png"></p><p>观察到：</p><ol><li> 大多数情况下自上而下的掩码优于自下而上的掩码。所以在低层次中选择所有预训练的权值是合理的。</li><li> 于自底向上掩蔽，增加掩蔽层数可以逐渐提高性能。</li><li> 在自顶向下掩蔽中，随着掩蔽层数的增加，CoLA性能提高，而MRPC和RTE不敏感。回想一下，CoLA测试的语言可接受性通常需要句法和语义信息。所有的BERT层都涉及到表示这个信息，因此允许更多的层进行更改应该会提高性能。</li></ol><h5 id="5-3-比较掩蔽和微调"><a href="#5-3-比较掩蔽和微调" class="headerlink" title="5.3 比较掩蔽和微调"></a>5.3 比较掩蔽和微调</h5><p><strong>性能比较</strong>。表1报告了11个NLP任务在开发集中屏蔽和微调的性能。我们观察到，将掩蔽应用于BERT/RoBERTa/DistilBERT可以获得与微调相当的性能。</p><p><img src="/img/2022-04-03-Pruning-Notes/3-8.png"></p><p><strong>内存比较</strong>。已经证明了掩蔽和微调的任务性能是可以比较的，接下来我们将演示掩蔽的一个关键强度:内存效率。</p><p><img src="/img/2022-04-03-Pruning-Notes/3-9.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/3-10.png"></p><h3 id="简短总结-1"><a href="#简短总结-1" class="headerlink" title="简短总结"></a>简短总结</h3><p>用训练掩蔽的参数去替代了微调的过程。这样就可以在相同的预训练模型上引入不同的参数去做不同的任务，对内存的利用效率会高得多。</p><p>在需要解决多个任务的时候，掩蔽可以在不改变预训练参数的情况下提高内存效率。</p><h3 id="创新点-1"><a href="#创新点-1" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#ZdX7oS"> 1 引言-贡献</a></p><h3 id="优劣-1"><a href="#优劣-1" class="headerlink" title="优劣"></a>优劣</h3><ol><li>我觉得mask的训练方式能有效可能是因为BERT的参数太多了，尤其是相比特定任务的输出数量上来说。其中去掉一些可能会造成负面影响的效率确实会有用。觉得这篇文章比较有创意性。</li><li>给掩蔽相比微调在性能以外找到了另外一个这么做的好处——内存效率。</li></ol><h3 id="流程与公式-1"><a href="#流程与公式-1" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#Oz9FK7">3 方法</a></p><h3 id="主要实验-1"><a href="#主要实验-1" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#JhBT6g">5 实验</a></p><h2 id="4-Compressing-BERT-Studying-the-Effects-of-Weight-Pruning-on-Transfer-Learning"><a href="#4-Compressing-BERT-Studying-the-Effects-of-Weight-Pruning-on-Transfer-Learning" class="headerlink" title="4 - Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"></a>4 - Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning</h2><h3 id="论文内容-3"><a href="#论文内容-3" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h4><p>探索BERT的权值剪枝，并提出了一个问题：预训练时进行的压缩会如何影响后续的迁移学习？</p><p>我们发现，剪枝在三个不同程度上对迁移学习的影响。(1)低程度的剪枝(30-40%)根本不会影响预训练的loss，也根本不会影响到下游任务。(2)中等程度的剪枝会增加预训练的损失，并阻止有用的预训练信息传递到下游任务。(3)高程度的剪枝还会阻止模型拟合下游数据集，导致进一步的退化。</p><p>观察到对特定任务进行微调BERT并不能提高其可被剪枝的能力。我们的结论是，BERT可以在预训练的时候进行剪枝，而不需要对每个任务单独进行剪枝，因为这样不会影响性能。</p><h4 id="7-结论-1"><a href="#7-结论-1" class="headerlink" title="7 结论"></a>7 结论</h4><p>于量级权值修剪，我们已经证明了30-40%的权值不编码任何有用的归纳偏差，可以丢弃而不影响BERT的普适性。 其余权重的相关性因任务的不同而不同，对下游任务的微调不会通过改变被修剪的权重来改变这种权衡的性质。</p><h4 id="1-引言-3"><a href="#1-引言-3" class="headerlink" title="1 引言"></a>1 引言</h4><p> 预训练的模型通常比单独在下游数据上训练的模型具有更高的准确性。 训练前范式虽然有效，但仍存在一些问题。虽然有人声称语言模型前训练是一项“通用的语言学习任务”，但这并没有理论依据，只有经验证据。</p><p>从头开始训练BERT-Base需要花费约7000美元，并排放约1438磅二氧化碳。</p><p>本文研究的主要问题：压缩BERT是否会影响其向新任务的迁移能力?微调是否使BERT更容易压缩?</p><h5 id="贡献-2"><a href="#贡献-2" class="headerlink" title="贡献"></a>贡献</h5><p>我们的研究结果如下:</p><ol><li>低水平的修剪(30-40%)根本不会增加训练前的损失或影响向下游任务的转移。中等水平的剪枝增加了训练前的损失，并阻止有用的训练前信息传递到下游任务。这一信息对每一项任务并不同样有用;任务随着训练前损耗的增加而线性下降，但速率不同。</li><li>根据下游数据集的大小进行高水平的修剪，可能会阻止模型拟合下游数据集，从而进一步降低性能。 </li><li>最后，我们观察到，对特定任务的BERT进行微调并不会提高其剪枝能力，也不会显著地改变剪枝顺序。</li></ol><p>据我们所知，之前的工作还没有表明BERT是否可以以任务通用的方式压缩，既保留了训练前的好处，又避免了多次压缩和重新训练BERT的昂贵实验。</p><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h4><p>基于公共BERT库进行了BERT剪枝实验，代码在<a href="https://github.com/mitchellgordon95/bert-prune">https://github.com/mitchellgordon95/bert-prune</a></p><p>我们对一个训练好的BERT-Base模型进行了权值修剪我们以10%的增量从0%到90%选择稀疏性，并在训练的前10k步中逐渐将BERT修剪成这种稀疏性。 图1中表示了不同方式的剪枝在各个剪枝比例下的准确度。</p><p>介绍了实验中BERT的参数和数据集等内容。</p><p><img src="/img/2022-04-03-Pruning-Notes/4-1.png"></p><h4 id="4-剪枝方法"><a href="#4-剪枝方法" class="headerlink" title="4 剪枝方法"></a>4 剪枝方法</h4><h5 id="4-1-30-40-的剪枝"><a href="#4-1-30-40-的剪枝" class="headerlink" title="4.1 30-40%的剪枝"></a>4.1 30-40%的剪枝</h5><p>图1显示，通过量级权重修剪后的前30-40%权重不影响任何下游任务的训练前损失或推理。这些权重可以在微调之前或之后进行修剪。 从稀疏架构搜索的剪枝角度来看，这是有意义的:当我们初始化BERT-Base时，我们初始化了许多可能的子网。SGD选择最佳的一个进行训练前训练，并将剩余的权重推到0。然后我们可以在不影响网络输出的情况下删除这些权值。</p><h5 id="4-2-中等级别的剪枝"><a href="#4-2-中等级别的剪枝" class="headerlink" title="4.2 中等级别的剪枝"></a>4.2 中等级别的剪枝</h5><p> 超过40%的修剪，性能开始下降。当我们修剪拟合训练前数据所需的权值时，训练前的损失就会增加(表1)。 隐藏层的特征激活开始偏离低修剪水平的模型(图2)在这一点上，下游精度也开始下降。</p><p><img src="/img/2022-04-03-Pruning-Notes/4-2.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/4-3.png"></p><p>可能影响下游任务的原因：</p><ol><li>pruning通过将权值设为0来删除训练前的信息，防止训练前学习到的有用的归纳偏差的转移。</li><li>剪枝通过保持某些权值为零来正则化模型，这可能会妨碍下游数据集的拟合。</li></ol><h5 id="4-3-高级别的剪枝"><a href="#4-3-高级别的剪枝" class="headerlink" title="4.3 高级别的剪枝"></a>4.3 高级别的剪枝</h5><p>当稀疏度达到70%及以上时，带有信息删除的模型恢复了一些精度，所以说明40-60%剪枝后的模型复杂度是导致性能下降的第二个原因。 表1可以看出，对于训练数据量最大的MNLI和QQP任务，信息删除比剪枝效果更好。 相比之下，SST-2和CoLA的模型恢复得不太好，因为它们的数据较少。</p><h5 id="4-4-分析被剪枝掉的BERT部分的价值"><a href="#4-4-分析被剪枝掉的BERT部分的价值" class="headerlink" title="4.4 分析被剪枝掉的BERT部分的价值"></a>4.4 分析被剪枝掉的BERT部分的价值</h5><p> 我们已经看到，过度剪枝的BERT会删除对下游任务有用的信息。图2中表示删掉的部分会线性影响预测的效果。</p><p>作者针对不同任务讨论了BERT每个部分的影响。</p><h4 id="5-下游微调并不会提高可剪枝性"><a href="#5-下游微调并不会提高可剪枝性" class="headerlink" title="5 下游微调并不会提高可剪枝性"></a>5 下游微调并不会提高可剪枝性</h4><p>图4显示，即使经过了下游微调的一段时间后，权值也会迅速地以新的排序顺序重新稳定下来，这意味着较长的下游训练对权值被修剪的影响很小。</p><p><img src="/img/2022-04-03-Pruning-Notes/4-4.png"></p><h3 id="简短总结-2"><a href="#简短总结-2" class="headerlink" title="简短总结"></a>简短总结</h3><p>本文主要研究的是，在预训练阶段进行剪枝，是否会影响到下游任务的效果？</p><p>作者给出的实验结果表示答案是否，也就是说只要在预训练模型剪枝好后再进行特定任务的微调即可，不会导致效果变得更差。</p><p>以及给出了不同的剪枝程度下进行实验的结果和讨论，分析造成这种结果的原因可能是什么。</p><h3 id="创新点-2"><a href="#创新点-2" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#TCpG2z">1 引言-贡献</a></p><h3 id="优劣-2"><a href="#优劣-2" class="headerlink" title="优劣"></a>优劣</h3><ol><li>感觉是实验的总结和分析，给出的结论是预训练剪枝和下游微调的关系影响，主要在描述这种实验中观察到的现象。</li><li>把剪枝的方法按照剪枝的比例分成了低中高三个阶段，从而得到高程度剪枝的效果比中程度高一点是因为中程度剪枝的复杂性影响了结果，这个结论是不是有点不太有道理。</li></ol><h3 id="流程与公式-2"><a href="#流程与公式-2" class="headerlink" title="流程与公式"></a>流程与公式</h3><h3 id="主要实验-2"><a href="#主要实验-2" class="headerlink" title="主要实验"></a>主要实验</h3><p> <a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#STmDaj">3 实验</a></p><h2 id="5-When-BERT-Plays-the-Lottery-All-Tickets-Are-Winning"><a href="#5-When-BERT-Plays-the-Lottery-All-Tickets-Are-Winning" class="headerlink" title="5 - When BERT Plays the Lottery, All Tickets Are Winning"></a>5 - When BERT Plays the Lottery, All Tickets Are Winning</h2><h3 id="论文内容-4"><a href="#论文内容-4" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-4"><a href="#摘要-4" class="headerlink" title="摘要"></a>摘要</h4><p>基于Transformer的大型模型被证明可以简化为更小数量的自注意头和层。我们从彩票假设的角度来考虑这一现象，使用了结构化（structured）和量化（magnitude）剪枝。</p><p>对于微调后的BERT，我们发现(a)有可能找到性能与完整模型相当的子网络，(b)从模型其余部分取样的类似大小的子网络性能更差。</p><p>使用结构化修剪，即使是最坏的子网络也仍然是高度可训练的，这表明大多数预先训练的BERT权值是潜在有用的。我们也研究了好的子网络，看看它们的成功是否可以归因于卓越的语言知识，但发现它们不稳定，不能用有意义的自我注意力机制来解释。</p><h4 id="7-结论-2"><a href="#7-结论-2" class="headerlink" title="7 结论"></a>7 结论</h4><p>本研究使用了基于量化的和基于重要性的剪枝方法对BERT微调中的彩票假设进行验证。 对于这两种方法，我们发现只有经过修剪的“好”子网才能达到与完整模型相当的性能，而“坏”子网则不能。 然而，对于结构化剪枝，即使是“坏”的子网也可以分别微调以达到相当强的性能。这说明好的子网在微调中是不稳定的，所以不能把“好”归功于自注意力机制。</p><p> 这表明，大多数预先训练的BERT在微调方面可能是有用的，它的成功可能更多地与优化表面有关，而不是特定的语言知识。</p><h4 id="1-引言-4"><a href="#1-引言-4" class="headerlink" title="1 引言"></a>1 引言</h4><p> 我们从彩票假设的角度，对GLUE任务上的BERT微调进行了系统的案例研究。我们对BERT自注意头的基于量化的权重剪枝和基于重要性的剪枝进行了实验和比较，我们将其扩展到BERT中的多层感知器(MLPs)。</p><h5 id="贡献-3"><a href="#贡献-3" class="headerlink" title="贡献"></a>贡献</h5><p>发现：</p><ol><li>使用这两种技术，我们发现“好”的子网达到了全模型性能的90%，并且比从模型其他部分取样的类似规模的子网性能好得多。</li><li>在许多情况下，即使是“坏”的子网也可以重新初始化为预训练的BERT权值，并分别进行微调以获得较强的性能。</li><li>在微调的随机初始化中，“好的”网络是不稳定的，而且它们的自我注意头并不一定编码有意义的语言模式。</li></ol><h4 id="2-相关工作-2"><a href="#2-相关工作-2" class="headerlink" title="2 相关工作"></a>2 相关工作</h4><p>彩票假说：Lottery Ticket Hypothesis</p><p>“密集的、随机初始化的前馈网络包含子网络(中奖彩票)，在单独训练时，这些子网络在类似次数的迭代中达到与原始网络相当的测试精度”。 然而，到目前为止，LTH的工作主要集中在“获胜的”随机初始化。</p><h4 id="3-方法论"><a href="#3-方法论" class="headerlink" title="3 方法论"></a>3 方法论</h4><p>实验都基于Transformer库中的BERT-base lowercase上完成的。 它对9个GLUE任务进行了微调，并使用表1所示的指标进行了评估。所有的评估都是在开发集上完成的，因为测试集不是公开发布的。每个实验我们随机测试5个种子。</p><p> BERT基本上是transformer编码器层堆起来的。每个层都有一个多头注意力机制（a multi-head self-attention，MHAtt），后面跟着一个MLP，每个省周围都有残差连接。</p><p>每个MHAtt都由一堆确定的参数决定。 MHAtt是输入x的每个头的输出之和:</p><p><img src="/img/2022-04-03-Pruning-Notes/5-1.png"></p><h5 id="3-1-量化剪枝"><a href="#3-1-量化剪枝" class="headerlink" title="3.1 量化剪枝"></a>3.1 量化剪枝</h5><p>对于量级修剪，我们对每个任务的BERT进行微调，并在整个模型中迭代地修剪10%的最小量级权重(不包括嵌入，因为本工作的重点是BERT的权重)。我们在每次迭代中检查开发集的分数，只要性能保持在完整的微调模型性能的90%以上，我们就会继续剪枝。</p><h5 id="3-2-结构化剪枝"><a href="#3-2-结构化剪枝" class="headerlink" title="3.2 结构化剪枝"></a>3.2 结构化剪枝</h5><p>研究了BERT结构块的结构化剪枝，并在约束条件下对其进行了掩模处理。 我们在一次向后传递中计算head和MLP的重要性分数，删除10%的head和一个得分最小的MLP，直到开发集的性能在90%以内。 </p><h4 id="4-BERT-Plays-the-Lottery"><a href="#4-BERT-Plays-the-Lottery" class="headerlink" title="4 BERT Plays the Lottery"></a>4 BERT Plays the Lottery</h4><p>我们将大小剪枝和结构剪枝分别称为m-剪枝和s-剪枝。 图1显示了QNLI的“好”子网的热图，即在修剪后保留90%的全模型性能的子网。</p><p><img src="/img/2022-04-03-Pruning-Notes/5-2.png"></p><p>对于s-pruning，我们展示了一个给定的头/MLP在剪枝中幸存下来的随机初始化的数量。对于m-pruning，我们计算了所有GLUE任务(不包括嵌入)中BERT头和mlp中存活权值的百分比。</p><p>图1a显示，在m-pruning中，所有的架构块都丢失了大约一半的权值(42-57%的权值)，但较早的层被修剪得更多。使用s-pruning(图1b)，最重要的头部往往在较早和中间层，而重要的mlp则更多地在中间。请注意，Liu等人(2019)还发现，中间的Transformer层是最可转移的。</p><p>图1b，将头部和mlp修剪在一起。当它们被单独修剪时，总体模式是相似的。当它们被单独修剪时，保留的头(或MLPs)更少(头为49%比22%，MLPs为75%比50%)，但将它们一起修剪总体上更高效(即产生更小的子网)。 <strong>这个实验暗示了BERT的自注意头和mlp之间的相当大的相互作用:随着可用的mlp减少，模型被迫更多地依赖自注意头，从而提高它们的重要性。</strong></p><h5 id="4-2-在BERT微调上测试LTH，彩票假说"><a href="#4-2-在BERT微调上测试LTH，彩票假说" class="headerlink" title="4.2 在BERT微调上测试LTH，彩票假说"></a>4.2 在BERT微调上测试LTH，彩票假说</h5><p>LTH预测，从零开始训练的“良好”子网应该能够匹配完整的网络性能。我们使用以下设置进行实验:</p><ol><li> “好”子网:通过两种技术从完整模型中选择的元素;</li><li> 随机子网:与“好”子网大小相同，但元素是随机从完整模型中取样的;</li><li> “坏”子网:从那些没有通过修剪的子网中取样的元素，加上一个剩余元素的随机样本，以匹配“好”子网的大小。</li></ol><p>选择这些子网络是为了很好地处理这些特定的数据，而相应的“坏”子网络仅与“好”子网络相关。因此，我们不期望这些子网络可以推广到其他数据，并且相信它们可以最好地说明BERT在微调中“学到”的东西。</p><p>每个子网络类型的性能如图2所示。主要的LTH预测得到了验证:“好的”子网可以单独成功地进行再训练。观察到的m-pruning和s-pruning的区别如下：</p><ol><li> 对于9个任务中的7个任务，m-pruning比s-pruning能获得更高的压缩(比s-pruning多10-15%的权重)。</li><li> 虽然m-pruned子网更小，但它们大多能达到全网络性能。对于s-剪枝，“良好”的子网大多略低于完整的网络性能。</li><li> 可以预期随机抽样的子网的性能比“坏”的好，但比“好”的差。这是m-pruning的情况，但对于s-pruning，它们的性能大多与“好”子网相当，这表明随机样本中的“好”头/MLPs子集足以达到完全的“好”子网性能。</li></ol><p><img src="/img/2022-04-03-Pruning-Notes/5-3.png"></p><h5 id="4-3-坏的子网有多坏？"><a href="#4-3-坏的子网有多坏？" class="headerlink" title="4.3 坏的子网有多坏？"></a>4.3 坏的子网有多坏？</h5><p> 表2显示了使用这两种方法进行修剪和重新微调的“坏”子网的结果，以及Wang等人(2018)对三个GLUE基线的开发集结果。在6/9的任务中，经过m-修剪的“坏”子网络比经过s-修剪的子网络至少落后5个点，在相关任务(CoLA和STS-B)上尤其糟糕。 </p><p><img src="/img/2022-04-03-Pruning-Notes/5-4.png"></p><p>我们随机初始化BERT，并应用随机s剪枝掩码，以使其与s剪枝的“坏”子网保持相同的大小。显然，即使这个模型在原则上是可以训练的(仍然超过大多数类的基线)，但平均来说，它比预先训练的权重要高出15个点。这表明即使是最糟糕的6GLUE排行榜也会使用宏观平均指标来对参与系统进行排名。我们只考虑表1中的指标来获得这个平均值。对于给定的任务，预训练的BERT组件仍然包含许多有用的信息。换句话说，一些彩票是“中奖”并产生最大的收益，但所有的子网都有大量的有用信息。</p><h4 id="5-对BERT’s-子网的研究"><a href="#5-对BERT’s-子网的研究" class="headerlink" title="5 对BERT’s 子网的研究"></a>5 对BERT’s 子网的研究</h4><p>然而，如果更好的性能来自于语言知识，我们将期望“好的”子网更好地编码这一知识，并在相同任务的微调运行中相对稳定。</p><p><img src="/img/2022-04-03-Pruning-Notes/5-5.png"></p><p>实验结果意味着“好的”子网是不稳定的，并且更多地依赖于随机初始化，而不是对特定任务使用部分预先训练的权值。 图3中显示的重要性分数的分布解释了为什么会出现这种情况。在任何给定的剪枝迭代中，大多数head和mlp都具有较低的重要性值，并且都可以以大致相等的成功率进行剪枝</p><h3 id="简短总结-3"><a href="#简短总结-3" class="headerlink" title="简短总结"></a>简短总结</h3><p>这篇文章使用量化剪枝和结构化剪枝（m-pruning &amp; s-pruning）在BERT上做了不同子网的实验，验证彩票假说LTH。主要的LTH预测得到了验证:“好的”子网可以单独成功地进行再训练。在这个基础上做了很多实验：1. “好的”、“随机”、“坏的”子网单独finetunig的结果比较。2. 比较BERT坏的子网和别的模型的性能。3. 讨论了好的子网可能学到的东西，这部分内容很多。</p><h3 id="创新点-3"><a href="#创新点-3" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#a4mClG"> 1 引言-贡献</a></p><h3 id="优劣-3"><a href="#优劣-3" class="headerlink" title="优劣"></a>优劣</h3><ol><li>在好的子网和坏的子网上可能学到的东西进行了很长篇幅的讨论，进行了很多实验比较。</li><li>彩票假说这个内容感觉很有意思。</li></ol><h3 id="流程与公式-3"><a href="#流程与公式-3" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#1AZ2RI">3 方法论</a></p><h3 id="主要实验-3"><a href="#主要实验-3" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#G3upH0">4 BERT Plays the Lottery</a></p><h2 id="6-Movement-Pruning-Adaptive-Sparsity-by-Fine-Tuning"><a href="#6-Movement-Pruning-Adaptive-Sparsity-by-Fine-Tuning" class="headerlink" title="6 - Movement Pruning: Adaptive Sparsity by Fine-Tuning"></a>6 - Movement Pruning: Adaptive Sparsity by Fine-Tuning</h2><h3 id="论文内容-5"><a href="#论文内容-5" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-5"><a href="#摘要-5" class="headerlink" title="摘要"></a>摘要</h4><p>量化剪枝是纯监督学习中一种广泛应用的模型压缩方法；然而，在NLP的迁移学习机制中，它的有效性较低。</p><p>我们提出使用运动剪枝（movement pruning），一个简单的一阶（first-order）权值剪枝方法，更适用于预训练的模型进行微调。我们给出了该方法的数学证明，并将其与现有的零阶（zeroth-）和一阶剪枝方法进行了比较。实验表明，当对大型的预训练语言模型剪枝时，运动剪枝可以显著改进高稀疏性。</p><p>当与蒸馏相结合时，该方法在只保留了3%模型参数的情况下实现了最小的精度损失。</p><h4 id="8-结论"><a href="#8-结论" class="headerlink" title="8 结论"></a>8 结论</h4><p>我们考虑了针对特定任务的微调的预训练模型的剪枝情况，比较了零阶和一阶剪枝方法。证明了一种基于直通式梯度的简单的权值剪枝方法对该任务是有效的，并且它适用于使用一阶重要性分值的方法。将这种运动剪枝应用到基于Transformer的结构中，在高稀疏状态下，我们的方法强很多。分析表明了这种方法如何适应微调制度的方式，而量化剪枝不能。</p><p>这篇文章是作者在huggingface的工作中的一部分。</p><h4 id="1-引言-5"><a href="#1-引言-5" class="headerlink" title="1 引言"></a>1 引言</h4><p>根据权重的重要性来去除权重的剪枝方法，是一种特别简单有效的模型压缩方法。量化剪枝是其中最广泛应用的方法，它保留了绝对值较高的权重。虽然量化剪枝对于监督学习是非常有效的，但它在迁移学习体制中是不太有用的。</p><p>这篇文章提出了运动剪枝，运动剪枝与量级剪枝的不同之处在于，当训练过程中权重值缩小时，无论高值还是低值，都可以进行剪枝。</p><p>这种剪枝策略可选0阶和1阶的参数，能让基于微调的目标效果更好。引入了一种简单、确定的运动剪枝方法，进行比较，这种方法基于直行估计器（straight estimator）。</p><h5 id="贡献-4"><a href="#贡献-4" class="headerlink" title="贡献"></a>贡献</h5><ol><li>在预训练的BERT上进行剪枝，在一系列不同任务中做实验。发现在高稀疏的区域（小于剩余权值的15%）这种方法相比量化剪枝和其他一阶剪枝（如L0正则化）提升很大。</li><li>在自然语言推理（MNLI）任务上，剪枝到只剩下5%编码器的权重的模型有原始BERT 95%的性能。</li><li>通过分析量化剪枝和运动剪枝的差异，发现两种方法的剪枝模型有很大的不同，其中运动剪枝对末端任务的适应能力更强。</li></ol><h4 id="3-背景：基于分数的剪枝策略"><a href="#3-背景：基于分数的剪枝策略" class="headerlink" title="3 背景：基于分数的剪枝策略"></a>3 背景：基于分数的剪枝策略</h4><p><img src="/img/2022-04-03-Pruning-Notes/6-1.png"></p><p>通过保留S矩阵（重要性分值矩阵）前v%的权重进行剪枝。</p><p>迭代量化剪枝:先训练模型，直到收敛，然后去掉最小幅度的权值。然后将去除的权值固定为0，对稀疏化的模型进行重新训练。重复此循环，直到达到所需的稀疏级别。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-2.png"></p><p>本文中涉及、对比的几种剪枝方法。</p><h4 id="4-运动剪枝"><a href="#4-运动剪枝" class="headerlink" title="4 运动剪枝"></a>4 运动剪枝</h4><p>量化剪枝可以看作是利用了0阶信息的模型。这项工作中，运动剪枝的重要性是1阶信息。意思是保留的不是远离0的信息（绝对值大），而是保留了训练过程中逐渐远离0（绝对值逐渐变大）的参数。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-3.png"></p><p>在训练时的loss$L$对$Si,j$的梯度由公式2给出。这意味着更新了权重的分数，即使这些权重在向前传递中被屏蔽。我们在附录A.1中证明了运动剪枝作为一个优化问题是收敛的。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-4.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/6-5.png"></p><p>意思就是绝对值变大，重要性$Si,j$也会随着变大。“我们认为，这对该方法的成功至关重要，因为它能够基于特定任务的数据进行修剪，而不仅仅是预先训练的值。”</p><h4 id="5-实验-1"><a href="#5-实验-1" class="headerlink" title="5 实验"></a>5 实验</h4><p>所有报告的稀疏度百分比都是相对于BERT-base的，并且甚至与基线相比，都精确地对应于模型大小。对于给定的任务，我们通过修剪方法对预训练的模型进行微调，以获得相同数量的更新(在6到10个epoch之间)。将三次稀疏调度用于量级剪枝(MaP)、运动剪枝(MvP)和软运动剪枝(SMvP)。在修剪结束时增加几个冷却步骤可以提高性能，特别是在高稀疏状态下。v的进度表为:</p><p><img src="/img/2022-04-03-Pruning-Notes/6-6.png"></p><h4 id="6-实验结果"><a href="#6-实验结果" class="headerlink" title="6 实验结果"></a>6 实验结果</h4><p>图2显示了在每个数据集上进行不同级别剪枝的主要剪枝方法的结果。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-7.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/6-8.png"></p><p>剪枝:在低稀疏度(超过剩余权值的70%)下，相对于密集模型，量级剪枝的性能比所有方法都好，几乎没有损失，而运动剪枝方法的性能即使在低稀疏度水平下也会迅速下降。但在稀疏度较高的情况下，量级剪枝的性能较差，性能下降非常快。相比之下，一阶方法在剩余权值小于15%的情况下表现出很强的性能。</p><p>运动剪枝和软运动剪枝与其他基线相比具有良好的性能，但QQP的RPP与软运动剪枝性能相当。运动剪枝也优于微调的迷你bert模型。这与[Li et al.， 2020]是一致的:<strong>训练一个大模型，然后压缩它，比从头开始训练一个小模型，既高效又有效。</strong></p><h5 id="Distillation-further-boosts-performance"><a href="#Distillation-further-boosts-performance" class="headerlink" title="Distillation further boosts performance"></a>Distillation further boosts performance</h5><p>在上述工作之后继续使用蒸馏，得到了更好的效果。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-9.png"></p><h4 id="7-分析"><a href="#7-分析" class="headerlink" title="7 分析"></a>7 分析</h4><p>运动剪枝适应性更强。</p><p>图4a比较了在相同稀疏度下使用幅度和运动剪枝对同一模型的同一矩阵进行剪枝后剩余权值的分布。我们观察到，根据定义，量级剪枝去除所有接近于零的权值，最终得到两个簇。相比之下，运动剪枝可以得到更平滑的分布，它可以覆盖除接近0的值之外的整个区间。</p><p>图4b显示了运动修剪中每个个体的权重与其相关的重要性得分。</p><p><img src="/img/2022-04-03-Pruning-Notes/6-10.png"></p><h3 id="简短总结-4"><a href="#简短总结-4" class="headerlink" title="简短总结"></a>简短总结</h3><p>本文提出了运动剪枝的方法，作为一种一阶(first-order)剪枝方法，在高稀疏的剪枝下能得到更好的效果，代表性的结果是在MNLI任务上用仅剩5%的权重得到了BERT95%的性能。</p><p>主要和量化剪枝进行了对比，前者是在剪枝中保留绝对值较大的，离0较远的权重值，本工作选择的是在剪枝中保留在训练集上逐渐远离0，绝对值变大的权值。作者做了一系列的实验，发现量化剪枝在低程度的剪枝上表现较小，但在高程度的剪枝上明显被运动剪枝打败，作者分析这可能是和运动剪枝能在训练中具有更强的适应性有关，相比量化剪枝，得到重要性分数的策略可能更适用。</p><h3 id="创新点-4"><a href="#创新点-4" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#VR7UkK">1 引言-贡献</a></p><h3 id="优劣-4"><a href="#优劣-4" class="headerlink" title="优劣"></a>优劣</h3><ol><li>比较了0阶和1阶的剪枝方法，做的实验和比较结果比较全面，在非常高的剪枝率上得到的效果感觉很有说服力，作者提出的运动剪枝更具有适应性的观点似乎站得住脚。</li><li>在结果后做了很多分析，但是一开始提出的契机是不是有点玄学。。</li></ol><h3 id="流程与公式-4"><a href="#流程与公式-4" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#Pcr5vS"> 4 运动剪枝</a></p><h3 id="主要实验-4"><a href="#主要实验-4" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#2uxxNy">5 实验</a></p><h2 id="7-The-Lottery-Ticket-Hypothesis-for-Pre-trained-BERT-Networks"><a href="#7-The-Lottery-Ticket-Hypothesis-for-Pre-trained-BERT-Networks" class="headerlink" title="7 - The Lottery Ticket Hypothesis for Pre-trained BERT Networks"></a>7 - The Lottery Ticket Hypothesis for Pre-trained BERT Networks</h2><h3 id="论文内容-6"><a href="#论文内容-6" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-6"><a href="#摘要-6" class="headerlink" title="摘要"></a>摘要</h4><p>对彩票假说（lottery ticket hypothesis）的研究表明，NLP和CV的模型包含更小的子网络，也能够独立地进行准确的训练，并能迁移到其他下游任务上。</p><p>在这项工作中，结合这些现象来评估这种可训练的、可转移的子网络是否存在于预训练的BERT模型中。对于一系列下游任务，我们确实发现匹配的子网络具有40%到90%的稀疏性。我们发现这些子网络是在预训练初始化的时出现的。子网络在MLM(与预训练模型相同的任务)上普遍都能迁移；那些在其他任务中被发现的子网络只能在有限程度上迁移。</p><h4 id="6-结论"><a href="#6-结论" class="headerlink" title="6 结论"></a>6 结论</h4><p>我们在预训练BERT模型的背景下研究彩票假设。我们发现，主要的彩票观察结果仍然成立:使用预先训练的初始化，BERT包含了稀疏的子网，这些子网具有非常的稀疏性，可以在一系列下游任务上进行隔离训练以获得充分的性能。</p><p>此外，还有通用的子网络可以传输到所有这些下游任务。这种转移意味着我们可以用一个更小的子网络取代完整的BERT模型，同时保持它的转移到其他任务的能力。</p><h4 id="1-引言-6"><a href="#1-引言-6" class="headerlink" title="1 引言"></a>1 引言</h4><p>在这场规模越来越大的模型竞赛的同时，一个新兴的子领域已经探索了在不牺牲性能的情况下训练更小的子网络来代替完整模型的前景。如果我们知道要选择哪个子网，我们可以从一开始就训练较小的网络。在越来越多关于彩票假说的研究中，出现了两个关键主题:</p><ol><li>预训练的初始化。 在计算机视觉和自然语言处理的大规模设置中[17-19]，彩票方法只能在训练的早期点找到匹配的子网，而不是在随机初始化时。</li><li>迁移学习。使用彩票方法查找匹配的子网是非常昂贵的。它需要对未修剪的网络进行完整的训练，删除不必要的权值，并将未修剪的权值从训练的早期点倒回它们的值。这比简单地训练整个网络要昂贵得多， 而且，为了获得最好的结果，它必须重复多次。但是，由此产生的子网在相关任务之间传输[22-24]。这个属性使得通过为许多不同的下游任务重用子网来证明这种投资是合理的。</li></ol><h5 id="贡献-5"><a href="#贡献-5" class="headerlink" title="贡献"></a>贡献</h5><p>虽然彩票假设已经在NLP[18,19]和transformer[18,25]的背景下得到了评估，但在BERT预训练的背景下没什么人做，本文主要探究这点。 我们特别关注这些子网络的传输行为，因为我们寻找通用的子网络，可以减少对下游任务进行微调的成本。在本研究过程中，我们有以下发现:</p><ol><li> 使用非结构化的量级剪枝，我们发现在标准GLUE和SQuAD下游任务下的BERT模型中，匹配子网络的稀疏度在40%到90%之间。</li><li> 与以往的NLP工作不同，我们发现这些子网是在(预训练的)初始化时，而不是在经过一定量的训练后。与之前的工作一样，这些子网的性能优于那些随机剪枝和随机重新初始化的子网。</li><li> 在大多数下游任务中，这些子网不会转移到其他任务，这意味着匹配的子网稀疏模式是特定于任务的。</li><li> 使用掩蔽语言建模任务(用于BERT前训练的任务)发现的稀疏度为70%的子网是通用的，并在保持准确性的情况下转移到其他任务。</li></ol><p>我们的结论是，从其他计算机视觉和NLP设置的彩票观察扩展到带有预先训练初始化的BERT模型。 在大规模的环境中，只能在训练的早期找到匹配的子网会消失了。 此外，确实有通用的子网络可以替代完整的BERT模型而且可以迁移学习。随着预训练在NLP和深度学习的其他领域变得越来越重要，我们的结果表明，可以从一开始就训练较小网络的令人兴奋的可能性——将成为这类学习算法的范例。</p><h4 id="2-相关工作-3"><a href="#2-相关工作-3" class="headerlink" title="2 相关工作"></a>2 相关工作</h4><p>BERT的压缩。剪枝和知识蒸馏的技术。</p><p>NLP中的彩票假说。之前的研究发现，匹配的子网在transformer和lstm训练的早期就存在，但在初始化时并不存在。</p><h4 id="3-前言"><a href="#3-前言" class="headerlink" title="3 前言"></a>3 前言</h4><p> 本节中，我们将详细介绍我们的实验设置和用于识别子网的技术。对子网的介绍：</p><p><img src="/img/2022-04-03-Pruning-Notes/7-1.png"></p><p>对Winning ticket 和 Universal subnetwork的解释。</p><p><img src="/img/2022-04-03-Pruning-Notes/7-2.png"></p><p>迭代训练找subnet的过程。</p><p><img src="/img/2022-04-03-Pruning-Notes/7-3.png"></p><h4 id="4-BERT中匹配子网的存在性"><a href="#4-BERT中匹配子网的存在性" class="headerlink" title="4 BERT中匹配子网的存在性"></a>4 BERT中匹配子网的存在性</h4><p> 我们评估了先前在彩票假设上的工作中提出的四个关于匹配子网的主张：</p><ol><li>在某些网络中，IMP发现中奖彩票。 有中奖的彩票吗?</li><li>IMP找到的随机剪枝的子网和随机初始化的子网不匹配。IMP中奖的票比随机修剪或初始化的子网稀疏吗?</li><li>在其他网络中，IMP只找到匹配的子网吗？在训练早期的某个步骤I处，或θi处初始化的子网络优于θ0[17]处初始化的子网络。 重放能提高性能吗?</li><li>当找到匹配的子网络时，它们与使用标准剪枝找到的子网络在相同的稀疏性下达到相同的精度。 IMP子网的性能与标准剪枝匹配吗?</li></ol><p><img src="/img/2022-04-03-Pruning-Notes/7-4.png"></p><p>A1： 我们确实为MLM任务和所有下游任务找到了中奖的票(表2)。为了考虑性能的波动，我们认为如果一个子网络的全BERT的性能在一个标准偏差的性能之内，那么这个子网络就是中奖的票我们发现这些中奖彩票的最高稀缺性从40% (SQuAD)、50% (MRPC和CoLA)到90% (QQP和WNLI)不等。每个任务的稀疏性和任务本身的属性(例如，训练集的大小)之间没有明显的关系</p><p>A2： 为了在BERT的背景下评估这些主张，我们训练一个子网络f(x;mRP吗?θ0，·)和一个随机修剪掩码(评估修剪掩码mIMP的重要性)和一个子网络f(x;mIMP吗?随机初始化(评估预训练初始化θ0的重要性)。从表2可以看出，两种情况下的表现都远低于中奖彩票的表现;例如，在MNLI上随机剪枝时，它下降了15个百分点，在重新初始化时下降了21个百分点。这确认了该设置中特定修剪权重和初始化的重要性。</p><p>A3： 对任何下游任务来说，rewinding不会显著提高性能。事实上，在某些情况下(STS-B和RTE)，性能下降如此之多，以至于子网不再匹配，即使我们有两个百分点的差额。这是一个显著的偏离之前的工作，在最坏的情况下重放对精度没有影响。STS-B和RTE的结果特别差的一个可能的解释是，它们的小训练集导致过拟合。</p><p><img src="/img/2022-04-03-Pruning-Notes/7-5.png"></p><p>A4： 在表3中，我们看到结果因任务的不同而不同。对于某些任务(QQP, QNLI, MRPC, MLM)，标准修剪可以提高中奖彩票的性能高达两个百分点。对于其他(STS-B, WNLI, RTE, SST-2)，性能下降高达3个百分点。最大的下降再次发生在具有小训练集的任务中，因为标准修剪可能过拟合。</p><h4 id="5-子网的迁移任务"><a href="#5-子网的迁移任务" class="headerlink" title="5 子网的迁移任务"></a>5 子网的迁移任务</h4><p>Q1： 中奖的彩票迁移吗? </p><p>虽然也有传输性能与相同任务性能匹配的情况，但这是例外。在我们考虑的11个任务中，只有三个源任务的子网会转移到两个以上的其他任务。然而，如果我们允许转移性能下降2.5个百分点，那么七个源任务转移到至少一半的其他任务。</p><p>MLM任务生成传输性能最好的子网。</p><p>Q2：在子网可转移性中是否存在模式?</p><p>可转移性似乎与任务训练实例的数量有关。MRPC和WNLI的训练集最小;MRPC子网只传输到另一个任务，而WNLI子网不传输到任何其他任务。另一方面，MNLI和SQuAD拥有最大的培训集，由此产生的子网分别转移到四个和三个其他任务。MLM，这是迄今为止最大的培训集，也生产的子网络转移最好。有趣的是，我们没有看到任何证据表明迁移与任务类型相关(使用表1中描述的分组)。</p><p>Q3： 初始化为θ0是否会导致更好的转移?</p><p>在几乎所有的情况下，复卷对目标任务的传输性能都具有相同或更高的性能，而标准剪枝对传输性能的不利影响也很小。这表明，至少对SQuAD来说，在源任务上训练的重量似乎能提高而不是降低转会表现。</p><h3 id="简短总结-5"><a href="#简短总结-5" class="headerlink" title="简短总结"></a>简短总结</h3><p>本文提出之前的彩票假说在NLP和CV得到了广泛的验证，但是没有人做BERT这样的预训练模型的。于是作者提出了一种实验方式，在BERT上进行中奖彩票假说的验证，对各种任务都使用IMP的方法找到了子网，对此结果也做了很多子网模型和迁移学习方面的讨论和探究。</p><h3 id="创新点-5"><a href="#创新点-5" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#HIQuKb">1 引言-贡献</a></p><h3 id="优劣-5"><a href="#优劣-5" class="headerlink" title="优劣"></a>优劣</h3><ol><li>作者自问自答的模式讨论了很多内容，附加很完善的实验结果，比较全面，有理有据。</li></ol><h3 id="流程与公式-5"><a href="#流程与公式-5" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#4VTT6R">3 前言</a></p><h3 id="主要实验-5"><a href="#主要实验-5" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#8bmmZj">4 BERT中匹配子网的存在性</a></p><h2 id="8-Pruning-Pretrained-Encoders-with-a-Multitask-Objective"><a href="#8-Pruning-Pretrained-Encoders-with-a-Multitask-Objective" class="headerlink" title="8 - Pruning Pretrained Encoders with a Multitask Objective"></a>8 - Pruning Pretrained Encoders with a Multitask Objective</h2><h3 id="论文内容-7"><a href="#论文内容-7" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-7"><a href="#摘要-7" class="headerlink" title="摘要"></a>摘要</h4><p>在这项工作中，我们在微调过程中对模型进行剪枝，以探讨是否有可能对单个编码器进行剪枝，从而使其可以用于多个任务。</p><p>我们分配了一个固定的参数预算，并比较了多任务目标模型用于执行单任务与最佳的单任务模型之间的剪枝。我们发现，在两种剪枝策略下(元素剪枝(element pruning)和秩剪枝(rank pruning))，多任务目标的方法在所有任务结果平均时优于单任务模型，并且在每个任务上都表现不错。</p><p>在剪枝过程中使用多任务目标也可以有效地减少低资源任务(low-resource tasks)的模型大小。</p><h4 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h4><p>我们研究了多任务剪枝，其目的是在给定参数预算小于单任务模型的情况下，在多任务模型上表现良好。 通过对非结构化(element-wise)剪枝和结构化(rank)剪枝策略的研究，我们发现基于多任务目标的剪枝策略优于基于单个任务目标的多个剪枝模型组合的剪枝策略。 此外，我们发现具有多任务目标的剪枝(不需要额外的超参数调优)可以帮助进一步剪枝低资源数据集的任务。</p><h4 id="1-引言-7"><a href="#1-引言-7" class="headerlink" title="1 引言"></a>1 引言</h4><p>在这项工作中，我们探索了进一步减少这些多任务模型使用的参数数量，使其大大小于n的可能性。具体来说，我们的目标是通过使用一个多任务训练目标来修剪多任务模型。除了以上所述的优点之外，我们的目标是实现两个方面的最佳效果:一个经过大幅精简的模型，它在多个任务上也能很好地执行。</p><h5 id="贡献-6"><a href="#贡献-6" class="headerlink" title="贡献"></a>贡献</h5><ol><li> 我们将结构化和非结构化剪枝方法扩展到多任务设置。</li><li> 在这两种方法下，我们发现在给定的预算下，多任务模型始终优于单任务模型的组合。</li><li>使用多任务目标并不一定会导致任何单个任务的性能损失，在某些情况下，可以改进单个任务目标。</li><li> 多任务目标能够提高具有较小数据集的任务的性能。</li></ol><h4 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h4><p>量化vs运动剪枝。我们探索了8种模型剪枝的设置，不同的剪枝方法(幅度vs.移动)，被剪枝的内容(基于元素的(非结构化)剪枝vs.等级的(结构化)剪枝)，以及在哪里(全局vs.局部)。表1总结了不同之处。我们使用稀疏性这个术语来指代被删除(或归零)的权重的比例。</p><p><img src="/img/2022-04-03-Pruning-Notes/8-1.png"></p><h5 id="2-2-多任务"><a href="#2-2-多任务" class="headerlink" title="2.2 多任务"></a>2.2 多任务</h5><p> 将这些方法扩展到多任务设置是很简单的。我们为每个任务训练独立的、未修剪的分类头，同时在所有任务中共享一组修剪后的编码器权重(在运动修剪的情况下，还学习了重要性分数)。</p><h4 id="3-实验和结构"><a href="#3-实验和结构" class="headerlink" title="3 实验和结构"></a>3 实验和结构</h4><p> 首先，我们在单个数据集上对这8种剪枝方法进行基准测试(第3.1节)。 目标是确定任务和实际的权衡之间的结构化和非结构化修剪和最佳设置。</p><p> 在第3.2节中，我们将一个带有多任务目标的模型与单任务模型的集合进行比较，以探讨多任务目标是否有效。</p><p>最后，我们问，即使在单任务设置中，多任务目标是否可以作为辅助目标提供好处(章节3.3)。</p><h5 id="3-1-比较修剪策略"><a href="#3-1-比较修剪策略" class="headerlink" title="3.1  比较修剪策略"></a>3.1  比较修剪策略</h5><p>各方法不同剪枝程度下的性能比较。</p><p><img src="/img/2022-04-03-Pruning-Notes/8-2.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/8-3.png"></p><h5 id="3-2-多任务修剪"><a href="#3-2-多任务修剪" class="headerlink" title="3.2 多任务修剪"></a>3.2 多任务修剪</h5><p> 具体来说，我们使用四个独立的目标:MNLI、QQP、SQuAD和multitask，将模型修剪到不同的稀疏程度。我们比较了单任务模型与多任务目标修剪后的单任务模型的最佳混合最好的混合是由Pareto frontier确定的。</p><p> 可能的模型集合。图2显示了多任务模型在宏观平均3个任务指标上优于混合模型。此外，它匹配或超过单个任务的性能。我们发现，多任务剪枝优于混合元素剪枝和秩剪枝，这表明在剪枝过程中利用多任务目标的能力可以扩展到其他新的剪枝方法。这也证明，在编码器中，存在可以跨多个任务利用的较小的共享子网。</p><h5 id="3-3-多任务训练作为辅助修剪目标"><a href="#3-3-多任务训练作为辅助修剪目标" class="headerlink" title="3.3  多任务训练作为辅助修剪目标"></a>3.3  多任务训练作为辅助修剪目标</h5><p>表2显示了使用局部大小秩剪枝时每个任务的性能。 这些结果表明，基于多任务的剪枝方法为低资源任务提供了一种有效剪枝模型的方法，而无需广泛的超参数搜索。</p><p><img src="/img/2022-04-03-Pruning-Notes/8-4.png"></p><h3 id="简短总结-6"><a href="#简短总结-6" class="headerlink" title="简短总结"></a>简短总结</h3><p>本文使用多任务目标函数进行预训练的语言模型BERT的剪枝，使用了多种剪枝的方法，得出的结论是多任务模型的剪枝比单任务的效果都要好。也研究了多任务函数作为一种辅助函数去剪枝单任务目标的表现。</p><h3 id="创新点-6"><a href="#创新点-6" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#j2KOTx">1 引言-贡献</a></p><h3 id="优劣-6"><a href="#优劣-6" class="headerlink" title="优劣"></a>优劣</h3><h3 id="流程与公式-6"><a href="#流程与公式-6" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#lJLEzP">2 方法</a></p><h3 id="主要实验-6"><a href="#主要实验-6" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#3ytHAj">3 实验和结构</a></p><h2 id="9-Layer-wise-Model-Pruning-based-on-Mutual-Information"><a href="#9-Layer-wise-Model-Pruning-based-on-Mutual-Information" class="headerlink" title="9-Layer-wise Model Pruning based on Mutual Information"></a>9-Layer-wise Model Pruning based on Mutual Information</h2><h3 id="论文内容-8"><a href="#论文内容-8" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-8"><a href="#摘要-8" class="headerlink" title="摘要"></a>摘要</h4><p>受支持向量机(SVM)中基于互信息(mutual information, MI)的特征选择和逻辑回归的启发，本文提出了基于互信息的分层剪枝：对于多层神经网络的每一层，每次保留MI值相对上层神经元较高的神经元。从顶部的softmax层开始，从上往下逐层剪枝，直到到达底部的词嵌入层(word embedding layer)。</p><p>与基于权重的剪枝技术相比，本文提出的剪枝策略具有以下优点:(1)由于表示和矩阵可以压缩成更小但更密集的对应对象，从而避免了不规则的内存访问(irregular meory access)，从而提高了速度;(2)基于顶层的训练信号，自上而下从更全局的角度进行剪枝，通过层层传播全局信号的效果来对每一层进行剪枝，在相同的稀疏级别上具有更好的性能。大量实验表明，在相同的稀疏程度下，该策略比基于权值的剪枝方法(如量化剪枝、运动剪枝)具有更好的速度和性能。</p><h4 id="6-结论-1"><a href="#6-结论-1" class="headerlink" title="6 结论"></a>6 结论</h4><p> 本文提出了基于MI的自然语言处理模型剪枝方法。该模型避免了不规则的内存访问问题，在相同的稀疏程度下具有较高的速度。此外，该策略基于全局训练信号，采用自上而下的方式对模型进行裁剪，从而获得更高的准确率。在未来的工作中，我们应该放弃神经元值来自高斯分布的强烈假设。</p><h4 id="1-引言-8"><a href="#1-引言-8" class="headerlink" title="1 引言"></a>1 引言</h4><p>基于权值的方法已成功地应用于广泛的神经模型中进行模型剪枝，但存在以下缺点:(1)矩阵中的权值被不规则地剪枝，导致内存访问不规则，导致运行效率低下;(2)权矩阵的修剪是独立的，忽略了顶层训练信号的全局监督，忽略了连续层之间的信息传播，可能导致修剪后的网络的次优性。</p><p>本文受支持向量机中基于互信息(MI)的特征选择和逻辑回归的启发，提出了基于MI的分层剪枝方法，以解决上述NLP中基于权值的剪枝方法的缺点。 </p><p>对于多层神经网络的每一层，相对于上层保存的神经元，MI值较高的神经元被保存。从顶部的softmax层开始，逐层剪枝，直到以自上而下的方式到达底部的输入字嵌入层。</p><h5 id="贡献-7"><a href="#贡献-7" class="headerlink" title="贡献"></a>贡献</h5><p>本文解决了基于权值的剪枝的两个缺点：</p><p>(1)由于将剪枝后的表示和矩阵压缩成更小但更密集的表示和矩阵，避免了不规则的内存访问;与基于权重的剪枝方法相比，在相同的稀疏级别上，这能够显著加快计算速度;</p><p>(2)该方法不是根据每个权值单独查看每个权值矩阵，而是基于顶层的训练信号，从更全局的角度进行操作，将全局训练信号的效果自上而下地传播到连续的层中，对每一层进行修剪。这将在相同的稀疏级别上带来更好的性能。</p><h4 id="3-模型"><a href="#3-模型" class="headerlink" title="3 模型"></a>3 模型</h4><p> 使用MI计算第1 - 1层和第l层维度之间的相关得分I(A, B)的定量方法。 </p><p><img src="/img/2022-04-03-Pruning-Notes/9-1.png"></p><p>互信息(MI)是两个随机变量之间量化信息量的一种度量，通过另一个变量得到关于一个变量的信息。MI的公式如下：</p><p><img src="/img/2022-04-03-Pruning-Notes/9-2.png"></p><h4 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h4><p>将提出的策略与以下权值的剪枝模型进行比较：量化剪枝、运动剪枝、L0剪枝。</p><p><img src="/img/2022-04-03-Pruning-Notes/9-3.png"></p><p> 因为互信息策略提供了一种基于输出标签的更全局的特征(维度)选择策略，而不是在矩阵操作中关注局部矩阵权值。对于量级剪枝和运动剪枝，我们发现运动剪枝在较低的稀疏级别上表现较差，但在较高的稀疏级别上表现较好。</p><p><img src="/img/2022-04-03-Pruning-Notes/9-4.png"></p><h3 id="简短总结-7"><a href="#简短总结-7" class="headerlink" title="简短总结"></a>简短总结</h3><p>使用互信息进行剪枝，和之前的基于剪枝的方法不同。解决的问题是：能避免不规则的内存访问，避免稀疏的操作，从而提高运行速度；能在剪枝的过程中看到更全局的信息，一层一层传递下来。</p><h3 id="创新点-7"><a href="#创新点-7" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#1nqEwE">1 引言-贡献</a></p><h3 id="优劣-7"><a href="#优劣-7" class="headerlink" title="优劣"></a>优劣</h3><h3 id="流程与公式-7"><a href="#流程与公式-7" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#ICPg8j">3 模型</a></p><h3 id="主要实验-7"><a href="#主要实验-7" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#KVMkcg">4 实验结果</a></p><h2 id="10-Block-Pruning-For-Faster-Transformers"><a href="#10-Block-Pruning-For-Faster-Transformers" class="headerlink" title="10-Block Pruning For Faster Transformers"></a>10-Block Pruning For Faster Transformers</h2><h3 id="论文内容-9"><a href="#论文内容-9" class="headerlink" title="论文内容"></a>论文内容</h3><h4 id="摘要-9"><a href="#摘要-9" class="headerlink" title="摘要"></a>摘要</h4><p>剪枝方法是减小模型尺寸的有效方法，而知识蒸馏是加速推理的有效方法。</p><p>我们提出了一种针对小模型和快速模型的块剪枝(block pruning)方法。我们的方法通过考虑任意大小的块来扩展结构化方法，并将这种结构加到运动剪枝(movement pruning)中，然后进行微调。我们发现，这种方法学会了删除底层模型的完整组件，例如注意头。实验包含分类和生成任务，得到了一个剪枝后的模型，比其他模型快2.4倍，让在SQuADv1上构建的BERT变小了74%，在F1上只下降了1%，而且速度比知识蒸馏模型快，大小比剪枝模型小。</p><h4 id="1-引言-9"><a href="#1-引言-9" class="headerlink" title="1 引言"></a>1 引言</h4><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p>在这项工作中，我们的目标是通过块剪枝来缩小这一差距。与修剪单个参数不同，这种方法鼓励在密集硬件上进行优化的修剪。与结构化方法中通常使用的基于行或列的剪枝相比，这是一种不那么严格的方法(McCarley, 2019)，结构化方法很难有效地应用于变压器。</p><p>我们将这种方法与运动剪枝相结合(Sanh等人，2020年)，这是一种在微调过程中对预训练模型进行剪枝的简单方法。最后的方法1几乎没有额外的超参数或训练要求。</p><p>尽管在训练过程中使用了子行正方形块，该方法学会了消除模型的全部组件，有效地降低了大量的注意头。这种效果允许模型实现加速，甚至超过标准的结构修剪的前馈层。结果显示，在SQuAD v1.1上加速2.4倍，F1下降1%，在QQP上加速2.3倍，F1下降1%。在CNN/DailyMail的所有ROUGE指标上，总结的实验也显示了1.39倍的加速，平均下降2点，解码器的权重减少3.5倍。</p><h4 id="8-结论-1"><a href="#8-结论-1" class="headerlink" title="8 结论"></a>8 结论</h4><p>我们已经证明，我们可以提取小的剪枝模型，它们在一个等效或比蒸馏网络更好。这种方法可以在微调期间完成，而不是在培训前。该方法不求助于数据增强或架构搜索等技术，它适用于各种任务和基本模型。随着更好、更大的模型以越来越快的速度发布，我们可以依靠一种简单而稳健的方法，在不牺牲精度的情况下，在特定任务上加快它们的速度，并在保持大部分原始模型精度的情况下，轻松地分发这些模型。</p><h4 id="4-块剪枝的模型"><a href="#4-块剪枝的模型" class="headerlink" title="4 块剪枝的模型"></a>4 块剪枝的模型</h4><p>在此工作中，我们将运动剪枝扩展到局部参数块上。具体来说，变压器中的每个矩阵被划分为固定大小的块。这种设置超越了对非结构化方法的任意修剪，其目标是鼓励数据局部性更接近效率所需要的东西。</p><p>在过去的工作中，这个模型经过了精馏的训练，以匹配教师模型的性能。与其他需要完全指定新模型结构的精馏方法不同，我们的方法只需要块的大小和形状，即模型中每个参数矩阵的(m0, n0)集合。如果块太大，则很难对其进行修剪，但如果块太小，则不支持有效推理。</p><p><img src="/img/2022-04-03-Pruning-Notes/10-1.png"></p><p><img src="/img/2022-04-03-Pruning-Notes/10-2.png"></p><h4 id="6-实验"><a href="#6-实验" class="headerlink" title="6 实验"></a>6 实验</h4><p> 图1显示了不同的块剪枝方法对注意层和前馈层的影响，蓝色被保留，粉色被修剪。我们发现，所有不同大小的块学会了在FFN层中修剪整个维度。有趣的是，我们发现block方法也可以从MHA中移除整个头部。 </p><p><img src="/img/2022-04-03-Pruning-Notes/10-3.png"></p><p>SQuAD的结果如图2所示，对于给定的速度，曲线越高，模型越精确。</p><p><img src="/img/2022-04-03-Pruning-Notes/10-4.png"></p><h3 id="简短总结-8"><a href="#简短总结-8" class="headerlink" title="简短总结"></a>简短总结</h3><h3 id="创新点-8"><a href="#创新点-8" class="headerlink" title="创新点"></a>创新点</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#Oa4XW5">1 引言-结果</a></p><h3 id="优劣-8"><a href="#优劣-8" class="headerlink" title="优劣"></a>优劣</h3><h3 id="流程与公式-8"><a href="#流程与公式-8" class="headerlink" title="流程与公式"></a>流程与公式</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#4gCHn4">4 块剪枝的模型</a></p><h3 id="主要实验-8"><a href="#主要实验-8" class="headerlink" title="主要实验"></a>主要实验</h3><p><a href="https://c9hu5keisa.feishu.cn/docs/doccnwWNPBakU1ILDdGYzTM5Vhe#DdFs67">6 实验</a></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理 NLP</tag>
      
      <tag>Transformer</tag>
      
      <tag>论文笔记</tag>
      
      <tag>BERT</tag>
      
      <tag>剪枝 Pruning</tag>
      
      <tag>模型压缩</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BERT 论文笔记</title>
    <link href="/2022-03-26-BERT-Notes/"/>
    <url>/2022-03-26-BERT-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="BERT-论文笔记"><a href="#BERT-论文笔记" class="headerlink" title="BERT 论文笔记"></a>BERT 论文笔记</h1><h2 id="论文主要信息"><a href="#论文主要信息" class="headerlink" title="论文主要信息"></a>论文主要信息</h2><ul><li>标题：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</li><li>作者：<a href="https://arxiv.org/search/cs?searchtype=author&query=Devlin,+J">Jacob Devlin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Chang,+M">Ming-Wei Chang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Lee,+K">Kenton Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&query=Toutanova,+K">Kristina Toutanova</a></li><li>机构：Google AI Language</li><li>来源：NAACL-HLT 2019（Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies）</li><li>代码：<a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></li></ul><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>本文介绍一个新的语言表示模型BERT（Bidirectional Encoder Representations from Transformers）。叫BERT和它的想法主要来自于另一份工作ELMo（芝麻街中的角色）很有关系。主要引用了ELMo和GPT两篇文章，BERT训练深的双向的表示，使用的是没有标号的数据，联合左右的上下文信息。因为我们的设计导致训练好的BERT，只需要加一个额外的输出层，就可以在很多的NLP任务上得到不错的结果，包括问答、语言推理等，而且不需要对任务做特别的架构上的改动。相比GPT的利用单向，左边的上下文信息预测未来，BERT使用的是左右侧双向的信息。相比ELMo使用的RNN的架构，BERT使用的是Transformer，所以当ELMo使用到较下游的任务的时候需要对架构进行调整，而BERT的地方相对比较简单，只需要改最上层就行（和GPT比较一样）。</p><p>BERT在概念上更加简单，而且在实验上更加好，在11个NLP任务上得到了更好的结果，包括GLUE、MultiNLI、SQuADv1.1、SQuADv2.0等（包括详细的精度比较）。</p><h2 id="1-引言-Introduction"><a href="#1-引言-Introduction" class="headerlink" title="1. 引言 Introduction"></a>1. 引言 Introduction</h2><p>在语言模型里，<strong>预训练</strong>可以用来提升很多自然语言的任务。比如词嵌入、GPT等。这些自然语言任务里包括两类，第一类叫句子层面的任务，建模句子之间的关系，比如对句子情绪的识别、分析两个句子之间的关系；第二类任务是词语言层面的任务，比如实体命名的识别和问答，这类任务需要输出一些细粒度的词元层面的输出。 </p><p>在使用预训练模型做特征表示的时候，一般有两类策略，第一种是基于特征的，第二种是基于微调的。第一种基于特征的代表作是ELMo，它对每一个下游的任务，构造一个和这个任务相关的神经网络（RNN），在预训练好这些表示后，作为一个额外的特征一起输入进这个模型里，希望这些特征有了比较好的表示，让模型训练比较容易。第二种基于微调的例子是GPT，把预训练好的模型放在下游后不需要改开头的，只需要在下游的任务上微调一下。这两个途径都是在使用一个相同的目标函数，使用一个单向的语言模型（根据前文去预测未来的语言）。</p><p>现在这样的技术有局限性，特别是在做预训练的表示的时候（pre-trained representations），主要问题是标准的语言模型是一个单向的，导致在选架构的时候有一定的局限性，比如GPT里面使用的是一个从左到右的架构，这个东西不是很好，如果要做一个句子层面的分析的话，比如一次从左看到右和从右看到左（完整看整个句子）都是合法的，如果要做一个词元级别的任务的时候，比如问答任务的时候，也是可以看完整个句子后去选答案，而不是必须要一个一个往下走。如果我把两个方向的信息都放进来之后，应该是能提高相关任务的性能的。</p><p>在这篇论文中，提出了BERT，BERT是用来减轻之前提到过的语言模型是单向的限制。它使用的是一个带掩码（<strong>MLM</strong>, Masked Language Model）的语言模型，这个东西是受一个叫Cloze task（1953年的一篇论文）的启发。这个带掩码的语言模型每次随机的选一些词元把它们盖住，然后目标函数就是预测那些被盖住的词，类似完形填空。这样允许我们训练一个深的双向的语言模型，在带掩码的模型之外，还有另一个任务预测下一个句子（<strong>NSP</strong>, Next Sentence Prediction），给两个句子，判断两个句子在原文中是否相邻，还是随机采样的两个句子放在一起。这样能让模型学习一些句子层面的信息。</p><p>这篇文章的贡献，作者罗列了三点：</p><ol><li>展示了双向信息的重要性。GPT只用了单向，之前有的工作是把一个从左看到右的语言模型和从右看到左的语言模型简单合并在一起，有点像双向的RNN模型，把它们链接在一起。BERT在双向信息的运用上更合理一些。</li><li>假设有一个比较好的预训练的模型的话，就可以不用对很多特定任务做特定的模型的改动了。BERT是在一系列NLP任务上的第一个基于微调的模型，这些任务包括句子层面、词元层面的任务上，都取得了最好的成绩。</li><li>代码和模型全部放在这里，大家可以随便用。</li></ol><h2 id="6-结论-Conclusion"><a href="#6-结论-Conclusion" class="headerlink" title="6. 结论 Conclusion"></a>6. 结论 Conclusion</h2><p>最近的一些实验表明使用非监督的预训练是非常好的，这样使得资源不多的任务，比如训练样本比较少的任务也能享受深度神经网络。作者主要的工序就是把前人的结果拓展到一个深的双向的架构上面，使得同样的一个预训练模型能够处理大量的NLP的任务。主要是把ELMo使用双向的想法和GPT使用Transformer的东西合起来，就成了BERT，具体的改动是在做语言模型的时候，从预测未来变成了完形填空。</p><h2 id="2-相关工作-Related-Work"><a href="#2-相关工作-Related-Work" class="headerlink" title="2. 相关工作 Related Work"></a>2. 相关工作 Related Work</h2><h3 id="2-1-非监督的基于特征的工作-Unsupervised-Feature-based-Approaches"><a href="#2-1-非监督的基于特征的工作-Unsupervised-Feature-based-Approaches" class="headerlink" title="2.1. 非监督的基于特征的工作 Unsupervised Feature-based Approaches"></a>2.1. 非监督的基于特征的工作 Unsupervised Feature-based Approaches</h3><p>主要是之前提到的ELMo，介绍了一些。</p><h3 id="2-2-非监督的基于微调的工作-Unsupervised-Fine-tuning-Approaches"><a href="#2-2-非监督的基于微调的工作-Unsupervised-Fine-tuning-Approaches" class="headerlink" title="2.2. 非监督的基于微调的工作 Unsupervised Fine-tuning Approaches"></a>2.2. 非监督的基于微调的工作 Unsupervised Fine-tuning Approaches</h3><p>代表作是GPT，和一些相关的工作。</p><h3 id="2-3-有监督的数据上做迁移学习-Transfer-Learning-from-Supervised-Data"><a href="#2-3-有监督的数据上做迁移学习-Transfer-Learning-from-Supervised-Data" class="headerlink" title="2.3. 有监督的数据上做迁移学习 Transfer Learning from Supervised Data"></a>2.3. 有监督的数据上做迁移学习 Transfer Learning from Supervised Data</h3><p>在NLP中有监督的数据包括了自然语言的一些推理和机器翻译，在这些有标号的数据集上训练好后在别的任务上使用。类比CV中这一块使用是很多的，比如大家在ImageNet上训练好模型，再去别的地方使用，但是在NLP这块似乎不是那么的理想。这两个任务（推理和机器翻译）可能和别的任务差别还是挺大的，也可能是数据规模是远远不够的。BERT以及之后的一些研究证明了在NLP上面使用没有编号的大量的数据集训练成果比有标号的小的数据集上效果更好。</p><h2 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3. BERT"></a>3. BERT</h2><p>在这一章中主要讲BERT的实现的细节。第一个叫预训练（pre-training）和微调（fine-tuning）。在预训练的时候模型是在没有标号的数据集上训练的，在微调的时候就是在下游任务的时候基于预训练好的模型在有标号的数据集上继续训练。每一个下游任务都会有一个新的BERT模型，虽然他们用的都是一样的预训练模型进行参数的初始化，但是对每一个下游任务，都会根据自己的数据训练自己的模型，在图1里面进行了展示。</p><p><img src="/img/2022-03-26-BERT-Notes/Fig1.png"></p><h3 id="模型架构-Model-Architecture"><a href="#模型架构-Model-Architecture" class="headerlink" title="模型架构 Model Architecture"></a>模型架构 Model Architecture</h3><p>BERT模型架构是一个多层的双向的Transformer的编码器，而且是直接基于原始的论文和原始的代码，没有做什么改动。（BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.） </p><p>在这篇文章的工作中，调了3个参数，第一个是Transformer块的个数、层数$L$，第二个是隐藏层的宽度大小$H$，第三个是在自注意力机制里多头的头的个数$A$。作者有两个模型$BERT_{base}(L=12,H=768,A=12)$，总共学习的参数是$110M$，和$BERT_{large}(L=24,H=1024,A=16)$，总共学习的参数是$340M$。</p><p>BERTbase是选取了和GPT模型的参数数量差不多的，是用于比较的目的。BERTlarge就是用于刷榜的。</p><blockquote><p>如何通过超参数的大小计算训练的参数个数，回顾Transformer的架构</p></blockquote><p><img src="/img/2022-03-26-BERT-Notes/Aft1.png"></p><h3 id="输入输出的表示-Input-Output-Representations"><a href="#输入输出的表示-Input-Output-Representations" class="headerlink" title="输入输出的表示 Input/Output Representations"></a>输入输出的表示 Input/Output Representations</h3><p>为了让BERT能处理各种下游任务，输入的表示既可以是一个句子，也可以是一个句子对。一个句子的意思是一段连续的文字，输入是一定的文字序列。</p><p>用的切词的方法是WordPiece，如果按照空格切词的话，因为数据量很大，所以可能导致词典的大小在百万级别。如果是这样的话，之前计算出的参数数量可能会导致训练后的参数整个都在嵌入层上。WordPiece的想法是如果一个词整体出现的概率不大，那么把它切开看它的子序列，如果某个子序列出现的概率比较大的话，那么很可能是个词根，保留这个子序列就行了，这样可以把一个相对来说比较长的词切成比较小的词根。所以用一个大小大概是30000的词典就可以表示整个文本了。</p><p>输入的表示有两点：第一，序列的开始永远是一个特殊的记号[CLS]，这个词的作用是BERT希望它的输出是整个序列的一个句子层面的信息。因为BERT使用的是Transformer的编码器，所以放在开头也能看到整个句子的信息。第二，两个句子合在一起，但是要做句子层面的分类，所以需要把两个句子区分开来，有两个办法来区分，第一个是在每个句子后面放一个特殊的词[SEP]，第二个是学一个特殊的嵌入层来表示这个句子是第一个句子还是第二个句子，在图1里有展示它长什么样子。</p><p>对于一个词元，它进入BERT的向量表示是这个词本身的一个Embedding加上在哪个句子和在哪个位置的Embedding，在图2里有展示出来BERT是如何从一个词元的序列得到一个向量的序列，这个向量的序列会进入Transformer块。</p><p><img src="/img/2022-03-26-BERT-Notes/Fig2.png"></p><h3 id="3-1-预训练的BERT-Pre-training-BERT"><a href="#3-1-预训练的BERT-Pre-training-BERT" class="headerlink" title="3.1. 预训练的BERT Pre-training BERT"></a>3.1. 预训练的BERT Pre-training BERT</h3><p>预训练主要关注目标函数和数据是什么？</p><h4 id="Task-1-Masked-LM-MLM"><a href="#Task-1-Masked-LM-MLM" class="headerlink" title="Task#1 Masked LM (MLM)"></a>Task#1 Masked LM (MLM)</h4><p>主要先介绍了带掩码的语言模型是什么，为什么双向的信息更好。对于一个输入的词元序列，如果一个词元是由WordPiece生成的话，有15%的概率会生成一个掩码，但是对于特殊的标记符词元，不会进行替换。比如输入的词元长度是1000的话，那么就要预测150个词。</p><p>这样也有一定的问题，当做掩码的时候，会把一定的词替换成[MASK]，那么在训练的时候，会看到15%的词元会对应成这个[MASK]，但是在微调的时候，其实是没有这个词元的，导致在预训练的时候和微调的时候看到的数据会不一样，这样会带来一定的问题。一个解决方法是，对于这15%的被选中的掩码的词，有80%的概率是真的替换成[MASK]，还有10%的概率是替换成一个随机的词元，还有10%的概率是什么都不干，但是用它来做预测。这个比例来自于有一个ablation study，跑了实验后发现这个东西还不错。</p><p><img src="/img/2022-03-26-BERT-Notes/MLM.png"></p><h4 id="Task-2-Next-Sentence-Prediction-NSP"><a href="#Task-2-Next-Sentence-Prediction-NSP" class="headerlink" title="Task#2 Next Sentence Prediction (NSP)"></a>Task#2 Next Sentence Prediction (NSP)</h4><p>在QA和在推理里，都是一个句子对，所以能让BERT学习一些句子层面的信息是不错的。具体来说，输入序列里有两个句子，a和b，有50%的概率是b在原文中就在a之后，有50%的概率是b就是随机选取的一个例子。在Section 5.1中有一些结果的比较，加入这个目标函数之后，能极大地提高QA和在语言推理的效果。</p><p><img src="/img/2022-03-26-BERT-Notes/NSP.png"></p><h4 id="Pre-training-data"><a href="#Pre-training-data" class="headerlink" title="Pre-training data"></a>Pre-training data</h4><p>用了两个数据集，第一个是BooksCorpus(800M words)，第二个是English Wikipedia(2,500M words)。我们应该用文本层面的数据集（一篇篇的文章），这是因为Transformer确实能处理整个文章层面的信息，比散乱的句子要好。</p><h3 id="3-2-微调的BERT-Fine-tuning-BERT"><a href="#3-2-微调的BERT-Fine-tuning-BERT" class="headerlink" title="3.2. 微调的BERT Fine-tuning BERT"></a>3.2. 微调的BERT Fine-tuning BERT</h3><p>BERT和一些基于编码器解码器的架构有什么不同。因为作者把整个句子对都放进去了，所以self attention能够在两端之间相互看，但是在编码器-解码器的架构里，编码器其实看不到解码器的信息，这一点在BERT里好一点。但是这一点也付出了代价，BERT不能像Transformer一样做机器翻译了。做下游任务的时候，会根据任务设计相关的输入和输出。好处是模型不怎么需要变，只需要把输入改成需要的那个句子对。如果真的有两个句子a和b的话，那么直接作为句子对输入进去就行，否则只有一个句子进行输入的话，要么是拿到第一个词元[CLS]做分类，要么是拿对应的词元去做对应的任务。不管如何，都是在最后加一个输出层，使用softmax得到一个输出的标号。</p><p>跟预训练比，微调的训练都相对便宜。所有的实验都只需要使用TPU跑一个小时就行了，使用GPU的话多跑几个小时也是可以的。具体对于每一个任务是如何构造输入输出的会在第四节进行介绍。</p><h2 id="4-实验-Experiments"><a href="#4-实验-Experiments" class="headerlink" title="4. 实验 Experiments"></a>4. 实验 Experiments</h2><h3 id="4-1-GLUE"><a href="#4-1-GLUE" class="headerlink" title="4.1. GLUE"></a>4.1. GLUE</h3><p>第一个是句子层面的任务GLUE。BERT是把第一个特殊词元[CLS]的最后的向量拿出来，学习一个输出层W，用softmax得到一个标号，就是一个正常的多分类问题。表1就是在GLUE所有任务上得到的一个结果展示。</p><p><img src="/img/2022-03-26-BERT-Notes/Tab1.png"></p><h3 id="4-2-SQuAD-v1-1"><a href="#4-2-SQuAD-v1-1" class="headerlink" title="4.2. SQuAD v1.1"></a>4.2. SQuAD v1.1</h3><p>这个是斯坦福的一个QA的数据集。在QA这个任务中，任务是给一段话，问一个问题，需要把答案找出来。答案已经在给出的那一段话里，只需要把答案对应的那一小段话找出来就可以了。任务其实就是对每个词元判断是不是答案的开头，是不是答案的结尾。BERT具体来说就是学两个向量S和E，对应的是这个词元是开头/结尾的概率。在做微调的时候使用的是3个Epoch（很小），学习率是5e-5，batch size是32。</p><h3 id="4-3-SQuAD-v2-0"><a href="#4-3-SQuAD-v2-0" class="headerlink" title="4.3. SQuAD v2.0"></a>4.3. SQuAD v2.0</h3><p>QA数据集的2.0版本</p><h3 id="4-4-SWAG"><a href="#4-4-SWAG" class="headerlink" title="4.4. SWAG"></a>4.4. SWAG</h3><p>用来判断两个句子的关系，和之前的训练没有太大区别，BERT结果也是比其他的要好。</p><h2 id="5-消融研究-Ablation-Study"><a href="#5-消融研究-Ablation-Study" class="headerlink" title="5. 消融研究 Ablation Study"></a>5. 消融研究 Ablation Study</h2><h3 id="5-1-Effect-of-Pre-training-Tasks"><a href="#5-1-Effect-of-Pre-training-Tasks" class="headerlink" title="5.1. Effect of Pre-training Tasks"></a>5.1. Effect of Pre-training Tasks</h3><p>如果去掉了NSP任务、或者去掉双向的机制会如何，发现BERT的效果都会打折，可见表5。</p><p><img src="/img/2022-03-26-BERT-Notes/Tab5.png"></p><h3 id="5-2-Effect-of-Model-Size"><a href="#5-2-Effect-of-Model-Size" class="headerlink" title="5.2. Effect of Model Size"></a>5.2. Effect of Model Size</h3><p>研究模型大小的影响。想法是和NLP其他模型是差不多的，认为当模型参数越来越大的时候，效果也会越来越好。认为这个是第一个展示了如果把模型变得特别大的时候，语言模型会有比较大的提升的工作。从现在角度来看，BERT的参数并不大，只有一个亿（GPT3的参数有一千个亿），甚至模型也在越来越大。但是在三年前，BERT是开创性的工作，能把模型推到这么大。</p><h3 id="5-3-Feature-based-Approach-with-BERT"><a href="#5-3-Feature-based-Approach-with-BERT" class="headerlink" title="5.3. Feature-based Approach with BERT"></a>5.3. Feature-based Approach with BERT</h3><p>假设作者不用BERT做微调的时候，而是把BERT的特征作为一个静态特征输进去会怎样。结论是效果确实没有微调那么好（用BERT的话应该在下游任务进行微调）。</p><h2 id="读后评论"><a href="#读后评论" class="headerlink" title="读后评论"></a>读后评论</h2><p>沐神：</p><ol><li>从写作上感觉还行，中规中矩。在这篇文章的结论里，他认为他最大的贡献是双向性，但是其实选用双向性这个词是有待商榷的。没有怎么写选用这个方法后失去了什么，比如很难做生成类的任务、机器翻译类的任务，因为相比Transformer或者GPT的架构，只有编码器而没有解码器。</li><li>从现在再回去看BERT的话，看到的其实是一个完整的一个解决问题的思路。符合了大家对一个深度学习的模型的期望，作者训练了一个很大很深的模型，在一个很大的数据集上训练好。这个模型训练后能通过微调用在很多小的数据集和任务上。这个在CV中用了很多年了，在NLP中BERT展示的是一个几个亿的参数训练了几百GB的数据集，很简单很暴力效果很好。沐神的疑惑：BERT是这些工作中的一篇，更大的数据集训练更好的模型，比前面都要好，类似ELMo和GPT出来的时候也是这样。BERT的作用是被后面的结果去超越。</li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理 NLP</tag>
      
      <tag>Transformer</tag>
      
      <tag>论文笔记</tag>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPT、GPT-2、GPT-3 论文笔记</title>
    <link href="/2022-03-27-GPT-Notes/"/>
    <url>/2022-03-27-GPT-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="GPT、GPT-2、GPT-3-论文笔记"><a href="#GPT、GPT-2、GPT-3-论文笔记" class="headerlink" title="GPT、GPT-2、GPT-3 论文笔记"></a>GPT、GPT-2、GPT-3 论文笔记</h1><h2 id="论文主要信息"><a href="#论文主要信息" class="headerlink" title="论文主要信息"></a>论文主要信息</h2><ul><li>标题： <ul><li>Improving Language Understanding by Generative Pre-Training（GPT）</li><li>Language Models are Unsupervised Multitask Learners （GPT-2）</li><li>Language Models are Few-Shot Learners（GPT-3）</li></ul></li><li>作者：<a href="https://www.semanticscholar.org/author/Alec-Radford/38909097">Alec Radford</a>, <a href="https://www.semanticscholar.org/author/Jeff-Wu/49387725">Jeff Wu</a>, <a href="https://www.semanticscholar.org/author/Rewon-Child/48422824">Rewon Child</a>, <a href="https://www.semanticscholar.org/author/D.-Luan/150970919">D. Luan</a>, <a href="https://www.semanticscholar.org/author/Dario-Amodei/2698777">Dario Amodei</a>, <a href="https://www.semanticscholar.org/author/Ilya-Sutskever/1701686">Ilya Sutskever</a></li><li>机构：OpenAI</li><li>来源：Computation and Language</li><li>代码：<a href="https://github.com/openai/gpt-3">https://github.com/openai/gpt-3</a></li><li>目前应用：<a href="https://gpt3demo.com/">https://gpt3demo.com/</a> <ul><li>生成一个以假乱真的技术博客</li><li>根据输入的要求生成一段HTML代码</li><li>GitHub Copilot</li><li>图灵测试问答机器人等等</li></ul></li></ul><h2 id="开始之前-GPT改进路线"><a href="#开始之前-GPT改进路线" class="headerlink" title="开始之前 GPT改进路线"></a>开始之前 GPT改进路线</h2><p><img src="/img/2022-03-27-GPT-Notes/Aft1.png"></p><p>首先GPT这篇文章是在Transformer出现之后运用了Transformer的编码器，做了一个预训练模型后在做下游任务的时候做微调的模型，GPT。GPT-2是在GPT的基础上把模型做得更大，朝着zero-shot的方向迈了一大步。GPT-3是在GPT-2的基础上暴力出奇迹，数据和模型都大了100倍，然后得到了很好的效果（详见GPT-3 demo等诸多应用）。</p><p>引用率相比BERT更少，约为1/2。沐神：不是因为创新度和效果不如谷歌的BERT系列，而是因为GPT选择解决更大的问题，所以技术上实现和出效果更难一些。GPT-3这个效果规模几乎是没有别的团队能够复现的。和OpenAI想做一个强人工智能的公司背景有关系。比如Transformer就是想解决机器翻译的问题，BERT就是想把CV界的预训练好的大模型在实际任务上微调的做法搬到NLP来。</p><h1 id="GPT-Improving-Language-Understanding-by-Generative-Pre-Training"><a href="#GPT-Improving-Language-Understanding-by-Generative-Pre-Training" class="headerlink" title="GPT | Improving Language Understanding by Generative Pre-Training"></a>GPT | Improving Language Understanding by Generative Pre-Training</h1><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>在自然语言理解里，有很多不一样的任务。虽然有很多大量的没有标号的文本内容，但是有标号的数据是相对较少的，这使得如果我们在labeled data上训练分辨模型的话还是比较难，因为数据相对来说比较的小。作者提出的解决方法是在没有标号的数据集上训练一个预训练的语言模型，接下来再在有标号的子任务上训练一个分辨的微调模型。作者是在微调的时候构造和任务相关的输入，从而只需要很少地改变模型的架构就行了。实验结果是12个任务里有9个任务超过了已有的成绩。</p><h2 id="1-导言-Introduction"><a href="#1-导言-Introduction" class="headerlink" title="1. 导言 Introduction"></a>1. 导言 Introduction</h2><p>如何利用无标号的文本数据？在当时最成功的模型还是词嵌入模型。用没有标号的文本的时候会遇到一些困难：第一是不知道用什么样的优化目标函数，损失函数。当时有一些选择是语言模型、机器翻译或者文本的一致性，但是发现没有某一个在所有的任务上都特别好。第二个难点是如何有效地把学到的文本的表示传递到下游的子任务上。因为NLP里的子任务差别很大，没有统一的方法能一致地迁移到子任务上。</p><p>GPT这篇文章提出了一个在没有标号的文本上进行的一个半监督（semi-supervised）的方法，训练出一个预训练模型后再在下游任务上微调。后面的研究工作基于BERT和GPT的，其实被叫做自监督模型（self supervised learning）而不是半监督学习。</p><p>首先第一个技术要点是，作者用到的模型是基于Transformer的架构，原因是：跟RNN这类模型相比，Transformer在迁移学习的时候学到的feature更加稳健一些。可能是因为Transformer里面有更结构化的记忆，使得能处理更长的文本信息，从而能抽取出来更好的句子层面和段落层面的语义信息。第二个技术点是在做迁移的时候用的是任务相关的输入表示，在后文有展示。</p><h2 id="2-相关工作-Related-Work"><a href="#2-相关工作-Related-Work" class="headerlink" title="2. 相关工作 Related Work"></a>2. 相关工作 Related Work</h2><p>NLP里的半监督学习是怎样的，无监督的预训练模型是怎样的，训练的时候使用多个目标函数会怎样（Auxiliary training objectives）。分别对应的是大的GPT模型在没有标号的数据上训练出来是如何的，以及怎么样在子任务上运用有标号的数据上进行微调，以及在子任务微调的时候作者使用了两个训练的目标函数。</p><h2 id="3-模型框架-Framework"><a href="#3-模型框架-Framework" class="headerlink" title="3. 模型框架 Framework"></a>3. 模型框架 Framework</h2><h3 id="3-1-无监督的预训练-Unsupervised-pre-training"><a href="#3-1-无监督的预训练-Unsupervised-pre-training" class="headerlink" title="3.1. 无监督的预训练 Unsupervised pre-training"></a>3.1. 无监督的预训练 Unsupervised pre-training</h3><p>假设输入的一个无标号的句子信息是$U=\lbrace u_1, …, u_n\rbrace$，作者使用了一个标准的语言模型目标去最大化下面这个似然函数：</p><p><img src="/img/2022-03-27-GPT-Notes/Eqn1.png"></p><p>语言模型就是预测第$i$个词出现的概率。具体到公式中就是给定模型，用一个长度为$k$的上下文窗口，每次拿$k$个连续的词，预测这些连续的词后面的那个词是谁。这个$L_1$是指第一个目标函数，因为取了log所以损失函数是相加。此处$\Theta$是模型的参数，$k$是超参数。用到的模型是Transformer的解码器。Transformer的编码器和解码器最大的不一样是，编码器对序列的第$i$个元素抽取特征的时候，能看到整个序列里所有的元素，但是对解码器而言，因为有掩码的存在，所以对第$i$个元素抽取特征的时候，只会看到当前元素和它之前的这些元素，后面的内容通过掩码使得计算注意力机制的时候贡献是0。所以对这个标准的语言模型（预测下一个词出现的概率）来讲，只能使用Transformer的解码器。下面的公式对Transformer的解码器进行了一定的解释：</p><p><img src="/img/2022-03-27-GPT-Notes/Eqn2.png"></p><p>GPT和BERT的区别：</p><p>BERT使用的不是标准的语言模型，而是完形填空，预测的是中间的句子，能看到前后的信息，所以能使用Transformer的编码器。主要区别在于目标函数的选取，相比BERT的完形填空，GPT选择的是预测未来这个较难的目标函数（信息较少），这也是训练和效果上GPT比BERT差一点的原因。但反过来如果模型是通过这样预测未来的方式训练出来并且能得到很好的效果，那么比BERT的完形填空训练方式得到的模型要强大很多，这也是后续GPT改进的一个重要方向，做大做强。</p><h3 id="3-2-有监督的微调-Supervised-fine-tuning"><a href="#3-2-有监督的微调-Supervised-fine-tuning" class="headerlink" title="3.2. 有监督的微调 Supervised fine-tuning"></a>3.2. 有监督的微调 Supervised fine-tuning</h3><p>在微调任务中，数据是有标号的。每次给一个长为$m$的句子$x^1,…,x^m$和一个标签$y$，根据句子去预测标签$y$的概率。要使得这个标准的分类的概率目标函数最大化。</p><p><img src="/img/2022-03-27-GPT-Notes/Eqn3.png"></p><p>在微调的时候只关心分类的精度，但如果把之前预训练的语言模型函数放进来也不错，也就是说微调的时候可以使用两个目标函数的时候训练效果是最佳的。这里的$\lambda$也是一个可以调的超参数。</p><p><img src="/img/2022-03-27-GPT-Notes/Eqn5.png"></p><p>接下来要考虑的是如何把NLP的一些不同的子任务表示成这样的一个通用的输入形式。</p><h3 id="3-3-针对不同的子任务的输入表示-Task-specific-input-transformations"><a href="#3-3-针对不同的子任务的输入表示-Task-specific-input-transformations" class="headerlink" title="3.3. 针对不同的子任务的输入表示 Task-specific input transformations"></a>3.3. 针对不同的子任务的输入表示 Task-specific input transformations</h3><p><img src="/img/2022-03-27-GPT-Notes/Fig1.png"></p><p>第一类是最常见的文本分类（Classification），比如对产品的评价是正面/负面。将要分类的文字在前面放一个开始（Start）的词元，后面做一个抽取（Extract）的词元，做成一个序列放进Transformer里面，把抽取的词元放到线性层（微调时新加入的）里投影到需要标号的空间。</p><p>第二类应用叫做蕴含（Entailment），给一段话，再给一个假设，判断前面这段话有没有蕴含假设提出来的东西。这个序列包含一个开始词元，分隔符（Delim）和抽取符。</p><p>第三类应用叫做相似（Similarity），给定两个文档是不是相似的，进行去重。因为相似是一个堆成的关系，如果a和b相似那么b和a相似的，但是语言模型是有先后顺序，所以这里做了两个序列，分别是ab和ba的句子顺序，两段序列分别进入模型后得到输出，再进行相加后输入到线性层得到结果。“是”或“不是”相似的一个二分类结果。</p><p>第四类应用叫做多选题（Multiple Choice），给出问题和几个答案，选出觉得正确的答案。如果有$n$个答案就构造$n$个序列，其中每个序列前面都是问题，后面就是答案。每个序列进入模型后再进入到一个线性投影层，输出的是每个答案是这个问题的答案的置信度。对每个答案计算这个标量，进行softmax后得到正确的答案置信度是多少。</p><p>不管任务是怎么变，核心的模型结构和输入表示都不会进行太大的改变。这是GPT和之前的文章一个比较大的区别，也是这篇文章的一个核心卖点。</p><h2 id="4-实验-Experiments"><a href="#4-实验-Experiments" class="headerlink" title="4. 实验 Experiments"></a>4. 实验 Experiments</h2><h3 id="4-1-Setup"><a href="#4-1-Setup" class="headerlink" title="4.1 Setup"></a>4.1 Setup</h3><p><strong>Unsupervised pre-training</strong>  在BooksCorpus这个数据集上进行训练，包含7000本没有发表的书。</p><p><strong>Model specifications</strong> 模型包含12个解码器，每一层的维度是768。这里就是$BERT_{base}$对标的GPT的版本。</p><p><strong>Fine-tuning details</strong> 介绍了微调的时候的超参数。</p><h3 id="4-2-Supervised-fine-tuning"><a href="#4-2-Supervised-fine-tuning" class="headerlink" title="4.2 Supervised fine-tuning"></a>4.2 Supervised fine-tuning</h3><p><img src="/img/2022-03-27-GPT-Notes/Tab3.png"></p><p>微调后对比之前的一些算法，效果都更加好。</p><h1 id="GPT-2-Language-Models-are-Unsupervised-Multitask-Learners"><a href="#GPT-2-Language-Models-are-Unsupervised-Multitask-Learners" class="headerlink" title="GPT-2 | Language Models are Unsupervised Multitask Learners"></a>GPT-2 | Language Models are Unsupervised Multitask Learners</h1><h2 id="摘要-Abstract-1"><a href="#摘要-Abstract-1" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>对比BERT采用的编码器，GPT系列采用的是解码器，所以如何在解码器上做到更好更强，就是GPT-2在BERT出来后要回应的问题。首先做了一个新的数据集叫WebText，有百万级别的网页文本。于是可以训练一个更大的模型（1.5B个参数，作为对比，BERT_large的参数是340M，文本变成了百万级别，模型参数3.4亿到15亿）。作者在GPT的基础上加入了zero-shot作为GPT-2的主要卖点。</p><h2 id="1-导言-Introduction-1"><a href="#1-导言-Introduction-1" class="headerlink" title="1. 导言 Introduction"></a>1. 导言 Introduction</h2><p>现在对ML systems的一个主流途径就是对一个任务收集一个数据集，然后在上面训练模型做预测。这个模型很流行是因为现在的模型的泛化性并不是很好，也就是说在一个任务上训练好的模型很难直接用到下一个任务上。多任务学习的观点是在训练一个模型的时候同时看多个数据集，可能通过增加多的损失函数来达到一个模型在多任务上都能使用（90年代末提出，00-10年比较流行的一个话题）。这个看上去很好，但是在NLP上用的不多，现在NLP主流的也是像之前GPT1和BERT那类的预训练后下游微调的模型。这样会造成两个问题：第一是对每个下游的任务还是得训练一个模型，第二个是也需要收集有标号的数据才行，这样拓展到新任务上还是有一定的成本。</p><p>GPT-2要干的事情是做语言模型的同时，在下游任务的时候要使用一个zero-shot的设定，做下有任务的时候不需要标注的信息，也不需要训练模型。</p><h2 id="2-方法-Approach"><a href="#2-方法-Approach" class="headerlink" title="2. 方法 Approach"></a>2. 方法 Approach</h2><p>GPT2的模型和GPT-1差不多。GPT-1的模型在预训练的时候是没有看到微调的时候构造的开始、分隔、抽取等词符的，而是在微调的时候去认识了这些词符。但是GPT-2如果要做zero-shot的话，也就是不进行微调，如果在下游引入了模型之前没有见过的词，模型会很困惑。所以在这样的设定下，下游任务就不能引入之前模型没有见过的符号，而是要使下游任务的输入和之前的输入形式要一样，输入的形式要更像自然的语言。</p><p>比如要做机器翻译的任务，可以输入一个序列(translate to French, english text, french text)，这在后面的研究中被称之为prompt(提示)。比如要做阅读理解的话，训练样例会被写成(answer the question, document, question, answer)。之后作者也花了较长笔墨解释为什么这样做是可行的，假如模型是足够强的话，也有前人的相关工作提到了这一点。</p><h3 id="2-1-训练数据集-Training-Dataset"><a href="#2-1-训练数据集-Training-Dataset" class="headerlink" title="2.1. 训练数据集  Training Dataset"></a>2.1. 训练数据集  Training Dataset</h3><p>前人用的是Wikipedia，或者是书，作者需要使用更大的数据集才行。作者提到一个可行的方法是一个叫做Common Crawl的项目，一群人写了一个爬虫，不断地去抓取网页，把抓取的网页放在aws的s3上面，供大家免费的下载（tb级别的数量级）。作者说这个数据集不好用，因为信噪比比较低，抓取到的网页里有大量比较垃圾的网页，如果要清理它需要花很长的时间。所以他使用了大家过滤好的网页，Reddit上至少有3个karma的帖子，最后得到了4500个链接，抽取了里面的文本信息，最后这个数据集里有大概800万个文本，40GB的文字。</p><p>作者一共设计了4个大小不同的模型。</p><p><img src="/img/2022-03-27-GPT-Notes/Tab2.png"></p><p>这张图表明了4个模型在不同的任务上取得的性能表现。在一些任务上做得还不错，别的任务上有一点点意思。（委婉说不太好）但是注意到随着模型变大，性能也是在上升的。</p><p><img src="/img/2022-03-27-GPT-Notes/Fig2-1.png"></p><h2 id="3-实验-Experiments"><a href="#3-实验-Experiments" class="headerlink" title="3. 实验 Experiments"></a>3. 实验 Experiments</h2><p>主要是和别的做zero-shot的SOTA方法进行比较，和BERT那一类不太一样。</p><p><img src="/img/2022-03-27-GPT-Notes/Tab2-3.png"></p><h1 id="GPT-3-Language-Models-are-Few-Shot-Learners"><a href="#GPT-3-Language-Models-are-Few-Shot-Learners" class="headerlink" title="GPT-3 | Language Models are Few-Shot Learners"></a>GPT-3 | Language Models are Few-Shot Learners</h1><p>文章长度有63页。不是投稿的文章，是技术报告。</p><h2 id="摘要-Abstract-2"><a href="#摘要-Abstract-2" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>作者训练了一个GPT-3模型，有175B个可学习的参数。和那些非稀疏（不会存在很多参数为0的模型）的模型比也是大了10倍。因为模型已经很大了，所以如果在做子任务的时候还要训练模型的话成本是很大的。所以GPT-3在作用到子任务上的时候不做任何的梯度更新或者微调。在NLP的任务上取得了很好的成绩（GPT-2的成绩挺差的）。GPT-3可以生成一些新闻文章，让人难以分辨是人写出来的还是生成的。</p><h2 id="1-导言-Introduction-2"><a href="#1-导言-Introduction-2" class="headerlink" title="1. 导言 Introduction"></a>1. 导言 Introduction</h2><p>最近NLP里大家都使用预训练的模型再微调。这是存在一定问题的，对每个子任务需要一个和任务相关的数据集和一个任务相关的微调。具体列举了三个问题：第一是大的数据集标注困难，第二是一个样本没有出现在数据分布里的时候，泛化性不一定比小模型微调后好（可能是过拟合预训练的训练数据），第三是人类不需要很大的数据集来做一个任务，可能通过几个例子就会掌握一个应用（few-shot）。</p><p>作者提出的解决方案是few-shot。作者取名叫meta-learning（训练了一个很大的模型，泛化性不错） / in-context learning（做下游任务即使告诉了训练样本，也不更新权重），强调的是模型权重在做下游任务的时候不做任何的更新。</p><p>GPT-3是一个有1750亿参数的模型，评估GPT-3的3个设定是：第一是few-shot learning，给每个任务提供10-100个样本，第二是one-shot learning，可以看成few-shot learning里特别的只有一个样本的情况，第三是zero-shot learning，没有训练样本直接进行子任务。下图展示了在三个设定下模型的区别：</p><p><img src="/img/2022-03-27-GPT-Notes/Fig3-1.3.png"></p><h2 id="2-方法-Approach-1"><a href="#2-方法-Approach-1" class="headerlink" title="2. 方法 Approach"></a>2. 方法 Approach</h2><p><img src="/img/2022-03-27-GPT-Notes/Fig3-2.1.png"></p><p>这里主要讲了微调、Few-shot、One-shot、Zero-shot具体是怎么做的。GPT-3这里就是不做梯度更新。这里Zero-shot的“=&gt;”就是一个prompt，提示该模型去进行输出了。One-shot就是在定义好任务后给一个例子（可以看作例子作为任务的一个输入，希望模型通过注意力机制从中抽取到有用的信息），只做预测，不做训练（不更新梯度），也就是上下文学习指的事情。Few-shot就是给多个例子。</p><h3 id="2-1-模型架构-Model-and-Architectures"><a href="#2-1-模型架构-Model-and-Architectures" class="headerlink" title="2.1. 模型架构 Model and Architectures"></a>2.1. 模型架构 Model and Architectures</h3><p>GPT-3的模型和GPT-2的模型是一样的。加入了一些Sparse Transformer里的改动，设计了8个不同大小的模型。</p><p><img src="/img/2022-03-27-GPT-Notes/Tab3-2.1.png"></p><p>批量大小是动辄百万级别，利用了分布式机器学习在数据并行性上更高的好处。批量大小变大的时候，批量里的噪音会很大，但是在大的模型中这样噪音的影响不会很大，也就是过拟合不太经常会发生。最近也有很多工作在研究其中的原因。</p><h3 id="2-2-训练数据集-Training-Dataset"><a href="#2-2-训练数据集-Training-Dataset" class="headerlink" title="2.2. 训练数据集 Training Dataset"></a>2.2. 训练数据集 Training Dataset</h3><p>由于GPT-3模型更大了，不得不重新考虑使用Common Crawl的数据了。在Common Crawl的数据集上基于GPT-2使用的高质量Reddit帖子的数据进行二分类，收集了更多质量偏高的文章。接下来做了一个去重的工作，具体用到的是lsh（信息检索，Information Retrieval）的算法。第三步也加了一些已知的高质量数据集，最后得到高质量的大的数据集。</p><p><img src="/img/2022-03-27-GPT-Notes/Tab3-2.2.png"></p><p>由于Common Crawl的质量还是不高，所以在batch上使用了不同的采样率，保证了更好的质量。</p><h3 id="2-3-训练过程-Training-Process"><a href="#2-3-训练过程-Training-Process" class="headerlink" title="2.3. 训练过程 Training Process"></a>2.3. 训练过程 Training Process</h3><p>（沐神：不“厚道”，GPT-3其实是非常难训练的。肯定有很复杂的模型分割和数据分割的过程，但是没有详细讲是怎么做这一步的）作者使用了微软的V100的DGX-1的集群，有很高的带宽。</p><h3 id="2-4-模型评估-Evaluation"><a href="#2-4-模型评估-Evaluation" class="headerlink" title="2.4. 模型评估 Evaluation"></a>2.4. 模型评估 Evaluation</h3><p>预训练好后直接进行评估，使用了上下文学习。下游任务采样$k$个样本，prompt用的是”Answer: “或者”A: “。二分类的结果是”True”或”False”，自由的答案就是生成后进行beam search（from 机器翻译）找到一个比较好的答案。</p><h2 id="3-结果-Results"><a href="#3-结果-Results" class="headerlink" title="3. 结果 Results"></a>3. 结果 Results</h2><p><img src="/img/2022-03-27-GPT-Notes/Fig3-3.1.png"></p><p>这张图展现的是不同大小模型的计算量上的区别，y轴是验证损失。每个模型最好的点拉成一条线是服从power law的分布，随着训练，损失是线性地往下降的。</p><p><img src="/img/2022-03-27-GPT-Notes/Fig3-3.2.png"></p><p>在LAMBADA任务上，和最好的Zero-shot和人类的表现进行了比较。</p><p><img src="/img/2022-03-27-GPT-Notes/Fig3-3.4.png"></p><p>机器翻译的结果。别的语言翻译到英语（实线）比英语翻译到别的语言（虚线）性能要好。</p><h2 id="5-局限性-Limitations"><a href="#5-局限性-Limitations" class="headerlink" title="5. 局限性 Limitations"></a>5. 局限性 Limitations</h2><p>虽然比GPT-2的效果好很多，但是在文本生成上效果是不太好的。假设需要生成一个很长的文章，如小说，可能会循环使用同样的文字进行生成，很难得到一个剧情类的内容向前推进。</p><p>有一些结构和算法上的局限性，因为GPT-3用的是语言类的模型，是往前看的，不能像BERT一样前后看。每次预测使用的是前面的所有的词，但是都有相同的权重，不一定能注意到重点的词。</p><p>由于只用了文本，没有使用视频或其他方面的素材，不够通用。样本的有效性不够。有一个不确定性是对于每个样本是从头开始学习还是从模型中找到了之前的任务，然后把它记住。</p><p>训练起来非常的贵。</p><p>GPT-3和很多别的深度学习的模型一样，是无法解释的。</p><h2 id="6-可能的影响-Broader-Impacts"><a href="#6-可能的影响-Broader-Impacts" class="headerlink" title="6. 可能的影响 Broader Impacts"></a>6. 可能的影响 Broader Impacts</h2><p>GPT-3这个模型已经很强大了，可以直接部署到生产环境里了。</p><h3 id="6-1-可能会被用来做坏事-Misuse-of-Language-Models"><a href="#6-1-可能会被用来做坏事-Misuse-of-Language-Models" class="headerlink" title="6.1. 可能会被用来做坏事 Misuse of Language Models"></a>6.1. 可能会被用来做坏事 Misuse of Language Models</h3><ol><li>散播不实信息，生成一些垃圾邮件，钓鱼邮件，论文造假。生成新闻都有些以假乱真了。</li><li>公平性、偏见。（男性/女性的偏见）（种族、宗教等等）</li><li>能耗。训练GPT-3需要几百台机器训练很多天。</li></ol><h2 id="8-结论-Conclusion"><a href="#8-结论-Conclusion" class="headerlink" title="8. 结论 Conclusion"></a>8. 结论 Conclusion</h2><p>我们做了一个175B参数的语言模型，在许多的NLP任务上做了zero-shot, one-shot, few-shot的学习，在很多情况下可以媲美使用更多带标号数据的基于微调的算法。一个卖点是能生成很多高质量的成本，展示了一个不用基于微调的可能性。</p><h1 id="读后评论"><a href="#读后评论" class="headerlink" title="读后评论"></a>读后评论</h1><p>沐神：</p><p>GPT系列在一开始选择了Transformer的二选一里的解码器，可能走了更难的路，但是作者继续进行尝试改进，得到了更好的结果。展示了语言模型能暴力出奇迹的。</p><p>我：</p><p>让我试试这些GPT-3 demo！！找到一个可用的，Blog Idea Generator。感觉像是高级版的废话生成器哈哈哈。GitHub Copilot需要体验权限，如果有了我再试试看，看起来的效果也太猛了。</p><p><img src="/img/2022-03-27-GPT-Notes/Aft2.png"></p><p><img src="/img/2022-03-27-GPT-Notes/Aft3.png"></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理 NLP</tag>
      
      <tag>Transformer</tag>
      
      <tag>论文笔记</tag>
      
      <tag>GPT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer 论文笔记</title>
    <link href="/2022-03-12-Transformer-Notes/"/>
    <url>/2022-03-12-Transformer-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer-论文笔记"><a href="#Transformer-论文笔记" class="headerlink" title="Transformer 论文笔记"></a>Transformer 论文笔记</h1><h2 id="论文主要信息"><a href="#论文主要信息" class="headerlink" title="论文主要信息"></a>论文主要信息</h2><ul><li>标题：Attention Is All You Need</li><li>作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</li><li>机构：Google Brain, Google Research</li><li>来源：NIPS2017</li><li>代码：<a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a></li></ul><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>序列转录模型主要依赖于复杂的循环或者是卷积神经网络，卷积神经网络一般会使用一个encoder和decoder。在性能最好的模型中，通常也会在编码器和解码器之间使用一个叫注意力机制的东西。</p><p>这篇文章提出了一个新的简单的架构，Transformer。这个模型仅仅依赖于注意力机制，没有使用循环或者是卷积。做了两个机器翻译的实验，显示这个模型在性能上很好。并行度更好，并且用更少的时间来训练。在WMT 2014 英语到德语的任务上拿到了28.4 BLEU，比最好的结果高了两个BLEU。在WMT 2014 英语到法语上，做了一个单模型，比所有模型效果都要好，拿到了41.8 BLEU，只在8个GPU上训练了3.5天。Transformer可以泛化到别的任务上效果也特别好。</p><h2 id="7-结论-Conclusion"><a href="#7-结论-Conclusion" class="headerlink" title="7. 结论 Conclusion"></a>7. 结论 Conclusion</h2><p>介绍了Transformer这个模型，是第一个做序列转录的模型，仅仅使用注意力，将所有的循环层换成了multi-headed self-attention。</p><p>在机器翻译这个任务上，Transformer能训练得比其他架构快很多，并且能得到更好的结果。作者对这种纯注意力的机制感到非常的激动，想用在别的任务上面。可以用在文本之外的数据，包括图片、语音、视频，使得生成不那么时序化，也是一个研究方向。</p><p>这篇文章所有代码放在了tensor2tensor的库中。</p><h2 id="1-导言-Introduction"><a href="#1-导言-Introduction" class="headerlink" title="1. 导言 Introduction"></a>1. 导言 Introduction</h2><p>在时序模型中，当前最常用的是RNN（2017），包括LSTM，GRU。在这里面有两个比较主流的模型，一个是语言模型，一个是在输出结构化信息比较多的时候，会使用一个编码器和解码器的架构。</p><p>RNN的特点（同时也是缺点）是给定一个序列的时候，它的计算是这个序列从左往右一步一步做，假设是一个句子的话，就是从左往右一个一个看，对第$t$个词会计算一个值$h_t$，是它的隐藏状态。这个$h_t$是由前面一个词$h_{t-1}$和当前第$t$个词本身决定的。就可以把前面学到的信息通过$h_{t-1}$传到当下，然后和当前的词计算得到输出。</p><p>这是RNN处理时序信息的关键，问题也来自于这里。第一是它的计算是一步一步计算的过程，难以并行。第二是历史信息是一步一步向后传递的，如果时序很长的话，那么较靠前的一些信息在后面可能会被丢掉。过去尝试了一些分解提高并行度等等的改进方法，但是本质还是没有很好的解决掉这个问题。</p><p>在这篇文章之前，Attention已经成功在RNN的编码器和解码器中有了应用。它主要用在怎样把编码器的东西很有效地传递到解码器。</p><p>这篇文章提出的Transformer提出了一个新的模型，不在基于之前的循环神经层，而是纯基于注意力机制了。这个东西是可以并行的，所以能在更短的时间内达到更好的效果。</p><h2 id="2-相关工作-Background"><a href="#2-相关工作-Background" class="headerlink" title="2. 相关工作 Background"></a>2. 相关工作 Background</h2><p>有工作是如何使用卷积神经网络替换掉循环神经网络，减少时序的计算。这些工作主要的问题是：卷积神经网络对于比较长的序列比较难建模。如果两个像素隔得很远的话，得通过很多层卷积才能把它们联合在一起。如果使用Transformer里的注意力机制的话，每一次可以看到所有的像素，每一层就能看到整个序列。卷积比较好的一点是可以做多个输出通道，一个输出通道可以认为是它去识别不一样的模式，作者也想要这样多输出的效果，所以作者提出了一个叫做Multi-Headed Attention，可以模拟卷积神经网络多输出的效果。</p><p>自注意力机制，是Transformer比较重要的一个东西，但是在之前也有相关工作已经提出来了。</p><p>Memory Networks也是之前的一个研究重点。</p><p>在作者的best knowledge里面，Transformer是第一个只依赖自注意力机制来做encoder到decoder之间的工作的模型。</p><h2 id="3-模型架构-Model-Architecture"><a href="#3-模型架构-Model-Architecture" class="headerlink" title="3. 模型架构 Model Architecture"></a>3. 模型架构 Model Architecture</h2><p>序列模型中现在比较好的一个模型是encoder-decoder的架构。编码器会将一个输入用$(x_1, …, x_n)$表示，比如一个句子，$x_t$用来表示第$t$个词。这个编码器的输出是一个$z = (z_1, …, z_n)$，里面的$z_t$就是对应$x_t$这个词的一个向量的表示。原始的输入变成了机器学习可以理解的一个向量。对于解码器，解码器会拿到编码器的输出$z$，然后生成一个长为$m$的序列（$m, n$可能是不一样长的）$(y_1, …, y_n)$，对解码器来说，和编码器最大的区别是这个词是一个一个生成的。在编码器的时候，编码器能一次性看到整个句子，但是在解码的时候，只能一个一个生成，这个是叫做自回归（auto-regresive）的一个模型，因为在这个模型中输出也是输入，比如在生成$y_t$的时候，其实已经可以拿到$(y_1, …, y_{t-1})$作为输入。</p><p>Transformer也是使用了一个encoder-decoder的架构，使用了self-attention和point-wise, fully connected layers连在了一起。下面的Fig 1.展示这个架构：</p><p><img src="/img/2022-03-12-Transformer-Notes/Fig1.png"></p><p>左边是编码器，右边是解码器。</p><p>编码器的输入是表述句子的向量，加上了Positional Encoding。进去之后会变成$n$个Transformer的块堆在一起的一个结构，有一个Multi-Head后的前馈神经网络，从输入到Add &amp; Norm有一个残差的连接。可以看成一个MLP（多层感知机）。然后输出接入到解码器的中间。</p><p>解码器的输入是之前的输出，一个一个向右移的（shifted right）。输出连接上编码器的输出后继续做一个前馈神经网络，最后输出后做一个softmax得到输出。</p><h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1. Encoder and Decoder Stacks"></a>3.1. Encoder and Decoder Stacks</h3><p>介绍具体的Encoder和Decoder是怎么设计的。</p><p>Encoder有一个$n = 6$层。每一层有两个子层。第一个子层是multi-head self-attention，第二个子层是用的一个简单的”MLP”。对于每个子层使用了残差连接。然后使用了layer normalization。输出的公式可以写成$LayerNorm(x+Sublayer(x))$，因为残差连接需要输入和输出大小一样，为了简单起见，让每个层输出的维度都是$d_{model} = 512$，固定长度。</p><blockquote><p>什么是Layer Norm，和Batch Norm做对比。</p></blockquote><p>Batch Norm，对每个特征做normalization，在训练的时候把每个小批量的地方算出一个方差均值。在预测的时候会把全局的一个均值算出来，整个数据扫一遍之后把平均的数值存起来，batch norm的时候使用。batch norm还会学习到一个$\lambda _1 \beta$的值，把向量通过学习放成方差和均值为任意一个值的东西。</p><p><img src="/img/2022-03-12-Transformer-Notes/Aft1.png"></p><p><img src="/img/2022-03-12-Transformer-Notes/Aft2.png"></p><p>Layer Norm，对每一个样本做normalization，每一行变成均值为0，方差为1。这样让方差的抖动变小，因为对于不同长度的预测样本切出来计算的均值方差更加稳定，相比Batch Norm。</p><p>Decoder和Encoder很像，也是由$n=6$层一样的层组成的，有两个子层的内容是和Encoder里一样的。不一样的是有一个第三部分的子层，同样是一个多头的注意力机制，同样使用了残差连接和Layer Norm。在解码的时候使用了自回归，当前的输入是上面的输出。由于在注意力机制的时候能看到全部的输入信息，为了防止在$t$时间的时候看到了$t$时间之后的内容，所以使用了一个带掩码的多头注意力机制。</p><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2. Attention"></a>3.2. Attention</h3><p>注意力函数是将一个query和一些key-value对映射成输出的函数，这些query, key, value, output都是向量。输出的计算是一个value的加权和。这个权重是从这个value对应的key和query的相似度计算出来的。这个相似度（或者compatibility function）在不同的注意力机制有不同的算法。</p><p><img src="/img/2022-03-12-Transformer-Notes/Aft3.png"></p><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1. Scaled Dot-Product Attention"></a>3.2.1. Scaled Dot-Product Attention</h4><p>这一节介绍了Transformer中使用的注意力机制。query和key的长度维度都是$d_k$，value的长度是$d_v$。具体的计算是key和query做内积，做相似度。内积的值越大，相似度越大。</p><p><img src="/img/2022-03-12-Transformer-Notes/Eqn1.png"></p><p>实际中如果每次query都做这个计算会很慢，所以使用Q作为一个query的矩阵，K是key-value的keys矩阵。softmax是对每一行做softmax，行与行之间是独立的，就可以得到每个query对应的权重。权重乘上V这个value的矩阵就得到了需要的输出。</p><p><img src="/img/2022-03-12-Transformer-Notes/Aft4.png"></p><p>因为是矩阵乘法的形式，所以很好做并行，能很快地进行计算。</p><p>有两种比较常见的注意力机制，第一种叫加型的注意力机制，可以处理query和key不等长的情况，另一种叫做点积的注意力机制，也就是作者使用的注意力机制不除以$\sqrt {d_k}$。作者说这两种注意力机制差不多，他这里选择了点乘，因为实现起来比较简单，而且非常高效，两次矩阵乘法就能算好。当$d_k$比较大的时候，点积产生的相对差距会更大，softmax后权重就会更加极端，这样算梯度的时候梯度会比较小，跑不动。所以Transformer中对应$d_k = 512$，除以$\sqrt {d_k}$的效果是比较好的。</p><p><img src="/img/2022-03-12-Transformer-Notes/Fig2.png"></p><p>这个矩阵乘法的过程是Fig. 2中左半部分展示的。Mask是为了避免前面时间节点的输出看到靠后时间节点的信息，因为注意力机制实际上会看到全部的信息，Mask可以看成把时间$t$后的数据都变成一个非常大的负数，这样经过了softmax的时候就会非常接近0。这样可以使得做预测的时候和时间信息是一一对应上的。</p><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2. Multi-Head Attention"></a>3.2.2. Multi-Head Attention</h4><p>Fig. 2的右半部分讲的是Multi-Head Attention具体的设计。作者说与其做一个单独的注意力函数，不如把整个query, key-value投影到低维$h$次，再做$h$次的注意力函数，每一个函数的输出并在一起投影回来，得到最终的输出。通过这样的多头设计，希望这个注意力函数能识别更多不同的模式。</p><p><img src="/img/2022-03-12-Transformer-Notes/Eqn1-1.png"></p><h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3. Applications of Attention in our Model"></a>3.2.3. Applications of Attention in our Model</h4><p>在作者提出的模型中，Fig. 1中黄色的地方表示的是注意力层。 注意力机制有三种使用的情况：</p><ul><li>每一个Multi-Head Attention的输入是K, V, Q三个矩阵，同样的东西使用了三遍，其实三个都是一个东西，这被叫做自注意力机制。</li><li>编码器和解码器的第一层都是一样的把相同的输入内容复制成三份进行。解码器不同的是有一个Masked的处理，所以在解码器里后面的内容需要设置为0。</li><li>第三个注意力层是解码器的第二个子层，Key和Value是来自于编码器的输出，Query是来自于解码器的上一层。相当于通过注意力机制在编码器生成出来的可能结果中挑选最感兴趣的一个作为下一个输出的结果。</li></ul><p><img src="/img/2022-03-12-Transformer-Notes/Aft5.png"></p><h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3. Position-wise Feed-Forward Networks"></a>3.3. Position-wise Feed-Forward Networks</h3><p>蓝色的子层，其实是一个MLP（多层感知机）。其实是把MLP对每一个词作用一个，对每个词作用的是同一个MLP，所以对于不同位置的词会有不同的信息，所以叫Position-wise。</p><p><img src="/img/2022-03-12-Transformer-Notes/Eqn2.png"></p><p>这里的$x$其实就是输出的向量，$d_{model} = 512$，$W_1$对$x$进行一个投影操作后把它变成$d_{ff}=2048$，维度扩大了四倍。进行残差连接后投影回去维度回到512。相当于单隐藏层的MLP。</p><p><img src="/img/2022-03-12-Transformer-Notes/Aft6.png"></p><p>这是Transformer和RNN对于MLP的使用不同的一个对比。重点在于RNN是通过将上一层的MLP信息输入到下一层去获取序列信息的。</p><h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4. Embeddings and Softmax"></a>3.4. Embeddings and Softmax</h3><p>需要把词对应成一个长度为$d$的向量进行表示，在Embedding层里，乘以了一个$\sqrt {d_{model}}$，防止加上Positional Encoding后因为scale不同被影响太多。</p><h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5. Positional Encoding"></a>3.5. Positional Encoding</h3><p>Attention这个东西是不会有序列信息的，只会通过key-value和query进行计算。给出一句话后词的顺序信息会丢失。Transformer在输入里加入时序信息。</p><h2 id="4-为什么选择自注意力机制-Why-Self-Attention"><a href="#4-为什么选择自注意力机制-Why-Self-Attention" class="headerlink" title="4. 为什么选择自注意力机制 Why Self-Attention"></a>4. 为什么选择自注意力机制 Why Self-Attention</h2><p>为什么要使用自注意力机制。相对使用循环层和卷积层来说，自注意力具有什么样的优点。</p><p><img src="/img/2022-03-12-Transformer-Notes/Tab1.png"></p><p>比较了四种层，第一种是自注意力，第二种是循环层，第三种是卷积层，第四种是受限制的自注意力层。比较的三点是每层的计算复杂度（越小越好），每次序列操作要等待的复杂度（越小并行性越好），第三个是信息从一个数据点到另一个数据点要走多远（越小越好）。</p><h2 id="5-实验-Training"><a href="#5-实验-Training" class="headerlink" title="5. 实验 Training"></a>5. 实验 Training</h2><h3 id="5-1-Training-Data-and-Batching"><a href="#5-1-Training-Data-and-Batching" class="headerlink" title="5.1. Training Data and Batching"></a>5.1. Training Data and Batching</h3><p>使用了英语-德语的一个37000个token的字典，在两种语言的Embedding的信息是共享的。</p><h3 id="5-2-Hardware-and-Schedule"><a href="#5-2-Hardware-and-Schedule" class="headerlink" title="5.2. Hardware and Schedule"></a>5.2. Hardware and Schedule</h3><p>使用了8块P100的显卡进行训练。Base模型使用小的参数，每一个batch用0.4s，在8张卡上用12小时训练了100000步，能得到不错的性能。在一个大的参数里，每一个batch用1.0s，大模型花了3.5天训练了300000步。</p><h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3. Optimizer"></a>5.3. Optimizer</h3><p>使用了Adam。学习率几乎不用调。</p><p><img src="/img/2022-03-12-Transformer-Notes/Eqn3.png"></p><h3 id="5-4-Regularization"><a href="#5-4-Regularization" class="headerlink" title="5.4. Regularization"></a>5.4. Regularization</h3><p>使用的正则化有：</p><ol><li>Residual Dropout。对每一个子层，包括多头注意力层和MLP，在输入上，在进入残差连接之前，进行了一个dropout，$P_{drop}=0.1$</li><li>Label Smoothing。在用softmax学东西的时候，正确的是逼近1，错误的是逼近0。但是其实softmax里很难逼近1，所以训练比较难。所以把正确的词的阈值的置信度是0.1就行了。这里会损失perplexity，可能会损失模型的确信度。但是置信度不那么高会提升精度和BLEU的分数。</li></ol><p><img src="/img/2022-03-12-Transformer-Notes/Tab2.png"></p><p>使用的一些超参数如上表。对比了不同超参数下的一些实验结果。</p><h2 id="读后评论"><a href="#读后评论" class="headerlink" title="读后评论"></a>读后评论</h2><p>沐神：</p><ol><li>写作是非常简洁的。不太推荐这么写，没有那么多故事让读者有一个很好的代入感，但是这篇文章有发现很多的东西，所以可能没有篇幅这么写。建议写文章的时候有更多的为什么做这个东西，更能说服读者的一些自己的思考。</li><li>Transformer这个模型本身。这个模型可以让大家训练很多很大易训练的模型。很像计算机视觉中的CNN的地位。不用管那么多如何特征提取，数据预处理的工作。预设的模型也让大家的工作比较简单。而且Transformer不仅仅在NLP方向上取得了成绩，在很多领域都能使用，影响力是很大的。能极大减少机器学习对不同领域产生影响的时间。</li><li>并不只是需要Attention就行了，只是将序列化的信息统一在一起集合起来了，起到了很关键的作用。但是如果没有后续的MLP等等，也是没有Transformer的。</li><li>为什么能打败RNN呢？现在大家觉得他使用了一个更广泛的归纳偏置。Attention没有做空间上的任何假设，也能得到和RNN差不多的效果。但是可能对数据的抓取效果变差了，所以可能需要更大的模型才有同样的训练效果。</li></ol>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理 NLP</tag>
      
      <tag>Transformer</tag>
      
      <tag>论文笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ResNet 论文笔记</title>
    <link href="/2022-03-06-ResNet-Notes/"/>
    <url>/2022-03-06-ResNet-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="ResNet-论文笔记"><a href="#ResNet-论文笔记" class="headerlink" title="ResNet 论文笔记"></a>ResNet 论文笔记</h1><h2 id="论文主要信息"><a href="#论文主要信息" class="headerlink" title="论文主要信息"></a>论文主要信息</h2><ul><li>标题：Deep Residual Learning for Image Recognition</li><li>简称：ResNet</li><li>作者：Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun </li><li>机构：Microsoft Research</li><li>来源：CVPR 2015</li><li>代码：<a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py">https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py</a></li></ul><h2 id="摘要-Abstract"><a href="#摘要-Abstract" class="headerlink" title="摘要 Abstract"></a>摘要 Abstract</h2><p>深的神经网络非常难以训练，于是做了一个残差学习的框架，使得训练非常深的神经网络，比之前轻松了很多。使用residual functions作为层输入去学习，而不是使用之前的unreferenced functions。</p><p>论文里提供了非常多的<strong>实验的证据</strong>，证明残差神经网络非常容易训练，而且能得到很好的训练精度，尤其是在神经网络层数加深之后。</p><ul><li>在ImageNet上使用了152层深度的ResNet，比VGG网络深8倍，但是计算复杂度更低。</li><li>在ImageNet测试集上达到了3.57%的错误率，赢下了ImageNet 2015年的竞赛。</li><li>也演示了在CIFAR-10上训练100和1000层的网络。</li></ul><p>对很多视觉的任务来说，深度是非常重要的。仅仅将网络换成之前训练的残差神经网络（深的），在COCO目标检测数据集上得到了28%的相对改进。也通过这个结果赢下了物体检测方面的一系列竞赛。</p><h2 id="1-导言-Introduction"><a href="#1-导言-Introduction" class="headerlink" title="1. 导言 Introduction"></a>1. 导言 Introduction</h2><p>深度卷积神经网络在图像分类上带来了一系列的突破。主要是在深度神经网络中会很自然地（不同层数）得到一些low/mid/high level的特征，用于识别图像。</p><p>思考深度对于神经网络效果提升的有效性，可以提出一个问题：<strong>一个好的神经网络就是直接把更多的层堆积起来就更好吗？</strong></p><ul><li>当网络加深的时候，可能会出现gradient vanishing/exploding的问题，对于这个问题可以使用一些方法来解决（比如正则初始化，BN等）。</li><li>但是另一个问题是，随着网络深度加深，其实性能/精度是变差的。这个不是因为层数变多，模型变大后导致的过拟合（因为训练误差也变大了）。</li></ul><p><img src="/img/2022-03-06-ResNet-Notes/Fig1.png"></p><p>对于第二个问题深入探讨：对于一个结构层数比较浅的神经网络，如果能得到不错的训练效果，那么在上面添加更多的层数，其实理论上应该是能得到更好的表现的，即使是新加的层数相比之前就是一个identity mapping，也不应该让误差更大。但是实验的结果中可以看到， stochastic gradient descent (SGD) （随机梯度下降）找不到这个更优的解，甚至可能层数加深后表现更差。</p><p>接下来作者介绍的是 deep residual learning framework，假设在一个已有的输出结果为$x$的层后添加新的层，假设要学习的目标是$H(x)$，那么就让真实的优化目标不是$H(x)$，而是减去原来输出结果$x$的$F(x):=H(x)-x$，并且在这一层的输出结果中把原来的训练结果加上，输出$F(x)+x$。<strong>意思是在这一层的训练内容中，不去重复训练已有的训练结果，而是去训练这个结果和这一层训练目标之间的残差（Residual）（</strong>$H(x)-x$<strong>而不是</strong>$H(x)$<strong>）</strong></p><p><img src="/img/2022-03-06-ResNet-Notes/Fig2.png"></p><p>$F(x)+x$这个动作可以在前向神经网络中通过”shortcut connections”实现。这个就是一个identity mapping，而且加的东西没有任何新的参数，只是一个加法，不会有模型计算复杂度上的增加。</p><p>通过实验证明了两点：</p><ol><li>文章提出的residual的网络非常容易优化，但是如果加入”plain”的网络，随着深度加深的同时会得到更高的训练误差。</li><li>residual的网络深度越深，训练的效果就越好</li></ol><p>以上的结果在CIFAR-10和ImageNet中都有体现。</p><h2 id="2-相关工作-Related-Work"><a href="#2-相关工作-Related-Work" class="headerlink" title="2. 相关工作 Related Work"></a>2. 相关工作 Related Work</h2><h3 id="Residual-Representation"><a href="#Residual-Representation" class="headerlink" title="Residual Representation"></a>Residual Representation</h3><p>图像识别中的VLAD是通过字典的残差向量进行编码。还有Fisher Vector是VLAD的一个概率上的表示。</p><p>Low-level vision和计算机图形学中求解 Partial Differential Equations (PDEs) 偏微分方程有一个广泛使用的方法Multigrid method。</p><p>（沐神：在机器学习中其实使用更广泛，使用residual训练一些弱的分类器叠加起来成为一个强的分类器。这篇论文可能发布在CVPR上主要回顾的是CV相关的工作。）</p><h3 id="Shortcut-Connections"><a href="#Shortcut-Connections" class="headerlink" title="Shortcut Connections"></a>Shortcut Connections</h3><p>“highway networks”等等都是比较复杂的运用方法，但是在ResNet中只是比较简单的累加的运用。</p><h2 id="3-Deep-Residual-Learning"><a href="#3-Deep-Residual-Learning" class="headerlink" title="3. Deep Residual Learning"></a>3. Deep Residual Learning</h2><h3 id="3-1-Residual-Learning"><a href="#3-1-Residual-Learning" class="headerlink" title="3.1. Residual Learning"></a>3.1. Residual Learning</h3><p>基本和导言中提到的内容是一致的，相比直接在$x$的输出结果基础上训练目标为$H(x)$的目标函数，改为先去掉上一层的输出结果，训练$F(x):=H(x)-x$，在训练后再加上原先的$x$，输出$F(x)+x$。</p><h3 id="3-2-Identity-Mapping-by-Shortcuts"><a href="#3-2-Identity-Mapping-by-Shortcuts" class="headerlink" title="3.2. Identity Mapping by Shortcuts"></a>3.2. Identity Mapping by Shortcuts</h3><p>Fig.2 展示的就是整个网络中的一部分，对应的也就是Eqn.(1)</p><p><img src="/img/2022-03-06-ResNet-Notes/Eqn1.png"></p><p>在Eqn.(1)中，要求$ F(x,\lbrace W_i\rbrace ) $和$x$的维度是一样的，这样才能相加，否则就要进行投影或者别的方式，在公式上可以体现为乘了一个新的矩阵$W_s$</p><p><img src="/img/2022-03-06-ResNet-Notes/Eqn2.png"></p><h3 id="3-3-Network-Architectures"><a href="#3-3-Network-Architectures" class="headerlink" title="3.3. Network Architectures"></a>3.3. Network Architectures</h3><p>残差连接如何处理输入和输出的维度是不等的情况。</p><p>两个方案：</p><ol><li>在输入和输出上添加一些额外的0，使得输入输出的维度相同。</li><li>在1x1的卷积上进行投影，增加输出通道数。（步幅为2）</li></ol><h3 id="3-4-Implementation"><a href="#3-4-Implementation" class="headerlink" title="3.4. Implementation"></a>3.4. Implementation</h3><p>ImageNet上的实现：</p><ol><li>在短边上随机采样到[256, 480]。好处是在裁剪到224*224的时候随机性会更多一些。</li><li>在颜色上进行了一定的增强。类似亮度、饱和度。</li><li>使用了Batch Normalization(BN)。</li><li>初始权重和[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 这篇文章中一样。</li><li>随机梯度下降（SGD）的批量大小是256，学习率是0.1，每次错误率比较平的时候除以10。</li><li>训练了$60 \times 10^4$次iterations，weight decay是0.0001，momentum是0.9。</li><li>没有使用dropout（因为没有全连接层）。</li></ol><p>测试的时候：</p><ol><li>使用了standard 10-crop testing（随机采样10个图片，进行预测求平均）。</li><li>测试的时候使用了几个不同的分辨率$\lbrace224, 256, 384, 480, 640 \rbrace$</li></ol><h2 id="4-实验-Experiment"><a href="#4-实验-Experiment" class="headerlink" title="4. 实验 Experiment"></a>4. 实验 Experiment</h2><h3 id="4-1-ImageNet-Classification"><a href="#4-1-ImageNet-Classification" class="headerlink" title="4.1. ImageNet Classification"></a>4.1. ImageNet Classification</h3><p>在ImageNet 2012分类数据集上实现ResNet。这个数据集包含1000个类别。这个模型在1.28 million的训练集上进行训练，验证集为50k。最后在100k的测试图片上得到了top-1和top-5的错误率。</p><p>对比的是<strong>Plain Networks</strong>。18-layers和34-layers的版本。</p><p><img src="/img/2022-03-06-ResNet-Notes/Fig3.png"></p><p>Table 1.是不同深度版本的ResNet的架构：</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab1.png"></p><p>初始的池化层和最后的全连接层的架构是一样的，中间是不一样的。</p><p>$\lbrace 64, 128, 256, … \rbrace$表示的是通道的数量。FLOPs，需要进行的浮点计算是可以计算出来的，体现出来深度加深之后其实运算的复杂度没有提高很多。</p><p>在Table 2.中对比了Plain Networks在34-layer的时候比18-layer具有更高的训练误差：</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab2.png"></p><p>Fig 4.左边是Plain Network，右边是ResNet：</p><p><img src="/img/2022-03-06-ResNet-Notes/Fig4.png"></p><p>粗线是测试精度，细线是训练精度。下降较明显的是学习率*0.1，改变梯度下降的步子。</p><p>主要展现的是有残差连接后：</p><ol><li>网络深层越深，训练和测试误差越低。</li><li>训练的时候收敛更快。</li></ol><p>在Table 3.中比较了Shortcut Connection的三种方案：</p><p>A. 在维度形状不同的时候添加0。</p><p>B. 在维度不同的时候使用投影增加输出通道。</p><p>C. 不管维度是否相同，都使用投影。</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab3.png"></p><p>B和C基本对A都有较大的提升。C看起来会更好一点，但是每次都做投影会带来更高的计算复杂度，所以在ResNet实现的时候选择使用了B方案，只在形状改变的时候进行投影（大约只有4次改变）。</p><p> <strong>Deeper Bottleneck Architectures</strong> </p><p>如何加深ResNet的深度，引入了一个叫做bottleneck design的东西，如Fig. 5：</p><p><img src="/img/2022-03-06-ResNet-Notes/Fig5.png"></p><p>通道数变大，从64-d到了256-d，因为直接计算的话是平方级的复杂度，会多16倍，所以进行投影到了原来的64-d。训练后再投影到256-d，这样计算复杂度会低很多。对应了Table 1.中更深层次的ResNet的设计。</p><p>Table 4. 展示随着深度加深，误差越来越小：</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab4.png"></p><p>Table 5. 对比的是其他的方法在ImageNet测试集上的表现：</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab5.png"></p><h3 id="4-2-CIFAR-10-and-Analysis"><a href="#4-2-CIFAR-10-and-Analysis" class="headerlink" title="4.2. CIFAR-10 and Analysis"></a>4.2. CIFAR-10 and Analysis</h3><p>CIFAR-10的数据集上基本是32*32的小图片，比ImageNet小很多，所以在CIFAR-10层上进行了新的设计的ResNet。Table 6.展示了1000层的ResNet误差率会比100层的高一些。</p><p><img src="/img/2022-03-06-ResNet-Notes/Tab6.png"></p><p>Fig 6. 主要表示的还是加入residual后效果还是比plain的网络要好很多。</p><p><img src="/img/2022-03-06-ResNet-Notes/Fig6.png"></p><p>Fig 7. 表示的是随着深度的加深，Standard deviations (std) 标准差的变化。而且可以看到的是，因为有投影的过程，所以可能在改变维度形状的时候标准差也是有波动的，但是如果按照维度大小排序后，整个网络的标准差是收敛的：</p><p><img src="/img/2022-03-06-ResNet-Notes/Fig7.png"></p><p>可能是因为ResNet学的是残差，在深度加深很多后没有新的东西可以学了，所以1000层的表现没有更好。</p><h3 id="4-3-Object-Detection-on-PASCAL-and-MS-COCO"><a href="#4-3-Object-Detection-on-PASCAL-and-MS-COCO" class="headerlink" title="4.3. Object Detection on PASCAL and MS COCO"></a>4.3. Object Detection on PASCAL and MS COCO</h3><p>展现了在目标检测上的数据集结果页很好。Detail在Appendix里。</p><p>（沐神：这篇文章没有Conclusion，全都是展示实验结果。）</p><h2 id="读后笔记"><a href="#读后笔记" class="headerlink" title="读后笔记"></a>读后笔记</h2><p>沐神：</p><p>在加入更多层次的网络的时候，如果训练精度更差，不如简单直观化，去学习一个更简单的内容。这篇论文没有做太多分析和数学证明，而是讲了很多实验的结果。</p><p>ResNet训练起来比较快主要是因为梯度收敛保持的比较好。原先求导的过程中，每一次梯度可能是0附近的高斯分布，乘起来可能就越来越小，出现了gradient vanishing的问题，但是ResNet中相当于加了一项$g(x)$进行求导，避免了梯度消失的问题。</p><p><img src="/img/2022-03-06-ResNet-Notes/Aft1.png"></p><p>在CIFAR-10上使用1000层的网络，有overfitting，但是没有特别做regularization也能保持在一个比较低的误差。“收敛没有任何意义”，但是ResNet可以一直train下去。ResNet主要是保持了一直有一个比较大的梯度，避免了SGD收敛到了一个比较不准确的位置，能够持续训练下去，所以可以得到一个比较好的结果。</p><p><img src="/img/2022-03-06-ResNet-Notes/Aft2.png"></p><h2 id="简单的尝试-Pytorch官方-ResNet实例"><a href="#简单的尝试-Pytorch官方-ResNet实例" class="headerlink" title="简单的尝试 Pytorch官方 ResNet实例"></a>简单的尝试 Pytorch官方 ResNet实例</h2><p>跑了一下单张图片上使用训练好的ResNet-18的预测情况，之后跑跑在ImageNet和CIFAR-10上训练的实验。<br>主要关注：</p><ol><li>代码中关于Residual的和Shortcut Connection的部分的实现。</li><li>如果不进行手动调Learning Rate，会持续收敛吗？沐神说的不手动调的现代的方法是什么？</li></ol><p><img src="/img/2022-03-06-ResNet-Notes/dog.png"></p>]]></content>
    
    
    <categories>
      
      <category>论文阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>论文笔记</tag>
      
      <tag>计算机视觉 CV</tag>
      
      <tag>ResNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《动手学深度学习》学习笔记 Ch.2 - 预备知识 （2.4-2.7）</title>
    <link href="/2022-02-25-D2L-Ch2-2/"/>
    <url>/2022-02-25-D2L-Ch2-2/</url>
    
    <content type="html"><![CDATA[<h2 id="2-4-微积分"><a href="#2-4-微积分" class="headerlink" title="2.4. 微积分"></a>2.4. 微积分</h2><p>我们可以将拟合模型的任务分解为两个关键问题：</p><ul><li><em>优化</em>（optimization）：用模型拟合观测数据的过程；</li><li><em>泛化</em>（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。</li></ul><h3 id="2-4-1-导数和微分"><a href="#2-4-1-导数和微分" class="headerlink" title="2.4.1. 导数和微分"></a>2.4.1. 导数和微分</h3><p>假设我们有一个函数$f:R^n→R$，其输入和输出都是标量。 如果ff的<em>导数</em>存在，这个极限被定义为<br>$$<br>f’(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}<br>$$<br>如果$f’(a)$存在，则称$f$在$a$处是<em>可微</em>（differentiable）的。如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。 我们可以将 <a href="https://zh.d2l.ai/chapter_preliminaries/calculus.html#equation-eq-derivative">(2.4.1)</a>中的导数$f′(x)$解释为$f(x)$相对于$x$的<em>瞬时</em>（instantaneous）变化率。 所谓的瞬时变化率是基于$x$中的变化$h$，且$h$接近$0$。</p><p>给定$y=f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量。以下表达式是等价的：<br>$$<br>f’(x) = y’ = \frac{dy}{dx} = \frac{df}{dx} =  = \frac{d}{dx}f(x) = Df(x) = D_xf(x)<br>$$<br>为了微分一个由一些常见函数组成的函数，下面的一些法则方便使用。 假设函数$f$和$g$都是可微的，$C$是一个常数，则：</p><ol><li>常数相乘法则</li></ol><p>$$<br>\frac{d}{dx}|Cf(x)| = C\frac{d}{dx}f(x)<br>$$</p><ol start="2"><li>加法法则</li></ol><p>$$<br>\frac{d}{dx}|f(x) + g(x)| = \frac{d}{dx}f(x) + \frac{d}{dx}g(x)<br>$$</p><ol start="3"><li>乘法法则</li></ol><p>$$<br>\frac{d}{dx}|f(x)g(x)| = f(x)\frac{d}{dx}|g(x)| + g(x)\frac{d}{dx}|f(x)|<br>$$</p><ol start="4"><li>除法法则</li></ol><p>$$<br>\frac{d}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)\frac{d}{dx}|f(x)| - f(x) \frac{d}{dx}|g(x)| }{ |g(x)|^2 }<br>$$</p><h3 id="2-4-2-偏导数"><a href="#2-4-2-偏导数" class="headerlink" title="2.4.2. 偏导数"></a>2.4.2. 偏导数</h3><p>到目前为止，我们只讨论了仅含一个变量的函数的微分。 在深度学习中，函数通常依赖于许多变量。 因此，我们需要将微分的思想推广到<em>多元函数</em>（multivariate function）上。</p><p>设$y=f(x_1,x_2,…,x_n)$是一个具有$n$个变量的函数。 $y$关于第$i$个参数$x_i$的<em>偏导数</em>（partial derivative）为：<br>$$<br>\frac{\partial y}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, …, x_{i-1}, x_i+h, x_{i+1},…,x_n) - f(x_1, …, x_i, …, x_n)}{h}<br>$$<br>为了计算$\frac{\partial y}{\partial x_i} $， 我们可以简单地将$x_1,…,x_{i−1},x_{i+1},…,x_n$看作常数， 并计算$y$关于$x_i$的导数。 对于偏导数的表示，以下是等价的：<br>$$<br>\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_if = D_{x_i}f<br>$$</p><h3 id="2-4-3-梯度"><a href="#2-4-3-梯度" class="headerlink" title="2.4.3. 梯度"></a>2.4.3. 梯度</h3><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的<em>梯度</em>（gradient）向量。 具体而言，设函数$f:R^n→R$的输入是 一个$n$维向量$x=[x_1,x_2,…,x_n]^T$，并且输出是一个标量。 函数$f(x)$相对于$x$的梯度是一个包含$n$个偏导数的向量:<br>$$<br>\nabla _xf(x) = [\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2}, …, \frac{\partial f(x)}{\partial x_n}]^T<br>$$<br>其中$∇_xf(x)$通常在没有歧义时被$∇f(x)$取代。</p><p>假设$x$为$n$维向量，在微分多元函数时经常使用以下规则:</p><ul><li>对于所有$A∈R^{m×n}$，都有$∇_xAx=A^T$</li><li>对于所有$A∈R^{n×m}$，都有$∇_xx^TA=A$</li><li>对于所有$A∈R^{n×n}$，都有$∇_xx^TAx=(A+A^T)x$</li><li>$∇_x||x||^2=∇_xx^Tx=2x$</li></ul><p>同样，对于任何矩阵$X$，都有$∇_X∥X∥^2_F=2X$。 正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。</p><h3 id="2-4-4-链式法则"><a href="#2-4-4-链式法则" class="headerlink" title="2.4.4. 链式法则"></a>2.4.4. 链式法则</h3><p>在深度学习中，多元函数通常是<em>复合</em>（composite）的， 所以我们可能没法应用上述任何规则来微分这些函数。 幸运的是，链式法则使我们能够微分复合函数。</p><p>假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：<br>$$<br>\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} u} \frac{\mathrm{d} u}{\mathrm{d} x}<br>$$<br>假设可微分函数$y$有变量$u_1,u_2,…,u_m$，其中每个可微分函数$u_i$都有变量$x_1,x_2,…,x_n$。 注意，$y$是$x_1,x_2，…,x_n$的函数。 对于任意$i=1,2,…,n$，链式法则给出：<br>$$<br>\frac{\mathrm{d} y}{\mathrm{d} x_i} = \frac{\mathrm{d} y}{\mathrm{d} u_1} \frac{\mathrm{d} u_1}{\mathrm{d} x_i}+ \frac{\mathrm{d} y}{\mathrm{d} u_2} \frac{\mathrm{d} u_2}{\mathrm{d} x_i}  + … + \frac{\mathrm{d} y}{\mathrm{d} u_m} \frac{\mathrm{d} u_m}{\mathrm{d} x_i}<br>$$</p><h3 id="2-4-5-小结"><a href="#2-4-5-小结" class="headerlink" title="2.4.5. 小结"></a>2.4.5. 小结</h3><ul><li>微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。</li><li>导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。</li><li>梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。</li><li>链式法则使我们能够微分复合函数。</li></ul><h2 id="2-5-自动微分"><a href="#2-5-自动微分" class="headerlink" title="2.5. 自动微分"></a>2.5. 自动微分</h2><p>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。</p><p> 实际中，根据我们设计的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p><h3 id="2-5-1-一个简单的例子"><a href="#2-5-1-一个简单的例子" class="headerlink" title="2.5.1. 一个简单的例子"></a>2.5.1. 一个简单的例子</h3><p>作为一个演示例子，假设我们想对函数$y=2x^Tx$关于列向量$x$求导。 </p><p>首先，我们创建变量<code>x</code>并为其分配一个初始值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.arange(<span class="hljs-number">4.0</span>)<br>x<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([0., 1., 2., 3.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>在我们计算$y$关于$x$的梯度之前，我们需要一个地方来存储梯度。</p><p> 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">x.requires_grad_(<span class="hljs-literal">True</span>)  <span class="hljs-comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span><br>x.grad  <span class="hljs-comment"># 默认值是None</span><br><br><span class="hljs-comment"># 现在让我们计算y。</span><br>y = <span class="hljs-number">2</span> * torch.dot(x, x)<br>y<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(28., grad_fn=&lt;MulBackward0&gt;)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p><code>x</code>是一个长度为4的向量，计算<code>x</code>和<code>x</code>的点积，得到了我们赋值给<code>y</code>的标量输出。 接下来，我们通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度，并打印这些梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">y.backward()<br>x.grad<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([ 0.,  4.,  8., 12.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>函数$y=2x^Tx$关于$x$的梯度应为$4x$。 让我们快速验证这个梯度是否计算正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x.grad == <span class="hljs-number">4</span> * x<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([True, True, True, True])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>现在让我们计算<code>x</code>的另一个函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span><br>x.grad.zero_()<br>y = x.<span class="hljs-built_in">sum</span>()<br>y.backward()<br>x.grad<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([1., 1., 1., 1.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-5-2-非标量变量的反向传播"><a href="#2-5-2-非标量变量的反向传播" class="headerlink" title="2.5.2. 非标量变量的反向传播"></a>2.5.2. 非标量变量的反向传播</h3><p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。 对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span><br><span class="hljs-comment"># 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的</span><br>x.grad.zero_()<br>y = x * x<br><span class="hljs-comment"># 等价于y.backward(torch.ones(len(x)))</span><br>y.<span class="hljs-built_in">sum</span>().backward()<br>x.grad<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([0., 2., 4., 6.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-5-3-分离计算"><a href="#2-5-3-分离计算" class="headerlink" title="2.5.3. 分离计算"></a>2.5.3. 分离计算</h3><p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。 想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，我们希望将<code>y</code>视为一个常数， 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p><p>在这里，我们可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x.grad.zero_()<br>y = x * x<br>u = y.detach()<br>z = u * x<br><br>z.<span class="hljs-built_in">sum</span>().backward()<br>x.grad == u<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([True, True, True, True])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>由于记录了<code>y</code>的计算结果，我们可以随后在<code>y</code>上调用反向传播， 得到<code>y=x*x</code>关于的<code>x</code>的导数，即<code>2*x</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">x.grad.zero_()<br>y.<span class="hljs-built_in">sum</span>().backward()<br>x.grad == <span class="hljs-number">2</span> * x<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([True, True, True, True])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-5-4-Python控制流的梯度计算"><a href="#2-5-4-Python控制流的梯度计算" class="headerlink" title="2.5.4. Python控制流的梯度计算"></a>2.5.4. Python控制流的梯度计算</h3><p>（个人理解是：Pytorch在自动微分的应用是，一个数学函数可以由Python的函数来定义）</p><p>使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span>(<span class="hljs-params">a</span>):</span><br>    b = a * <span class="hljs-number">2</span><br>    <span class="hljs-keyword">while</span> b.norm() &lt; <span class="hljs-number">1000</span>:<br>        b = b * <span class="hljs-number">2</span><br>    <span class="hljs-keyword">if</span> b.<span class="hljs-built_in">sum</span>() &gt; <span class="hljs-number">0</span>:<br>        c = b<br>    <span class="hljs-keyword">else</span>:<br>        c = <span class="hljs-number">100</span> * b<br>    <span class="hljs-keyword">return</span> c<br></code></pre></td></tr></table></figure><p>让我们计算梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.randn(size=(), requires_grad=<span class="hljs-literal">True</span>)<br>d = f(a)<br>d.backward()<br></code></pre></td></tr></table></figure><p>我们现在可以分析上面定义的<code>f</code>函数。 请注意，它在其输入<code>a</code>中是分段线性的。 换言之，对于任何<code>a</code>，存在某个常量标量<code>k</code>，使得<code>f(a)=k*a</code>，其中<code>k</code>的值取决于输入<code>a</code>。 因此，我们可以用<code>d/a</code>验证梯度是否正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">a.grad == d / a<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(True)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-5-5-小结"><a href="#2-5-5-小结" class="headerlink" title="2.5.5. 小结"></a>2.5.5. 小结</h3><ul><li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li></ul><h2 id="2-6-概率"><a href="#2-6-概率" class="headerlink" title="2.6. 概率"></a>2.6. 概率</h2><h3 id="2-6-1-基本概率论"><a href="#2-6-1-基本概率论" class="headerlink" title="2.6.1. 基本概率论"></a>2.6.1. 基本概率论</h3><p> 对于每个骰子，我们将观察到$\lbrace  1,…,6 \rbrace$中的一个值。对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数， 即此<em>事件</em>（event）概率的<em>估计值</em>。 <em>大数定律</em>（law of large numbers）告诉我们： 随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。</p><p>在统计学中，我们把从概率分布中抽取样本的过程称为<em>抽样</em>（sampling）。 笼统来说，可以把<em>分布</em>（distribution）看作是对事件的概率分配， 稍后我们将给出的更正式定义。 将概率分配给一些离散选择的分布称为<em>多项分布</em>（multinomial distribution）。</p><h4 id="2-6-1-1-概率论公理"><a href="#2-6-1-1-概率论公理" class="headerlink" title="2.6.1.1. 概率论公理"></a>2.6.1.1. 概率论公理</h4><p>在处理骰子掷出时，我们将集合$S=\lbrace 1,2,3,4,5,6 \rbrace$称为<em>样本空间</em>（sample space）或<em>结果空间</em>（outcome space）， 其中每个元素都是<em>结果</em>（outcome）。 <em>事件</em>（event）是一组给定样本空间的随机结果。 </p><p><em>概率</em>（probability）可以被认为是将集合映射到真实值的函数。 在给定的样本空间$S$中，事件$A$的概率， 表示为$P(A)$，满足以下属性：</p><ul><li>对于任意事件$A$，其概率从不会是负数，即$P(A)≥0$；</li><li>整个样本空间的概率为$1$，即$P(S)=1$；</li><li>对于<em>互斥</em>（mutually exclusive）事件（对于所有$i≠j$都有$A_i∩A_j=∅$）的任意一个可数序列$A1,A2,…$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(⋃^∞_{i=1}A_i)=∑^∞_{i=1}P(A_i) $。</li></ul><p>以上也是概率论的公理，由科尔莫戈罗夫于1933年提出。有了这个公理系统，我们可以避免任何关于随机性的哲学争论； 相反，我们可以用数学语言严格地推理。 例如，假设事件$A_1$为整个样本空间， 且当所有$i&gt;1$时的$A_i=∅$， 那么我们可以证明$P(∅)=0$，即不可能发生事件的概率是0。</p><h4 id="2-6-1-2-随机变量"><a href="#2-6-1-2-随机变量" class="headerlink" title="2.6.1.2. 随机变量"></a>2.6.1.2. 随机变量</h4><p>在我们掷骰子的随机实验中，我们引入了<em>随机变量</em>（random variable）的概念。随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。 考虑一个随机变量$X$，其值在掷骰子的样本空间$S=\left{ 1,2,3,4,5,6 \rbrace$中。 我们可以将事件“看到一个$5$”表示为$\left{X=5\right}$或$(X=5)$， 其概率表示为$P\left{X=5\right}$或$P(X=5)$。 </p><h3 id="2-6-2-处理多个随机变量"><a href="#2-6-2-处理多个随机变量" class="headerlink" title="2.6.2. 处理多个随机变量"></a>2.6.2. 处理多个随机变量</h3><h4 id="2-6-2-1-联合概率"><a href="#2-6-2-1-联合概率" class="headerlink" title="2.6.2.1. 联合概率"></a>2.6.2.1. 联合概率</h4><p><em>联合概率</em>（joint probability）$P(A=a,B=b)$。 给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？ </p><h4 id="2-6-2-2-条件概率"><a href="#2-6-2-2-条件概率" class="headerlink" title="2.6.2.2. 条件概率"></a>2.6.2.2. 条件概率</h4><p>联合概率的不等式带给我们一个有趣的比率： $0≤\frac{P(A=a,B=b)}{P(A=a)}≤1$。 我们称这个比率为<em>条件概率</em>（conditional probability）， 并用$P(B=b∣A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。</p><h4 id="2-6-2-3-贝叶斯定理"><a href="#2-6-2-3-贝叶斯定理" class="headerlink" title="2.6.2.3. 贝叶斯定理"></a>2.6.2.3. 贝叶斯定理</h4><p>使用条件概率的定义，我们可以得出统计学中最有用的方程之一： <em>Bayes定理</em>（Bayes’ theorem）。 根据<em>乘法法则</em>（multiplication rule ）可得到$P(A,B)=P(B∣A)P(A)$。 根据对称性，可得到$P(A,B)=P(A∣B)P(B)$。 假设$P(B)&gt;0$，求解其中一个条件变量，我们得到<br>$$<br>P(A|B) = \frac{P(B|A)P(A)}{P(B)}<br>$$<br>请注意，这里我们使用紧凑的表示法： 其中$P(A,B)$是一个<em>联合分布</em>（joint distribution）， $P(A∣B)$是一个<em>条件分布</em>（conditional distribution）。 这种分布可以在给定值$A=a,B=b$上进行求值。</p><h4 id="2-6-2-4-边际化"><a href="#2-6-2-4-边际化" class="headerlink" title="2.6.2.4. 边际化"></a>2.6.2.4. 边际化</h4><p>为了能进行事件概率求和，我们需要<em>求和法则</em>（sum rule）， 即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：<br>$$<br>P(B) = \sum_{A}P(A,B)<br>$$<br>这也称为<em>边际化</em>（marginalization）。 边际化结果的概率或分布称为<em>边际概率</em>（marginal probability） 或<em>边际分布</em>（marginal distribution）。</p><h4 id="2-6-2-5-独立性"><a href="#2-6-2-5-独立性" class="headerlink" title="2.6.2.5. 独立性"></a>2.6.2.5. 独立性</h4><p>另一个有用属性是<em>依赖</em>（dependence）与<em>独立</em>（independence）。 如果两个随机变量$A$和$B$是独立的，意味着事件AA的发生跟BB事件的发生无关。 在这种情况下，统计学家通常将这一点表述为$A⊥B$。 根据贝叶斯定理，马上就能同样得到$P(A∣B)=P(A)$。</p><h4 id="2-6-2-6-应用"><a href="#2-6-2-6-应用" class="headerlink" title="2.6.2.6. 应用"></a>2.6.2.6. 应用</h4><p>是一个计算题的例子，略</p><h3 id="2-6-3-期望和方差"><a href="#2-6-3-期望和方差" class="headerlink" title="2.6.3. 期望和方差"></a>2.6.3. 期望和方差</h3><p>为了概括概率分布的关键特征，我们需要一些测量方法。 一个随机变量XX的<em>期望</em>（expectation，或平均值（average））表示为<br>$$<br>E|X| = \sum_{x}xP(X=x)<br>$$<br>当函数$f(x)$的输入是从分布$P$中抽取的随机变量时，$f(x)$的期望值为<br>$$<br>E_{x∼P}[f(x)]=\sum_{x}f(x)P(x).<br>$$<br>在许多情况下，我们希望衡量随机变量$X$与其期望值的偏置。这可以通过方差来量化<br>$$<br>Var[X]=E[(X−E[X])^2]=E[X^2]−E[X]^2.<br>$$<br>方差的平方根被称为<em>标准差</em>（standard deviation）。 随机变量函数的方差衡量的是：当从该随机变量分布中采样不同值$x$时， 函数值偏离该函数的期望的程度：<br>$$<br>Var[f(x)]=E[(f(x)−E[f(x)])^2].<br>$$</p><h3 id="2-6-4-小结"><a href="#2-6-4-小结" class="headerlink" title="2.6.4. 小结"></a>2.6.4. 小结</h3><ul><li>我们可以从概率分布中采样。</li><li>我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。</li><li>期望和方差为概率分布的关键特征的概括提供了实用的度量形式。</li></ul><h2 id="2-7-查阅文档"><a href="#2-7-查阅文档" class="headerlink" title="2.7. 查阅文档"></a>2.7. 查阅文档</h2><p>由于本书篇幅限制，我们不可能介绍每一个PyTorch函数和类（你可能也不希望我们这样做）。 API文档、其他教程和示例提供了本书之外的大量文档。 在本节中，我们为你提供了一些查看PyTorch API的指导。</p><h3 id="2-7-1-查找模块中的所有函数和类"><a href="#2-7-1-查找模块中的所有函数和类" class="headerlink" title="2.7.1. 查找模块中的所有函数和类"></a>2.7.1. 查找模块中的所有函数和类</h3><p>为了知道模块中可以调用哪些函数和类，我们调用<code>dir</code>函数。 例如，我们可以查询随机数生成模块中的所有属性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">dir</span>(torch.distributions))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">[&#x27;AbsTransform&#x27;, &#x27;AffineTransform&#x27;, &#x27;Bernoulli&#x27;, &#x27;Beta&#x27;, &#x27;Binomial&#x27;, &#x27;CatTransform&#x27;, &#x27;Categorical&#x27;, &#x27;Cauchy&#x27;, &#x27;Chi2&#x27;, &#x27;ComposeTransform&#x27;, &#x27;ContinuousBernoulli&#x27;, &#x27;CorrCholeskyTransform&#x27;, &#x27;Dirichlet&#x27;, &#x27;Distribution&#x27;, &#x27;ExpTransform&#x27;, &#x27;Exponential&#x27;, &#x27;ExponentialFamily&#x27;, &#x27;FisherSnedecor&#x27;, &#x27;Gamma&#x27;, &#x27;Geometric&#x27;, &#x27;Gumbel&#x27;, &#x27;HalfCauchy&#x27;, &#x27;HalfNormal&#x27;, &#x27;Independent&#x27;, &#x27;IndependentTransform&#x27;, &#x27;Kumaraswamy&#x27;, &#x27;LKJCholesky&#x27;, &#x27;Laplace&#x27;, &#x27;LogNormal&#x27;, &#x27;LogisticNormal&#x27;, &#x27;LowRankMultivariateNormal&#x27;, &#x27;LowerCholeskyTransform&#x27;, &#x27;MixtureSameFamily&#x27;, &#x27;Multinomial&#x27;, &#x27;MultivariateNormal&#x27;, &#x27;NegativeBinomial&#x27;, &#x27;Normal&#x27;, &#x27;OneHotCategorical&#x27;, &#x27;OneHotCategoricalStraightThrough&#x27;, &#x27;Pareto&#x27;, &#x27;Poisson&#x27;, &#x27;PowerTransform&#x27;, &#x27;RelaxedBernoulli&#x27;, &#x27;RelaxedOneHotCategorical&#x27;, &#x27;ReshapeTransform&#x27;, &#x27;SigmoidTransform&#x27;, &#x27;SoftmaxTransform&#x27;, &#x27;StackTransform&#x27;, &#x27;StickBreakingTransform&#x27;, &#x27;StudentT&#x27;, &#x27;TanhTransform&#x27;, &#x27;Transform&#x27;, &#x27;TransformedDistribution&#x27;, &#x27;Uniform&#x27;, &#x27;VonMises&#x27;, &#x27;Weibull&#x27;, &#x27;__all__&#x27;, &#x27;__builtins__&#x27;, &#x27;__cached__&#x27;, &#x27;__doc__&#x27;, &#x27;__file__&#x27;, &#x27;__loader__&#x27;, &#x27;__name__&#x27;, &#x27;__package__&#x27;, &#x27;__path__&#x27;, &#x27;__spec__&#x27;, &#x27;bernoulli&#x27;, &#x27;beta&#x27;, &#x27;biject_to&#x27;, &#x27;binomial&#x27;, &#x27;categorical&#x27;, &#x27;cauchy&#x27;, &#x27;chi2&#x27;, &#x27;constraint_registry&#x27;, &#x27;constraints&#x27;, &#x27;continuous_bernoulli&#x27;, &#x27;dirichlet&#x27;, &#x27;distribution&#x27;, &#x27;exp_family&#x27;, &#x27;exponential&#x27;, &#x27;fishersnedecor&#x27;, &#x27;gamma&#x27;, &#x27;geometric&#x27;, &#x27;gumbel&#x27;, &#x27;half_cauchy&#x27;, &#x27;half_normal&#x27;, &#x27;identity_transform&#x27;, &#x27;independent&#x27;, &#x27;kl&#x27;, &#x27;kl_divergence&#x27;, &#x27;kumaraswamy&#x27;, &#x27;laplace&#x27;, &#x27;lkj_cholesky&#x27;, &#x27;log_normal&#x27;, &#x27;logistic_normal&#x27;, &#x27;lowrank_multivariate_normal&#x27;, &#x27;mixture_same_family&#x27;, &#x27;multinomial&#x27;, &#x27;multivariate_normal&#x27;, &#x27;negative_binomial&#x27;, &#x27;normal&#x27;, &#x27;one_hot_categorical&#x27;, &#x27;pareto&#x27;, &#x27;poisson&#x27;, &#x27;register_kl&#x27;, &#x27;relaxed_bernoulli&#x27;, &#x27;relaxed_categorical&#x27;, &#x27;studentT&#x27;, &#x27;transform_to&#x27;, &#x27;transformed_distribution&#x27;, &#x27;transforms&#x27;, &#x27;uniform&#x27;, &#x27;utils&#x27;, &#x27;von_mises&#x27;, &#x27;weibull&#x27;]</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>通常，我们可以忽略以“<code>__</code>”（双下划线）开始和结束的函数（它们是Python中的特殊对象）， 或以单个“<code>_</code>”（单下划线）开始的函数（它们通常是内部函数）。 根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法， 包括从均匀分布（<code>uniform</code>）、正态分布（<code>normal</code>）和多项分布（<code>multinomial</code>）中采样。</p><h3 id="2-7-2-查找特定函数和类的用法"><a href="#2-7-2-查找特定函数和类的用法" class="headerlink" title="2.7.2. 查找特定函数和类的用法"></a>2.7.2. 查找特定函数和类的用法</h3><p>有关如何使用给定函数或类的更具体说明，我们可以调用<code>help</code>函数。 例如，我们来查看张量<code>ones</code>函数的用法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">help</span>(torch.ones)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">Help on built-in function ones:</span><br><span class="hljs-string"></span><br><span class="hljs-string">ones(...)</span><br><span class="hljs-string">    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns a tensor filled with the scalar value 1, with the shape defined</span><br><span class="hljs-string">    by the variable argument size.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        size (int...): a sequence of integers defining the shape of the output tensor.</span><br><span class="hljs-string">            Can be a variable number of arguments or a collection like a list or tuple.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keyword arguments:</span><br><span class="hljs-string">        out (Tensor, optional): the output tensor.</span><br><span class="hljs-string">        dtype (torch.dtype, optional): the desired data type of returned tensor.</span><br><span class="hljs-string">            Default: if None, uses a global default (see torch.set_default_tensor_type()).</span><br><span class="hljs-string">        layout (torch.layout, optional): the desired layout of returned Tensor.</span><br><span class="hljs-string">            Default: torch.strided.</span><br><span class="hljs-string">        device (torch.device, optional): the desired device of returned tensor.</span><br><span class="hljs-string">            Default: if None, uses the current device for the default tensor type</span><br><span class="hljs-string">            (see torch.set_default_tensor_type()). device will be the CPU</span><br><span class="hljs-string">            for CPU tensor types and the current CUDA device for CUDA tensor types.</span><br><span class="hljs-string">        requires_grad (bool, optional): If autograd should record operations on the</span><br><span class="hljs-string">            returned tensor. Default: False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Example::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; torch.ones(2, 3)</span><br><span class="hljs-string">        tensor([[ 1.,  1.,  1.],</span><br><span class="hljs-string">                [ 1.,  1.,  1.]])</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; torch.ones(5)</span><br><span class="hljs-string">        tensor([ 1.,  1.,  1.,  1.,  1.])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>在Jupyter记事本中，我们可以使用<code>?</code>指令在另一个浏览器窗口中显示文档。 例如，<code>list?</code>指令将创建与<code>help(list)</code>指令几乎相同的内容，并在新的浏览器窗口中显示它。 此外，如果我们使用两个问号，如<code>list??</code>，将显示实现该函数的Python代码。</p><h3 id="2-7-3-小结"><a href="#2-7-3-小结" class="headerlink" title="2.7.3. 小结"></a>2.7.3. 小结</h3><ul><li>官方文档提供了本书之外的大量描述和示例。</li><li>我们可以通过调用<code>dir</code>和<code>help</code>函数或在Jupyter记事本中使用<code>?</code>和<code>??</code>查看API的用法文档。</li></ul>]]></content>
    
    
    <categories>
      
      <category>《动手学深度学习》学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>学习笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《动手学深度学习》学习笔记 Ch.2 - 预备知识 （2.1-2.3）</title>
    <link href="/2022-02-22-D2L-Ch2-1/"/>
    <url>/2022-02-22-D2L-Ch2-1/</url>
    
    <content type="html"><![CDATA[<h1 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h1><p>要学习深度学习，首先需要先掌握一些基本技能。 所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。</p><h2 id="2-1-数据操作"><a href="#2-1-数据操作" class="headerlink" title="2.1. 数据操作"></a>2.1. 数据操作</h2><p>首先，我们介绍$n$维数组，也称为<em>张量</em>（tensor）。 使用过Python中NumPy计算包的读者会对本部分很熟悉。 无论使用哪个深度学习框架，它的<em>张量类</em>（在MXNet中为<code>ndarray</code>， 在PyTorch和TensorFlow中为<code>Tensor</code>）都与Numpy的<code>ndarray</code>类似。 但深度学习框架又比Numpy的<code>ndarray</code>多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。</p><h3 id="2-1-1-入门"><a href="#2-1-1-入门" class="headerlink" title="2.1.1. 入门"></a>2.1.1. 入门</h3><blockquote><p>如果你已经具有相关经验，想要深入学习数学内容，可以跳过本节。</p></blockquote><p>张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的<em>向量</em>（vector）； 具有两个轴的张量对应数学上的<em>矩阵</em>（matrix）； 具有两个轴以上的张量没有特殊的数学名称。</p><ol><li>我们可以使用 <code>arange</code> 创建一个行向量 <code>x</code>。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 <em>元素</em>（element）。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1.</span><br>x = torch.arange(<span class="hljs-number">12</span>)<br>x <br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>要想改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数。 例如，可以把张量<code>x</code>从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 2.</span><br>X = x.reshape(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>X<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0,  1,  2,  3],</span><br><span class="hljs-string">        [ 4,  5,  6,  7],</span><br><span class="hljs-string">        [ 8,  9, 10, 11]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="3"><li>我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。同样，我们可以创建一个形状为<code>(2,3,4)</code>的张量，其中所有元素都设置为1。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 3.</span><br>torch.zeros((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[[0., 0., 0., 0.],</span><br><span class="hljs-string">         [0., 0., 0., 0.],</span><br><span class="hljs-string">         [0., 0., 0., 0.]],</span><br><span class="hljs-string"></span><br><span class="hljs-string">        [[0., 0., 0., 0.],</span><br><span class="hljs-string">         [0., 0., 0., 0.],</span><br><span class="hljs-string">         [0., 0., 0., 0.]]])</span><br><span class="hljs-string"></span><br><span class="hljs-string">tensor([[[1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.]],</span><br><span class="hljs-string"></span><br><span class="hljs-string">        [[1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.],</span><br><span class="hljs-string">         [1., 1., 1., 1.]]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="4"><li>以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 4.</span><br>torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0.4315, -0.8804, -0.1730, -1.2925],</span><br><span class="hljs-string">        [ 0.3317, -1.1386, -0.6625,  0.3001],</span><br><span class="hljs-string">        [ 0.0371, -0.4246,  0.0326,  0.1565]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-2-运算符"><a href="#2-1-2-运算符" class="headerlink" title="2.1.2. 运算符"></a>2.1.2. 运算符</h3><p>我们想在这些数据上执行数学运算，其中最简单且最有用的操作是<em>按元素</em>（elementwise）运算。 它们将标准标量运算符应用于数组的每个元素。</p><ol><li>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>])<br>y = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])<br>x + y, x - y, x * y, x / y, x ** y  <span class="hljs-comment"># **运算符是求幂运算</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([ 3.,  4.,  6., 10.]),</span><br><span class="hljs-string"> tensor([-1.,  0.,  2.,  6.]),</span><br><span class="hljs-string"> tensor([ 2.,  4.,  8., 16.]),</span><br><span class="hljs-string"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span><br><span class="hljs-string"> tensor([ 1.,  4., 16., 64.]))</span><br><span class="hljs-string"> &#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.exp(x)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="3"><li>我们也可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。 </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.arange(<span class="hljs-number">12</span>, dtype=torch.float32).reshape((<span class="hljs-number">3</span>,<span class="hljs-number">4</span>))<br>Y = torch.tensor([[<span class="hljs-number">2.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]])<br>torch.cat((X, Y), dim=<span class="hljs-number">0</span>), torch.cat((X, Y), dim=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># （轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="hljs-string">         [ 4.,  5.,  6.,  7.],</span><br><span class="hljs-string">         [ 8.,  9., 10., 11.],</span><br><span class="hljs-string">         [ 2.,  1.,  4.,  3.],</span><br><span class="hljs-string">         [ 1.,  2.,  3.,  4.],</span><br><span class="hljs-string">         [ 4.,  3.,  2.,  1.]]),</span><br><span class="hljs-string"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="hljs-string">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="hljs-string">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="4"><li>有时，我们想通过<em>逻辑运算符</em>构建二元张量。 以<code>X == Y</code>为例： 对于每个位置，如果<code>X</code>和<code>Y</code>在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句<code>X == Y</code>在该位置处为真，否则该位置为0。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">X == Y<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[False,  True, False,  True],</span><br><span class="hljs-string">        [False, False, False, False],</span><br><span class="hljs-string">        [False, False, False, False]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="5"><li>对张量中的所有元素进行求和，会产生一个单元素张量。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X.<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(66.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-3-广播机制"><a href="#2-1-3-广播机制" class="headerlink" title="2.1.3. 广播机制"></a>2.1.3. 广播机制</h3><p>在某些情况下，即使形状不同，我们仍然可以通过调用 <em>广播机制</em>（broadcasting mechanism）来执行按元素操作。种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.arange(<span class="hljs-number">3</span>).reshape((<span class="hljs-number">3</span>, <span class="hljs-number">1</span>))<br>b = torch.arange(<span class="hljs-number">2</span>).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>a, b<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([[0],</span><br><span class="hljs-string">         [1],</span><br><span class="hljs-string">         [2]]),</span><br><span class="hljs-string"> tensor([[0, 1]]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>a + b<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[0, 1],</span><br><span class="hljs-string">        [1, 2],</span><br><span class="hljs-string">        [2, 3]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-4-索引和切片"><a href="#2-1-4-索引和切片" class="headerlink" title="2.1.4. 索引和切片"></a>2.1.4. 索引和切片</h3><p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。</p><p>如下所示，我们可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">X<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="hljs-string">        [ 4.,  5.,  6.,  7.],</span><br><span class="hljs-string">        [ 8.,  9., 10., 11.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>X[-<span class="hljs-number">1</span>], X[<span class="hljs-number">1</span>:<span class="hljs-number">3</span>]<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([ 8.,  9., 10., 11.]),</span><br><span class="hljs-string"> tensor([[ 4.,  5.,  6.,  7.],</span><br><span class="hljs-string">         [ 8.,  9., 10., 11.]]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>X[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>, :] = <span class="hljs-number">12</span><br>X<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[12., 12., 12., 12.],</span><br><span class="hljs-string">        [12., 12., 12., 12.],</span><br><span class="hljs-string">        [ 8.,  9., 10., 11.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-5-节省内存"><a href="#2-1-5-节省内存" class="headerlink" title="2.1.5. 节省内存"></a>2.1.5. 节省内存</h3><p>运行一些操作可能会导致为新结果分配内存。 例如，如果我们用<code>Y = X + Y</code>，我们将取消引用<code>Y</code>指向的张量，而是指向新分配的内存处的张量。</p><p>在下面的例子中，我们用Python的<code>id()</code>函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">before = <span class="hljs-built_in">id</span>(Y)<br>Y = Y + X<br><span class="hljs-built_in">id</span>(Y) == before<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">False</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>幸运的是，执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如<code>Y[:] = &lt;expression&gt;</code>。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">Z = torch.zeros_like(Y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;id(Z):&#x27;</span>, <span class="hljs-built_in">id</span>(Z))<br>Z[:] = X + Y<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;id(Z):&#x27;</span>, <span class="hljs-built_in">id</span>(Z))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">id(Z): 140116336758272</span><br><span class="hljs-string">id(Z): 140116336758272</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-6-转换为其他Python对象"><a href="#2-1-6-转换为其他Python对象" class="headerlink" title="2.1.6. 转换为其他Python对象"></a>2.1.6. 转换为其他Python对象</h3><p>将深度学习框架定义的张量转换为NumPy张量（<code>ndarray</code>）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">A = X.numpy()<br>B = torch.tensor(A)<br><span class="hljs-built_in">type</span>(A), <span class="hljs-built_in">type</span>(B)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(numpy.ndarray, torch.Tensor)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>要将大小为1的张量转换为Python标量，我们可以调用<code>item</code>函数或Python的内置函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">a = torch.tensor([<span class="hljs-number">3.5</span>])<br>a, a.item(), <span class="hljs-built_in">float</span>(a), <span class="hljs-built_in">int</span>(a)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([3.5000]), 3.5, 3.5, 3)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-1-7-小结"><a href="#2-1-7-小结" class="headerlink" title="2.1.7. 小结"></a>2.1.7. 小结</h3><ul><li>深度学习存储和操作数据的主要接口是张量（nn维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。</li></ul><h2 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2. 数据预处理"></a>2.2. 数据预处理</h2><h3 id="2-2-1-读取数据集"><a href="#2-2-1-读取数据集" class="headerlink" title="2.2.1. 读取数据集"></a>2.2.1. 读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">os.makedirs(os.path.join(<span class="hljs-string">&#x27;..&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>), exist_ok=<span class="hljs-literal">True</span>)<br>data_file = os.path.join(<span class="hljs-string">&#x27;..&#x27;</span>, <span class="hljs-string">&#x27;data&#x27;</span>, <span class="hljs-string">&#x27;house_tiny.csv&#x27;</span>)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_file, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    f.write(...)<br>    ...<br><br>data = pd.read_csv(data_file)<br><span class="hljs-built_in">print</span>(data) <br></code></pre></td></tr></table></figure><h3 id="2-2-2-处理缺失值"><a href="#2-2-2-处理缺失值" class="headerlink" title="2.2.2. 处理缺失值"></a>2.2.2. 处理缺失值</h3><p>注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括<em><strong>插值法</strong></em>和<em><strong>删除法</strong></em>， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs, outputs = data.iloc[:, <span class="hljs-number">0</span>:<span class="hljs-number">2</span>], data.iloc[:, <span class="hljs-number">2</span>] <span class="hljs-comment"># data.iloc[] 获取index of ...</span><br>inputs = inputs.fillna(inputs.mean()) <span class="hljs-comment"># 插入平均值</span><br></code></pre></td></tr></table></figure><p>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = pd.get_dummies(inputs, dummy_na=<span class="hljs-literal">True</span>)<br><br><span class="hljs-string">&#x27;&#x27;&#x27; BEFORE</span><br><span class="hljs-string">   NumRooms Alley</span><br><span class="hljs-string">0       3.0  Pave</span><br><span class="hljs-string">1       2.0   NaN</span><br><span class="hljs-string">2       4.0   NaN</span><br><span class="hljs-string">3       3.0   NaN</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;AFTER</span><br><span class="hljs-string">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="hljs-string">0       3.0           1          0</span><br><span class="hljs-string">1       2.0           0          1</span><br><span class="hljs-string">2       4.0           0          1</span><br><span class="hljs-string">3       3.0           0          1</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-2-3-转换为张量格式"><a href="#2-2-3-转换为张量格式" class="headerlink" title="2.2.3. 转换为张量格式"></a>2.2.3. 转换为张量格式</h3><p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)<br>X, y<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([[3., 1., 0.],</span><br><span class="hljs-string">         [2., 0., 1.],</span><br><span class="hljs-string">         [4., 0., 1.],</span><br><span class="hljs-string">         [3., 0., 1.]], dtype=torch.float64),</span><br><span class="hljs-string"> tensor([127500, 106000, 178100, 140000]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="2-3-线性代数"><a href="#2-3-线性代数" class="headerlink" title="2.3. 线性代数"></a>2.3. 线性代数</h2><p>在你已经可以存储和操作数据后，让我们简要地回顾一下部分基本线性代数内容。</p><h3 id="2-3-1-标量"><a href="#2-3-1-标量" class="headerlink" title="2.3.1. 标量"></a>2.3.1. 标量</h3><p>标量由只有一个元素的张量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.tensor(<span class="hljs-number">3.0</span>)<br>y = torch.tensor(<span class="hljs-number">2.0</span>)<br><br>x + y, x * y, x / y, x**y<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-2-向量"><a href="#2-3-2-向量" class="headerlink" title="2.3.2. 向量"></a>2.3.2. 向量</h3><p>你可以将向量视为标量值组成的列表。 我们将这些标量值称为向量的<em>元素</em>（element）或<em>分量</em>（component）。</p><ol><li>我们通过一维张量处理向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">4</span>)<br>x<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([0, 1, 2, 3])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>我们可以使用下标来引用向量的任一元素。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x[<span class="hljs-number">3</span>]<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(3)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="2-3-2-1-长度、维度和形状"><a href="#2-3-2-1-长度、维度和形状" class="headerlink" title="2.3.2.1. 长度、维度和形状"></a>2.3.2.1. 长度、维度和形状</h4><p>向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。 </p><p>向量的长度通常称为向量的<em>维度</em>（dimension）。</p><p><strong><em>维度</em>（dimension）</strong><em>向量</em>或<em>轴</em>的维度被用来表示<em>向量</em>或<em>轴</em>的长度，即向量或轴的元素数量。</p><h3 id="2-3-3-矩阵"><a href="#2-3-3-矩阵" class="headerlink" title="2.3.3. 矩阵"></a>2.3.3. 矩阵</h3><p>矩阵，我们通常用粗体、大写字母来表示 （例如，X、Y和Z）， 在代码中表示为具有两个轴的张量。</p><p>当调用函数来实例化张量时， 我们可以通过指定两个分量$m$和$n$来创建一个形状为$m×n$的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">20</span>).reshape(<span class="hljs-number">5</span>, <span class="hljs-number">4</span>)<br>A<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0,  1,  2,  3],</span><br><span class="hljs-string">        [ 4,  5,  6,  7],</span><br><span class="hljs-string">        [ 8,  9, 10, 11],</span><br><span class="hljs-string">        [12, 13, 14, 15],</span><br><span class="hljs-string">        [16, 17, 18, 19]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>当我们交换矩阵的行和列时，结果称为矩阵的<em>转置</em>（transpose）。 我们用$a^T$来表示矩阵的转置，如果$B=A^T$， 则对于任意ii和jj，都有$b_{ij}=a_{ji}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">A.T<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0,  4,  8, 12, 16],</span><br><span class="hljs-string">        [ 1,  5,  9, 13, 17],</span><br><span class="hljs-string">        [ 2,  6, 10, 14, 18],</span><br><span class="hljs-string">        [ 3,  7, 11, 15, 19]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>作为方阵的一种特殊类型，<em>对称矩阵</em>（symmetric matrix）$A$等于其转置：$A=A^T$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">B = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]])<br>B<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[1, 2, 3],</span><br><span class="hljs-string">        [2, 0, 4],</span><br><span class="hljs-string">        [3, 4, 5]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>B == B.T<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[True, True, True],</span><br><span class="hljs-string">        [True, True, True],</span><br><span class="hljs-string">        [True, True, True]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-4-张量"><a href="#2-3-4-张量" class="headerlink" title="2.3.4. 张量"></a>2.3.4. 张量</h3><p>张量（本小节中的“张量”指代数对象）为我们提供了描述具有任意数量轴的$n$维数组的通用方法。</p><p>张量用特殊字体的大写字母表示（例如，X、Y和Z）， 它们的索引机制（例如$x_{ijk}$和$[X]_{1,2i−1,3}$）与矩阵类似。</p><h3 id="2-3-5-张量算法的基本性质"><a href="#2-3-5-张量算法的基本性质" class="headerlink" title="2.3.5. 张量算法的基本性质"></a>2.3.5. 张量算法的基本性质</h3><ol><li>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">20</span>, dtype=torch.float32).reshape(<span class="hljs-number">5</span>, <span class="hljs-number">4</span>)<br>B = A.clone()  <span class="hljs-comment"># 通过分配新内存，将A的一个副本分配给B</span><br>A, A + B<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="hljs-string">         [ 4.,  5.,  6.,  7.],</span><br><span class="hljs-string">         [ 8.,  9., 10., 11.],</span><br><span class="hljs-string">         [12., 13., 14., 15.],</span><br><span class="hljs-string">         [16., 17., 18., 19.]]),</span><br><span class="hljs-string"> tensor([[ 0.,  2.,  4.,  6.],</span><br><span class="hljs-string">         [ 8., 10., 12., 14.],</span><br><span class="hljs-string">         [16., 18., 20., 22.],</span><br><span class="hljs-string">         [24., 26., 28., 30.],</span><br><span class="hljs-string">         [32., 34., 36., 38.]]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号$⊙$）。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">A * B<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[  0.,   1.,   4.,   9.],</span><br><span class="hljs-string">        [ 16.,  25.,  36.,  49.],</span><br><span class="hljs-string">        [ 64.,  81., 100., 121.],</span><br><span class="hljs-string">        [144., 169., 196., 225.],</span><br><span class="hljs-string">        [256., 289., 324., 361.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="3"><li>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">a = <span class="hljs-number">2</span><br>X = torch.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>a + X, (a * X).shape<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([[[ 2,  3,  4,  5],</span><br><span class="hljs-string">          [ 6,  7,  8,  9],</span><br><span class="hljs-string">          [10, 11, 12, 13]],</span><br><span class="hljs-string"></span><br><span class="hljs-string">         [[14, 15, 16, 17],</span><br><span class="hljs-string">          [18, 19, 20, 21],</span><br><span class="hljs-string">          [22, 23, 24, 25]]]),</span><br><span class="hljs-string"> torch.Size([2, 3, 4]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-6-降维"><a href="#2-3-6-降维" class="headerlink" title="2.3.6. 降维"></a>2.3.6. 降维</h3><ol><li><p>我们可以对任意张量进行的一个有用的操作是计算其元素的和。</p><p> 我们可以表示任意形状张量的元素和。 例如，矩阵$A$中元素的和可以记为$\sum^m_{i=1}\sum^n_{j=1}a_{ij}$。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">A.shape, A.<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(torch.Size([5, 4]), tensor(190.))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li><p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。</p><p> 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），我们可以在调用函数时指定<code>axis=0</code>。</p><p> 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">A_sum_axis0 = A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>)<br>A_sum_axis0, A_sum_axis0.shape<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([40., 45., 50., 55.]), torch.Size([4]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>A_sum_axis1 = A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)<br>A_sum_axis1, A_sum_axis1.shape<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>A.<span class="hljs-built_in">sum</span>(axis=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])  <span class="hljs-comment"># SameasA.sum()</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(190.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="3"><li><p>一个与求和相关的量是<em>平均值</em>（mean或average）。</p><p> 同样，计算平均值的函数也可以沿指定轴降低张量的维度。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">A.mean(), A.<span class="hljs-built_in">sum</span>() / A.numel()<br><span class="hljs-comment"># A.numel() = A: number of elements</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor(9.5000), tensor(9.5000))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>A.mean(axis=<span class="hljs-number">0</span>), A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) / A.shape[<span class="hljs-number">0</span>]<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="2-3-6-1-非降维求和"><a href="#2-3-6-1-非降维求和" class="headerlink" title="2.3.6.1. 非降维求和"></a>2.3.6.1. 非降维求和</h4><ol><li><p>有时在调用函数来计算总和或均值时保持轴数不变会很有用。</p><p> 由于<code>sum_A</code>在对每行进行求和后仍保持两个轴，我们可以通过<strong>广播</strong>将<code>A</code>除以<code>sum_A</code>。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">sum_A = A.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>sum_A<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 6.],</span><br><span class="hljs-string">        [22.],</span><br><span class="hljs-string">        [38.],</span><br><span class="hljs-string">        [54.],</span><br><span class="hljs-string">        [70.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>A / sum_A<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[0.0000, 0.1667, 0.3333, 0.5000],</span><br><span class="hljs-string">        [0.1818, 0.2273, 0.2727, 0.3182],</span><br><span class="hljs-string">        [0.2105, 0.2368, 0.2632, 0.2895],</span><br><span class="hljs-string">        [0.2222, 0.2407, 0.2593, 0.2778],</span><br><span class="hljs-string">        [0.2286, 0.2429, 0.2571, 0.2714]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><ol start="2"><li>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），我们可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">A.cumsum(axis=<span class="hljs-number">0</span>)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="hljs-string">        [ 4.,  6.,  8., 10.],</span><br><span class="hljs-string">        [12., 15., 18., 21.],</span><br><span class="hljs-string">        [24., 28., 32., 36.],</span><br><span class="hljs-string">        [40., 45., 50., 55.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-7-点积（Dot-Product）"><a href="#2-3-7-点积（Dot-Product）" class="headerlink" title="2.3.7. 点积（Dot Product）"></a>2.3.7. 点积（Dot Product）</h3><p>给定两个向量$x,y∈R^d$， 它们的<em>点积</em>（dot product）$x^Ty$ （或$⟨x,y⟩$） 是相同位置的按元素乘积的和：$x^Ty=\sum^d_{i=1}x_iy_i$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">y = torch.ones(<span class="hljs-number">4</span>, dtype = torch.float32)<br>x, y, torch.dot(x, y)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-8-矩阵-向量积（matrix-vector-product）"><a href="#2-3-8-矩阵-向量积（matrix-vector-product）" class="headerlink" title="2.3.8. 矩阵-向量积（matrix-vector product）"></a>2.3.8. 矩阵-向量积（matrix-vector product）</h3><p>我们为矩阵<code>A</code>和向量<code>x</code>调用<code>torch.mv(A, x)</code>时，会执行矩阵-向量积。 注意，<code>A</code>的列维数（沿轴1的长度）必须与<code>x</code>的维数（其长度）相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">A.shape, x.shape, torch.mv(A, x)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-9-矩阵-矩阵乘法（matrix-matrix-multiplication）"><a href="#2-3-9-矩阵-矩阵乘法（matrix-matrix-multiplication）" class="headerlink" title="2.3.9. 矩阵-矩阵乘法（matrix-matrix multiplication）"></a>2.3.9. 矩阵-矩阵乘法（matrix-matrix multiplication）</h3><p>矩阵-矩阵乘法可以简单地称为<strong>矩阵乘法</strong>，不应与“Hadamard积”混淆。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">B = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>)<br>torch.mm(A, B)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor([[ 6.,  6.,  6.],</span><br><span class="hljs-string">        [22., 22., 22.],</span><br><span class="hljs-string">        [38., 38., 38.],</span><br><span class="hljs-string">        [54., 54., 54.],</span><br><span class="hljs-string">        [70., 70., 70.]])</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-3-10-范数"><a href="#2-3-10-范数" class="headerlink" title="2.3.10. 范数"></a>2.3.10. 范数</h3><p>线性代数中最有用的一些运算符是<em>范数</em>（norm）。 非正式地说，一个向量的<em>范数</em>告诉我们一个向量有多大。 这里考虑的<em>大小</em>（size）概念不涉及维度，而是分量的大小。</p><p>性质：</p><ol><li><p>在线性代数中，向量范数是将向量映射到标量的函数ff。 给定任意向量xx，向量范数要满足一些属性。 第一个性质是：如果我们按常数因子αα缩放向量的所有元素， 其范数也会按相同常数因子的<em>绝对值</em>缩放：</p><p> $$<br> f(\alpha x)=|\alpha |f(x).<br> $$</p></li><li><p>第二个性质是我们熟悉的三角不等式:<br> $$<br> f(x+y)≤f(x)+f(y).<br> $$</p></li><li><p>第三个性质简单地说范数必须是非负的:<br> $$<br> f(x)≥0.<br> $$<br> 这是有道理的。因为在大多数情况下，任何东西的最小的<em>大小</em>是0。 最后一个性质要求范数最小为0，当且仅当向量全由0组成。</p><p> $$<br> ∀i,[x]i=0⇔f(x)=0.<br> $$</p></li></ol><hr><p>欧几里得距离是一个$L2$范数： 假设$n$维向量$x$中的元素是$x1,…,xn$，其$L2$范数是向量元素平方和的平方根：<br>$$<br>||x|| _ 2 = \sqrt {∑_{i=1}^nx^2_i}<br>$$</p><p>其中，在$L2$范数中常常省略下标$2$，也就是说$‖x‖$等同于$‖x‖2$。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">u = torch.tensor([<span class="hljs-number">3.0</span>, -<span class="hljs-number">4.0</span>])<br>torch.norm(u)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(5.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>$L1$范数，它表示为向量元素的绝对值之和：<br>$$<br>||x|| _ 1 = ∑_{i=1}^n|x_i|.<br>$$<br>与$L2$范数相比，$L1$范数受异常值的影响较小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.<span class="hljs-built_in">abs</span>(u).<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># Same as below</span><br>torch.norm(u, <span class="hljs-number">1</span>)<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(7.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>$L2$范数和$L1$范数都是更一般的$L_p$范数的特例：<br>$$<br>|| x || _ p=(∑_{i=1}^n|x_i|^p)^{1/p}.<br>$$<br>类似于向量的$L2$范数，矩阵$X∈R^{m×n}$的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根：</p><p>$$<br>\left |  X\right | _ F = \sqrt{∑_{i=1}^m∑_{j=1}^nx^2_{ij}}<br>$$</p><p>Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的L2L2范数。 调用以下函数将计算矩阵的Frobenius范数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.norm(torch.ones((<span class="hljs-number">4</span>, <span class="hljs-number">9</span>)))<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">tensor(6.)</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><h4 id="2-3-10-1-范数和目标"><a href="#2-3-10-1-范数和目标" class="headerlink" title="2.3.10.1. 范数和目标"></a>2.3.10.1. 范数和目标</h4><p>在深度学习中，我们经常试图解决优化问题： <em>最大化</em>分配给观测数据的概率; <em>最小化</em>预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。</p><p>目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。</p><h3 id="2-3-11-关于线性代数的更多信息"><a href="#2-3-11-关于线性代数的更多信息" class="headerlink" title="2.3.11. 关于线性代数的更多信息"></a>2.3.11. 关于线性代数的更多信息</h3><p>可以参考<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html">线性代数运算的在线附录</a>或其他优秀资源 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#strang-1993">Strang, 1993]</a>[<a href="https://zh.d2l.ai/chapter_references/zreferences.html#kolter-2008">Kolter, 2008]</a>[<a href="https://zh.d2l.ai/chapter_references/zreferences.html#petersen-pedersen-ea-2008">Petersen et al., 2008]</a>。</p><h3 id="2-3-12-小结"><a href="#2-3-12-小结" class="headerlink" title="2.3.12. 小结"></a>2.3.12. 小结</h3><ul><li>标量、向量、矩阵和张量是线性代数中的基本数学对象。</li><li>向量泛化自标量，矩阵泛化自向量。</li><li>标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。</li><li>一个张量可以通过<code>sum</code>和<code>mean</code>沿指定的轴降低维度。</li><li>两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。</li><li>在深度学习中，我们经常使用范数，如L1L1范数、L2L2范数和Frobenius范数。</li><li>我们可以对标量、向量、矩阵和张量执行各种操作。</li></ul>]]></content>
    
    
    <categories>
      
      <category>《动手学深度学习》学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>学习笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《动手学深度学习》学习笔记 Ch.1 - 前言</title>
    <link href="/2022-02-20-D2L-Ch1/"/>
    <url>/2022-02-20-D2L-Ch1/</url>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><h2 id="1-1-日常生活中的机器学习"><a href="#1-1-日常生活中的机器学习" class="headerlink" title="1.1. 日常生活中的机器学习"></a>1.1. 日常生活中的机器学习</h2><p>如图所示，训练过程通常包含如下步骤：</p><ol><li>从一个随机初始化参数的模型开始，这个模型基本毫不“智能”。</li><li>获取一些数据样本（例如，音频片段以及对应的{是,否}{是,否}标签）。</li><li>调整参数，使模型在这些样本中表现得更好。</li><li>重复第2步和第3步，直到模型在任务中的表现令你满意。</li></ol><p><img src="https://zh.d2l.ai/_images/ml-loop.svg" alt="机器学习"></p><h2 id="1-2-关键组件"><a href="#1-2-关键组件" class="headerlink" title="1.2. 关键组件"></a>1.2. 关键组件</h2><ol><li>我们可以学习的<em>数据</em>（data）。</li><li>如何转换数据的<em>模型</em>（model）。</li><li>一个<em>目标函数</em>（objective function），用来量化模型的有效性。</li><li>调整模型参数以优化目标函数的<em>算法</em>（algorithm）。</li></ol><h3 id="1-2-1-数据"><a href="#1-2-1-数据" class="headerlink" title="1.2.1. 数据"></a>1.2.1. 数据</h3><p>毋庸置疑，如果没有数据，那么数据科学毫无用武之地。 每个数据集由一个个<em>样本</em>（example, sample）组成，大多时候，它们遵循独立同分布(independently and identically distributed, i.i.d.)。 样本有时也叫做<em>数据点</em>（data point）或者<em>数据实例</em>（data instance），通常每个样本由一组称为<em>特征</em>（features，或<em>协变量</em>（covariates））的属性组成。 机器学习模型会根据这些属性进行预测。 在上面的监督学习问题中，要预测的是一个特殊的属性，它被称为<em>标签</em>（label，或<em>目标</em>（target））。</p><p>如果数据中充满了错误，或者如果数据的特征不能预测任务目标，那么模型很可能无效。 有一句古语很好地反映了这个现象：“输入的是垃圾，输出的也是垃圾。”（“Garbage in, garbage out.”）</p><h3 id="1-2-2-模型"><a href="#1-2-2-模型" class="headerlink" title="1.2.2. 模型"></a>1.2.2. 模型</h3><p>深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为<em>深度学习</em>（deep learning）。</p><h3 id="1-2-3-目标函数"><a href="#1-2-3-目标函数" class="headerlink" title="1.2.3. 目标函数"></a>1.2.3. 目标函数</h3><p> 在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为<em>目标函数</em>（objective function）。 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为<em>损失函数</em>（loss function，或cost function）。</p><p>当任务在试图预测数值时，最常见的损失函数是<em>平方误差</em>（squared error），即预测值与实际值之差的平方。 </p><p>通常，损失函数是根据模型参数定义的，并取决于数据集。 在一个数据集上，我们通过最小化总损失来学习模型参数的最佳值。 该数据集由一些为训练而收集的样本组成，称为<em>训练数据集</em>（training dataset，或称为<em>训练集</em>（training set））。 然而，在训练数据上表现良好的模型，并不一定在“新数据集”上有同样的效能，这里的“新数据集”通常称为<em>测试数据集</em>（test dataset，或称为<em>测试集</em>（test set））。</p><h3 id="1-2-4-优化算法"><a href="#1-2-4-优化算法" class="headerlink" title="1.2.4. 优化算法"></a>1.2.4. 优化算法</h3><p>深度学习中，大多流行的优化算法通常基于一种基本方法–<em>梯度下降</em>（gradient descent）。 简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。 然后，它在可以减少损失的方向上优化参数。</p><h2 id="1-3-各种机器学习问题"><a href="#1-3-各种机器学习问题" class="headerlink" title="1.3. 各种机器学习问题"></a>1.3. 各种机器学习问题</h2><h3 id="1-3-1-监督学习"><a href="#1-3-1-监督学习" class="headerlink" title="1.3.1. 监督学习"></a>1.3.1. 监督学习</h3><p><em>监督学习</em>（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个<em>样本</em>（example）。 </p><p>虽然监督学习只是几大类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都是监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为：在给定一组特定的可用数据的情况下，估计未知事物的概率。比如：</p><ul><li>根据计算机断层扫描（CT）肿瘤图像，预测是否为癌症。</li><li>给出一个英语句子，预测正确的法语翻译。</li><li>根据本月的财务报告数据，预测下个月股票的价格。</li></ul><p><img src="https://zh.d2l.ai/_images/supervised-learning.svg" alt="监督学习"></p><h4 id="1-3-1-1-回归"><a href="#1-3-1-1-回归" class="headerlink" title="1.3.1.1. 回归"></a>1.3.1.1. 回归</h4><p><em>回归</em>（regression）是最简单的监督学习任务之一。假设你在市场上寻找新房子，你可能需要估计一栋房子的公平市场价值。 销售价格，即标签，是一个数值。 当标签取任意数值时，我们称之为<em>回归</em>问题。</p><p>总而言之，判断回归问题的一个很好的经验法则是，任何有关“多少”的问题很可能就是回归问题。比如：</p><ul><li>这个手术需要多少小时？</li><li>在未来六小时，这个镇会有多少降雨量？</li></ul><h4 id="1-3-1-2-分类"><a href="#1-3-1-2-分类" class="headerlink" title="1.3.1.2. 分类"></a>1.3.1.2. 分类</h4><p>在<em>分类</em>问题中，我们希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class））。 </p><p>当我们有两个以上的类别时，我们把这个问题称为<em>多元分类</em>（multiclass classification）问题。 常见的例子包括手写字符识别 {0,1,2,…9,a,b,c,…}。 与解决回归问题不同，分类问题的常见损失函数被称为<em>交叉熵</em>（cross-entropy），我们将在后面的章节中详细阐述。</p><h4 id="1-3-1-3-标记问题"><a href="#1-3-1-3-标记问题" class="headerlink" title="1.3.1.3. 标记问题"></a>1.3.1.3. 标记问题</h4><p>学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification）。</p><h4 id="1-3-1-4-搜索"><a href="#1-3-1-4-搜索" class="headerlink" title="1.3.1.4. 搜索"></a>1.3.1.4. 搜索</h4><p>有时，我们不仅仅希望输出为一个类别或一个实值。 在信息检索领域，我们希望对一组项目进行排序。以网络搜索为例，我们的目标不是简单的“查询（query）-网页（page）”分类，而是在海量搜索结果中找到用户最需要的那部分。</p><h4 id="1-3-1-5-推荐系统"><a href="#1-3-1-5-推荐系统" class="headerlink" title="1.3.1.5. 推荐系统"></a>1.3.1.5. 推荐系统</h4><p>另一类与搜索和排名相关的问题是<em>推荐系统</em>（recommender system），它的目标是向特定用户进行“个性化”推荐。 例如，对于电影推荐，科幻迷和喜剧爱好者的推荐结果页面可能会有很大不同。 类似的应用也会出现在零售产品、音乐和新闻推荐等等。</p><h4 id="1-3-1-6-序列学习"><a href="#1-3-1-6-序列学习" class="headerlink" title="1.3.1.6. 序列学习"></a>1.3.1.6. 序列学习</h4><ol><li><strong>标记和解析</strong>。这涉及到用属性注释文本序列。 </li><li><strong>自动语音识别</strong>。</li><li><strong>文本到语音</strong>。这与自动语音识别相反。 </li><li><strong>机器翻译</strong>。 在语音识别中，输入和输出的出现顺序基本相同。 而在机器翻译中，颠倒输入和输出的顺序非常重要。</li></ol><h3 id="1-3-2-无监督学习"><a href="#1-3-2-无监督学习" class="headerlink" title="1.3.2. 无监督学习"></a>1.3.2. 无监督学习</h3><p>如果你的工作没有十分具体的目标，你就需要“自发”地去学习了。 （如果你打算成为一名数据科学家，你最好培养这个习惯。） 比如，你的老板可能会给你一大堆数据，然后让你用它做一些数据科学研究，却没有对结果有要求。 我们称这类数据中不含有“目标”的机器学习问题为<em>无监督学习</em>（unsupervised learning）， 我们将在后面的章节中讨论无监督学习技术。 那么无监督学习可以回答什么样的问题呢？我们来看看下面的例子：</p><ul><li><em>聚类</em>（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？</li><li><em>主成分分析</em>（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。</li><li><em>因果关系</em>（causality）和<em>概率图模型</em>（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</li><li><em>生成对抗性网络</em>（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。</li></ul><h3 id="1-3-3-与环境互动"><a href="#1-3-3-与环境互动" class="headerlink" title="1.3.3. 与环境互动"></a>1.3.3. 与环境互动</h3><p>到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为<em>离线学习</em>（offline learning）。</p><p><img src="https://zh.d2l.ai/_images/data-collection.svg" alt="从环境中为监督学习收集数据。"></p><p>考虑“与真实环境互动”将打开一整套新的建模问题。以下只是几个例子：</p><ul><li>环境还记得我们以前做过什么吗？</li><li>环境是否有助于我们建模？例如，用户将文本读入语音识别器。</li><li>环境是否想要打败模型？例如，一个对抗性的设置，如垃圾邮件过滤或玩游戏？</li><li>环境是否重要？</li><li>环境是否变化？例如，未来的数据是否总是与过去相似，还是随着时间的推移会发生变化？是自然变化还是响应我们的自动化工具而发生变化？</li></ul><p>当训练和测试数据不同时，最后一个问题提出了<em>分布偏移</em>（distribution shift）的问题。 接下来，我们将简要描述强化学习问题，这是一类明确考虑与环境交互的问题。</p><h3 id="1-3-4-强化学习"><a href="#1-3-4-强化学习" class="headerlink" title="1.3.4. 强化学习"></a>1.3.4. 强化学习</h3><p>如果你对使用机器学习开发与环境交互并采取行动感兴趣，那么你最终可能会专注于<em>强化学习</em>（reinforcement learning）。 这可能包括应用到机器人、对话系统，甚至开发视频游戏的人工智能（AI）。 <em>深度强化学习</em>（deep reinforcement learning）将深度学习应用于强化学习的问题，是非常热门的研究领域。 突破性的深度<em>Q网络</em>（Q-network）在雅达利游戏中仅使用视觉输入就击败了人类， 以及 AlphaGo 程序在棋盘游戏围棋中击败了世界冠军，是两个突出强化学习的例子。</p><p>在强化学习问题中，agent在一系列的时间步骤上与环境交互。 在每个特定时间点，agent从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后agent从环境中获得<em>奖励</em>（reward）。 此后新一轮循环开始，agent接收后续观察，并选择后续操作，依此类推。 强化学习的过程在 <a href="https://zh.d2l.ai/chapter_introduction/index.html#fig-rl-environment">图1.3.7</a> 中进行了说明。 请注意，强化学习的目标是产生一个好的<em>策略</em>（policy）。 强化学习agent选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。</p><p><img src="https://zh.d2l.ai/_images/rl-environment.svg" alt="强化学习和环境之间的相互作用"></p><p>当环境可被完全观察到时，我们将强化学习问题称为<em>马尔可夫决策过程</em>（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为<em>上下文赌博机</em>（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的<em>多臂赌博机</em>（multi-armed bandit problem）。</p><h2 id="1-4-起源"><a href="#1-4-起源" class="headerlink" title="1.4. 起源"></a>1.4. 起源</h2><p><em>神经网络</em>（neural networks）的得名源于生物灵感。 一个多世纪以来（追溯到1873年亚历山大·贝恩和1890年詹姆斯·谢林顿的模型），研究人员一直试图组装类似于相互作用的神经元网络的计算电路。 随着时间的推移，对生物学的解释变得不再肤浅，但这个名字仍然存在。 其核心是当今大多数网络中都可以找到的几个关键原则：</p><ul><li>线性和非线性处理单元的交替，通常称为<em>层</em>（layers）。</li><li>使用链式规则（也称为<em>反向传播</em>（backpropagation））一次性调整网络中的全部参数。</li></ul><p>在最初的快速发展之后，神经网络的研究从1995年左右一直开始停滞不前，直到到2005年才稍有起色。 这主要是因为两个原因。 首先，训练网络（在计算上）非常昂贵。 在上个世纪末，随机存取存储器（RAM）非常强大，而计算能力却很弱。 其次，数据集相对较小。 事实上，费舍尔1932年的鸢尾花卉数据集是测试算法有效性的流行工具， 而MNIST数据集的60000个手写数字的数据集被认为是巨大的。 考虑到数据和计算的稀缺性，<em>核方法</em>（kernel method）、<em>决策树</em>（decision tree）和<em>图模型</em>（graph models）等强大的统计工具（在经验上）证明是更为优越的。 与神经网络不同的是，这些算法不需要数周的训练，而且有很强的理论依据，可以提供可预测的结果。</p><h2 id="1-5-深度学习之路"><a href="#1-5-深度学习之路" class="headerlink" title="1.5. 深度学习之路"></a>1.5. 深度学习之路</h2><p>大约2010年开始，那些在计算上看起来不可行的神经网络算法变得热门起来，实际上是以下两点导致的： 其一，随着互联网的公司的出现，为数亿在线用户提供服务，大规模数据集变得触手可及。 另外，廉价又高质量的传感器、廉价的数据存储（克莱德定律）以及廉价计算（摩尔定律）的普及，特别是GPU的普及，使大规模算力唾手可得。</p><h2 id="1-6-成功案例"><a href="#1-6-成功案例" class="headerlink" title="1.6. 成功案例"></a>1.6. 成功案例</h2><p>直到最近，人工智能才成为人们关注的焦点，主要是因为解决了以前被认为难以解决的问题，这些问题与消费者直接相关。许多这样的进步都归功于深度学习。</p><ul><li>智能助理，如苹果的Siri、亚马逊的Alexa和谷歌助手，都能够相当准确地回答口头问题。这包括一些琐碎的工作，比如打开电灯开关（对残疾人来说是个福音）甚至预约理发师和提供电话支持对话。这可能是人工智能正在影响我们生活的最明显的迹象。</li><li>数字助理的一个关键要素是准确识别语音的能力。逐渐地，在某些应用中，此类系统的准确性已经提高到与人类同等水平的程度 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#xiong-wu-alleva-ea-2018">Xiong et al., 2018]</a>。</li><li>物体识别同样也取得了长足的进步。估计图片中的物体在2010年是一项相当具有挑战性的任务。在ImageNet基准上，来自NEC实验室和伊利诺伊大学香槟分校的研究人员获得了28%的Top-5错误率 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#lin-lv-zhu-ea-2010">Lin et al., 2010]</a> 。到2017年，这一错误率降低到2.25% [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#hu-shen-sun-2018">Hu et al., 2018]</a> 。同样，在鉴别鸟类或诊断皮肤癌方面也取得了惊人的成果。</li><li>游戏曾经是人类智慧的堡垒。从TD-Gammon开始，一个使用时差强化学习的五子棋游戏程序，算法和计算的进步导致了算法被广泛应用。与五子棋不同的是，国际象棋有一个复杂得多的状态空间和一组动作。深蓝公司利用大规模并行性、专用硬件和高效搜索游戏树 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#campbell-hoane-jr-hsu-2002">Campbell et al., 2002]</a> 击败了加里·卡斯帕罗夫(Garry Kasparov)。围棋由于其巨大的状态空间，难度更大。AlphaGo在2015年达到了相当于人类的棋力，使用和蒙特卡洛树抽样 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#silver-huang-maddison-ea-2016">Silver et al., 2016]</a> 相结合的深度学习。扑克中的挑战是状态空间很大，而且没有完全观察到（我们不知道对手的牌）。在扑克游戏中，库图斯使用有效的结构化策略超过了人类的表现 [<a href="https://zh.d2l.ai/chapter_references/zreferences.html#brown-sandholm-2017">Brown &amp; Sandholm, 2017]</a> 。这说明了游戏取得了令人瞩目的进步以及先进的算法在其中发挥了关键作用的事实。</li><li>人工智能进步的另一个迹象是自动驾驶汽车和卡车的出现。虽然完全自主还没有完全触手可及，但在这个方向上已经取得了很好的进展，特斯拉（Tesla）、英伟达（NVIDIA）和Waymo等公司的产品至少实现了部分自主。让完全自主如此具有挑战性的是，正确的驾驶需要感知、推理和将规则纳入系统的能力。目前，深度学习主要应用于这些问题的计算机视觉方面。其余部分则由工程师进行大量调整。</li></ul><h2 id="1-7-特点"><a href="#1-7-特点" class="headerlink" title="1.7. 特点"></a>1.7. 特点</h2><p>到目前为止，我们已经广泛地讨论了机器学习，它既是人工智能的一个分支，也是人工智能的一种方法。 虽然深度学习是机器学习的一个子集，但令人眼花缭乱的算法和应用程序集让人很难评估深度学习的具体成分是什么。 这就像试图确定披萨所需的配料一样困难，因为几乎每种成分都是可以替代的。</p><h2 id="1-8-小结"><a href="#1-8-小结" class="headerlink" title="1.8. 小结"></a>1.8. 小结</h2><ul><li>机器学习研究计算机系统如何利用经验（通常是数据）来提高特定任务的性能。它结合了统计学、数据挖掘和优化的思想。通常，它是被用作实现人工智能解决方案的一种手段。</li><li>表示学习作为机器学习的一类，其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过学习多层次的转换来进行的多层次的表示学习。</li><li>深度学习不仅取代了传统机器学习的浅层模型，而且取代了劳动密集型的特征工程。</li><li>最近在深度学习方面取得的许多进展，大都是由廉价传感器和互联网规模应用所产生的大量数据，以及（通过GPU）算力的突破来触发的。</li><li>整个系统优化是获得高性能的关键环节。有效的深度学习框架的开源使得这一点的设计和实现变得非常容易。</li></ul>]]></content>
    
    
    <categories>
      
      <category>《动手学深度学习》学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>学习笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021厦门大学CS保研经历 | 夏令营游记 | MAC实验室</title>
    <link href="/2021-10-26-XMU-Summer-Camp/"/>
    <url>/2021-10-26-XMU-Summer-Camp/</url>
    
    <content type="html"><![CDATA[<h2 id="【个人情况】"><a href="#【个人情况】" class="headerlink" title="【个人情况】"></a>【个人情况】</h2><p>本科：南京某不知名211，计算机科学与技术专业<br>排名：12/242（夏令营绩点排名），5/242（预推免综排）<br>荣誉奖项：2020年国家奖学金<br>竞赛经历：ICPC EC 铜牌，江苏省赛银牌第二，CSP 390分等ACM相关<br>项目经历：软件杯三等奖 * 2<br>科研经历：几乎无<br>实习经历：7月中-9月底在腾讯实习（然而并没在保研过程中写进简历）<br>最终去向：厦门大学CS学硕，MAC实验室</p><h2 id="【院校选择】"><a href="#【院校选择】" class="headerlink" title="【院校选择】"></a>【院校选择】</h2><p>其实在保研正式开始前，对自己的定位还是倾向于就业，我的优势是ACM打的多，<del>劣势是没打出成绩x</del>，CSP也有前1%，劣势是科研项目经历比较少。原本的打算找羊导在研究生期间疯狂刷实习经历最后去大厂，也是因此在大三下学期初面试拿了腾讯实习的offer，但这可能也是这次实习坚定了我最后选择厦大MAC的决心吧，不过是后话了。</p><p>按往年的情况分析，自己大概率能在预推免时候上岸华五软院（特指南软浙软）专硕。本人对学硕和科研没有太大执念，但是也不排斥，如果方向感兴趣也可以考虑尝试。目标院校层次大概是南软or浙软或北航、同济、电科、华师等（个人感觉南软浙软和别的选择有很大的区别，可能是偏向牌子刷简历，但就业去向基本全靠自己这样）。</p><p>投递夏令营的时候还是处于比较迷茫的状态，于是选择了对着GitHub仓库把大部分中九及以上都投递了一遍（当然也是被刷得很惨了TUT）。</p><h2 id="【入营情况】"><a href="#【入营情况】" class="headerlink" title="【入营情况】"></a>【入营情况】</h2><p>简历投递和入营开奖是一场持久战……很多同学到最后甚至因为投递&amp;入营太多，懒得关注其他中九的开营情况了，而入营开奖更是让人煎熬。一边经历着最心累的一次期末备考，一边天天关注着开奖情况。厦大是最早开的，北航由于夏令营的时间和厦大冲突了，而且厦大是线下，所以后来选择撤回报名，打算预推免再报。过了很久之后，收到了同济的入营，之后等了很久，但再也没有了……</p><p>入营后开始了解学校老师的情况，也是入营厦大后才了解到厦大的纪荣嵘老师和MAC实验室，当时就去实验室主页和纪老师的个人主页看了看。并且在了解到纪老师的研究方向以及<strong>最最打动我</strong>的招生介绍后，立刻和纪老师发了邮件，也得到了回复。同济方面，我找在同济CS的高中同学了解了一下情况后，也联系了同济SLAM的老师，但是可能正值期末，老师过了一周后才回复了我欢迎报考的邮件。</p><p>最终入营情况：厦门大学信息学院CS系，同济大学电院CS系。<br>（华师、南软都被刷，心里其实挺不好受的……但这也充分说明了夏令营阶段绩点排名才是yyds）</p><h2 id="【面试经验】"><a href="#【面试经验】" class="headerlink" title="【面试经验】"></a>【面试经验】</h2><ul><li><strong>厦门大学（7.9-7.11）</strong></li></ul><p>厦大是今年比较少有的线下举办的夏令营，7.9下午从南京飞到厦门，去厦门大学海韵校区报到领取宿舍钥匙等生活用品，晚上开营仪式。路上还结识了HEU的金牌爷爷。厦大CS系的考核比较多，第一天上午机试，下午笔试，第二天面试。没记错的话分数比例应该是3:3:4。</p><p><img src="/img/2021-10-26-XMU-Summer-Camp/XMU_welcome.jpeg" alt="厦大报到"></p><p>机考挺简单的，三道题，两小时。毕竟ACM签到这么多年……半小时左右就第一个AK了，坐了一会儿出来了，后续应该陆陆续续有3、40的人都AK了？第一题是矩阵蛇形填充，第二题是最大子段和，第三题是结构体的排序。机试完进行了系内实验室和老师的介绍讲座，1. MAC实验室没来CS系宣讲，不知道AI系那边有没有 2. CS系这边最大的ASC实验室似乎也是做SLAM的，王程教授的气场真的很足……</p><p>笔试破防了，一点点。全考算法与数据结构，九道大题，主要是各种基础算法和数据结构的考察，难度中等，但我确实没考太好。考前还匆忙看了下厦大本科生的教材，重点看了下哈希和排序那块的考法顺便加深了下印象。但是卷子里的B-树插入过程和AOE网的题属实没防住，确实基础不牢还没全面复习，应该是被拉了一定的分……当晚和同校通信专业的同学一起去找厦大学姐，下午带我们逛了逛校园，晚上去思明的海边沙滩踩水。住在海边真的太棒了，很惬意~</p><p><img src="/img/2021-10-26-XMU-Summer-Camp/XMU_campus.jpeg" alt="厦大校园"></p><p><img src="/img/2021-10-26-XMU-Summer-Camp/XMU_MAC.jpeg" alt="厦大MAC"></p><p><img src="/img/2021-10-26-XMU-Summer-Camp/XMU_beach.jpeg" alt="厦大旁的海滩"></p><p>第二天分组到上午进行面试，面试过程挺顺利的，首先是英文2分钟自我介绍+ppt中文展示个人情况。老师们主要围绕我的项目提了几个问题，考察了一下项目相关的算法原理是否清楚了解。当时印象很深刻的是王程教授正好在我这组，第一个问题是厦大如果给我offer我会来吗，走前最后一句话是跟我说我拿了优营就一定要来，不管是去哪个实验室。</p><p>当天中午收到了联系的MAC实验室老师的邮件和电话，说她也在我参加面试的那组，对我的表现很满意，如果我想去MAC愿意为我留个位置，可以免考核去。当时真的超级开心！！感觉这三年逐渐给自己增加的压力和焦虑、迷茫都在这一瞬间值得了，自己向往的去向也给予我了这份认可。从那时起就开始非常认真地考虑自己是不是就要去厦大MAC了。</p><ul><li><strong>同济大学（未参加考核，7.12-7.14）</strong></li></ul><p>同济大学是线上举办夏令营，按同学的话说，非常类似一场小考研，专业课+机考+英语+面试……按往年的经验看，最后就是夏令营和预推免的同学按成绩总分的高低排序，顺序录取，所以夏令营可能也没有太大的优势。</p><p>听宣讲的时候就有点三心二意了，一边想着厦大的offer，一边也很忐忑自己这最近的状态可能根本没准备好这场专业课大联考，而且如果拿到同济的offer，地理位置&amp;导师选择还是让我一直都在犹豫要不要继续参加。感谢当时一直听我纠结自己事情的同学、学长，最后也是觉得厦大MAC都要我了，中九好像没什么必要看，同济对我来讲也没有那么大的吸引力了，遂放弃。</p><p>放弃同济那晚，心里一块大石头也算落了地，一个人在深圳的酒店里好好哭了一场。</p><ul><li><strong>预推免（未参加，等厦大学硕候补）</strong></li></ul><p>其实在当时我看来，我已经是决定了要去从零开始我的CV生涯了，而且厦大MAC在我心里的权重，已经完全打败了其他的中九。并且我觉得华五CS就算我努努力拿到了它专硕的offer，可能在导师和研究方向上又没有选择权。而且那时的我在经受了腾讯实习的毒打后，也是越发觉得南软or浙软的学习生活应该不是我想要的那种。并且也在拿到厦大优营后签了承诺书，于是整个八九月也就没有继续准备预推免和参加别的环节了，全程在等待厦大的候补。</p><p>不得不说厦大的学硕名额确实是有点少，夏令营也只有12个……我自己在笔试的表现也不够好，所以只有等。期间也有学长表示不理解我为什么守着厦大专硕不试试别的学校，心态也确实越发焦虑，不过很幸运，927晚上通知我补录到了（厦大的鸽子也是真的很多……）。我的保研也就落下帷幕了~</p><h2 id="【一些感慨】"><a href="#【一些感慨】" class="headerlink" title="【一些感慨】"></a>【一些感慨】</h2><h3 id="1-厦大给我的印象真的太美好了"><a href="#1-厦大给我的印象真的太美好了" class="headerlink" title="1. 厦大给我的印象真的太美好了"></a>1. 厦大给我的印象真的太美好了</h3><p>我的本科学校是一所理工气氛相当浓厚的院校，学校面积小，人文气息不是很重，甚至常常感觉晚上的校园太安静了，甚至可以说有点死气沉沉。自己这三年又一直在各种莫名其妙地卷，感觉自己过得不太开心，这种感觉尤其是到了大三的下学期积蓄到了顶峰。</p><p>但是在厦大夏令营时，第一次在大学里感受到这样温暖和惬意，感觉自己的灵魂得到了净化。与其难受地学习生活、为了未来莫名其妙地卷，不如至少选个自己待着舒服的地方吧。我理解了厦大为什么坚持要线下开夏令营，也成功被她的魅力拐走了。</p><center>    <img src="/img/2021-10-26-XMU-Summer-Camp/chat_1.jpg" width="50%">    <br>    <font size="2" face="cursive" color="#999">聊天记录1</font>    <br><br>    <img src="/img/2021-10-26-XMU-Summer-Camp/chat_2.jpg" width="50%">    <br>    <font size="2" face="cursive" color="#999">聊天记录2</font></center><h3 id="2-建议大家还是不要学我"><a href="#2-建议大家还是不要学我" class="headerlink" title="2. 建议大家还是不要学我"></a>2. 建议大家还是不要学我</h3><p>从学长学姐那的经验&amp;其他帖子的分享说到，厦大是很看重诚信的，自己给的优营一定是不会咕的，但是会有很多学生咕厦大。个人的体验看来也是如此，由于去年我校学长学姐有咕厦大的经历，所以非常明显地体现在了今年入营的情况上，CS和AI专业我校入营人数相比去年少了非常多。从去年前年关注保研到现在，真的看到了太多学校咕学生的案例，自己的内心也是给某些学校画上了重点符号，学校海，学生也海，相互猜疑，感觉这样的推免过程真的很不正常。在这样的情况下受伤更多的总还是老实人学生，希望像厦大这样的学校能更多吧。</p><p>所以一点个人建议：首先就是不要学我做老实人，第二就是不要学我做躺平人……当然这也和每个人具体的情况和选择有关系，总之希望大家抓住机会，提前联系老师，保研过程一切顺利吧！</p>]]></content>
    
    
    <categories>
      
      <category>日记随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>日记</tag>
      
      <tag>厦门大学</tag>
      
      <tag>保研夏令营</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我在腾讯实习的第一周 | Dragonbra的第一篇博客</title>
    <link href="/2021-07-18-Internship/"/>
    <url>/2021-07-18-Internship/</url>
    
    <content type="html"><![CDATA[<h1 id="正文前的吐槽"><a href="#正文前的吐槽" class="headerlink" title="正文前的吐槽"></a>正文前的吐槽</h1><p>2021-07-18的晚上，今天已经是入职后的第一个周日了，经历了两天的忙碌租房和回酒店躺尸后，我又回到了我的工位<del>（为了夜宵券）</del>，终于有时间提笔来写这篇博客了。</p><p>其实上周一就火急火燎地赶来深圳了，在鹅厂提供的大house住了几天才在15号去报到，究其原因，还得问问为什么我的期末考试、软件杯DDL、厦大和同济的夏令营能那么满满当当地给我排在一起……</p><h1 id="Day01-报到"><a href="#Day01-报到" class="headerlink" title="Day01 - 报到"></a>Day01 - 报到</h1><blockquote><ul><li>居然就捡到了别人的Token（听导师说遗失罚款500）</li></ul></blockquote><p>其实早在三月初，我就接到了这次来实习的offer，结果七月中才来，心里还想着，我不会是最晚来的了吧。（谁知道这学期课这么多，为了保研……忍了）</p><p>15号当天，来到T站，领取了入职大礼包~（当天排队的人好多，又是大热天，沿着深南大道走那一路都让我浑身汗湿了，排队更是雪上加霜……感受到了夏天的恶意┭┮﹏┭┮）</p><p><img src="/img/2021-07-18-Internship/%E5%B7%A5%E5%8D%A1.png" alt="报到领取的工卡"></p><p>↑ 大家看这张工卡，不知道有没有细心的同学能看出来，我是硬掰开那个铁环塞进了我的工卡套的（原因可能是小姐姐忘记给我那个可拉伸的标着“Tencent”的小零件了，以致于后续……）</p><p>在我收拾收拾准备刷卡离开T站的时候，后面的小姐姐叫住我：“是不是你的Token掉了？”我半信半疑地接了过来（因为上面有我没有的那个”Tencent”的小圆牌啊！），带回了自己的工位。</p><p>结果我后续在新人报到群里问了下，有人帮我查了一下Token的序列号，果然不是我的。（那么旧又脏脏的，我想也不至于……）于是后来在企微上拉了个小群，约了一下见面地点就归还给那位同学了。</p><h1 id="Day01-导师与吃饭"><a href="#Day01-导师与吃饭" class="headerlink" title="Day01 - 导师与吃饭"></a>Day01 - 导师与吃饭</h1><p>报到后，我就自己拿着工卡前往部门所在的位置了。因为前一天也有联系，大概知道在哪里。怀揣着激动的心情，背着被QGG公仔填满的书包，冲上了腾大隔壁的大族。结果到了部门后，陷入了迷茫……没想到一层楼里这么大，人也很多，我不敢乱跑，在茶水间给导师和Leader打电话发微信也没有回我，最后还是可怜巴巴地被之前群里认识的同学领了进去，发现导师和Leader正在一起面谈事情。</p><p><img src="/img/2021-07-18-Internship/%E5%A4%A7%E6%97%8F%E5%A4%A7%E5%8E%A6.jpeg" alt="大族大厦"></p><p>大概自我介绍后，和导师相认。被带到了工位，愉快地”看着“自己即将使用的iMac纸箱（因为没工位了，坐不下，所以暂时没法拆 <del>这就是晚来的坏处吗</del> ）后来一直到11点多，部门秘书才告诉我可以搬去一个临时工位，但是这个位置上好像还是有人的，只是请了一周的假，不知道下周一会怎么办啊。</p><p>拆开Mac，简单组装了一下，意外发现配置居然比之前短信说的翻了个倍（指内存16G→32G，不知道是不是前任主人自己加的），小开心。简单装了下环境，买了一下五折Q币，就被导师领去吃饭了~</p><p>附近以腾大为中心的几栋楼都是腾讯的，不得不感慨一声，有钱真好。</p><p>中午在附近的购物中心里吃的，因为人实在是太多了，和导师还有当时一面面我的面试官一起，简单吃了碗牛肉面。晚上导师带我去腾大吃了吃自选食堂，还演示了一次夜宵券的用法O(∩_∩)O哈哈！感觉接下来两三个月里，有够吃了！</p><h1 id="Day02-工作与任务"><a href="#Day02-工作与任务" class="headerlink" title="Day02 - 工作与任务"></a>Day02 - 工作与任务</h1><p>第一天下午，简单和导师还有Leader一起聊了聊，确认了我最近的工作上的任务：先自己熟悉一下objective-C的语法，学习学习，读读组里的文档和源码，到觉得自己能理解了的时候，差不多也会给我提任务和需求交给我来完成了~</p><p>第二天的下午，正好到了组里周会的时间，开了快两个小时，和在别处办公的小伙伴们一起远程协商沟通，还看到了好多前两周还在软件工程这门课的PPT和卷子上折磨我的知识点，对组里的工作稍微有了更全面的了解吧~</p><p>这两天基本上都在配环境和读文档（dfs读文档读了一整个上午……还没完全看完），感觉新人刚来，想正式上手项目还有不短的一段路要走，尤其是我这种可能没什么实习经验的。同组还有几位实习生，但我好像是里面最小的，大家都是研二研三来实习，我知道的只有我还是本科qwq。感觉组里氛围很好，大家人都很nice的样子。（虽然大家好像对我的id很在意……）而且感觉组里最近似乎比较轻松，大家都在商量着休息团建之类的事，希望趁这段时间自己能好好补牢一下基础吧！</p><p><img src="/img/2021-07-18-Internship/%E5%8E%A6%E5%A4%A7&%E8%85%BE%E8%AE%AF.jpeg" alt="厦大&amp;&amp;腾讯，七月的收获"></p><p>上午的工作间隙，还收到了前些时参加的厦门大学夏令营优营的通知，也是一个小惊喜吧！（关于夏令营的具体日记可以看这篇~ <a href="https://dragonbra.github.io/2021/10/26/2021/2021-10-26-XMU-Summer-Camp/">2021厦门大学CS保研经历 | 夏令营游记 | MAC实验室</a> ）顺便用云打印打了一下优营承诺书，不得不说这个纸的质量也是真好……拿着我的木头铅笔在上面甚至有点画不出印子来，太厚了。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>在写下这篇博客的时候，我在腾讯实习的”第一周“（半个）也是要结束啦！</p><p>趁着周末的时间去把房子租了，感慨一下：短租房真的好难租啊！！七月的深圳真的好难找房啊！！不过最终也还算好，不到三千的价格在自如上租到了公司半小时通勤的位置的一个单间，至少比想象中的大出血要好得多了，明天早上就要从腾讯提供的中转住宿里退房搬到新住处啦，希望一切顺利~</p><p>另外，腾讯的周末好像是真的不加班啊，995企业名不虚传？本来想着这两天去逛逛各个办公楼的爱马哥（image咖啡），淘淘咖啡杯之类的周边，结果被告知周末都不开门。两天也都去了趟公司，基本也没有什么人在，感觉挺好的！（当然也可能只是刚好来对时间了哈哈哈）</p><p>预祝自己在接下来两个月时间里收获满满吧~当然，如果回学校了还能出来，那就是三个月吧~</p>]]></content>
    
    
    <categories>
      
      <category>日记随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>腾讯实习</tag>
      
      <tag>日记</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
